apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
---
apiVersion: v1
kind: Namespace
metadata:
  name: coturn
---
apiVersion: v1
kind: Namespace
metadata:
  name: crossplane
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: csi-proxmox
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: Namespace
metadata:
  name: hajimari
---
apiVersion: v1
kind: Namespace
metadata:
  name: immich
---
apiVersion: v1
kind: Namespace
metadata:
  name: keycloak
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: kube-system
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: netbird
---
apiVersion: v1
kind: Namespace
metadata:
  name: nextcloud
---
apiVersion: v1
kind: Namespace
metadata:
  name: ollama
---
apiVersion: v1
kind: Namespace
metadata:
  name: openwebui
---
apiVersion: v1
kind: Namespace
metadata:
  name: sealed-secrets
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: traefik
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: proxmox-csi
mountOptions:
  - noatime
parameters:
  cache: writethrough
  csi.storage.k8s.io/fstype: ext4
  ssd: "true"
  storage: local-zfs
provisioner: csi.proxmox.sinextra.dev
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-startupapicheck
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook
  namespace: cert-manager
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r2
    helm.sh/chart: coturn-1.0.1
  name: coturn
  namespace: coturn
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: rbac-manager
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster
  namespace: gitea
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-envoy
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-operator
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-generate-certs
  namespace: kube-system
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  name: hubble-relay
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-ui
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.8.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.13
  name: proxmox-cloud-controller-manager
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.7.0
    helm.sh/chart: ollama-1.17.0
  name: ollama
  namespace: ollama
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.10
    helm.sh/chart: open-webui-6.16.0
  name: open-webui
  namespace: openwebui
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets
  namespace: sealed-secrets
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
  namespace: traefik
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-cainjector-leader-election
      - cert-manager-cainjector-leader-election-core
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificaterequests
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-tokenrequest
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager-webhook-ca
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-controller
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - watch
      - list
      - delete
      - update
      - create
  - apiGroups:
      - storage.k8s.io
    resources:
      - csistoragecapacities
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator-tlsinterception-secrets
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - delete
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-tlsinterception-secrets
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - ""
    resourceNames:
      - hubble-server-certs
      - hubble-relay-client-certs
      - hubble-relay-server-certs
      - hubble-metrics-server-certs
      - hubble-ui-client-certs
    resources:
      - secrets
    verbs:
      - update
  - apiGroups:
      - ""
    resourceNames:
      - cilium-ca
    resources:
      - secrets
    verbs:
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - sealed-secrets-key
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - 'http:sealed-secrets:'
      - http:sealed-secrets:http
      - sealed-secrets
    resources:
      - services/proxy
    verbs:
      - create
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
rules:
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
      - create
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - csinodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattributesclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattachments
    verbs:
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattachments/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - create
      - update
      - patch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
  name: cert-manager-cluster-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-approve:cert-manager-io
rules:
  - apiGroups:
      - cert-manager.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - approve
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-certificates
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificates/status
      - certificaterequests
      - certificaterequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/finalizers
      - certificaterequests/finalizers
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
    verbs:
      - create
      - delete
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-certificatesigningrequests
rules:
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - certificates.k8s.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - sign
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-challenges
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - challenges/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - httproutes
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - route.openshift.io
    resources:
      - routes/custom-host
    verbs:
      - create
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-clusterissuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - clusterissuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-ingress-shim
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
    verbs:
      - create
      - update
      - delete
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/finalizers
    verbs:
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways
      - httproutes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways/finalizers
      - httproutes/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-issuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - issuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-orders
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - orders/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - create
      - delete
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: cert-manager-edit
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/status
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: cert-manager-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook:subjectaccessreviews
rules:
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - services
      - pods
      - endpoints
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
      - list
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - list
      - watch
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumbgppeeringpolicies
      - ciliumbgpnodeconfigs
      - ciliumbgpadvertisements
      - ciliumbgppeerconfigs
      - ciliumclusterwideenvoyconfigs
      - ciliumclusterwidenetworkpolicies
      - ciliumegressgatewaypolicies
      - ciliumendpoints
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumidentities
      - ciliumlocalredirectpolicies
      - ciliumnetworkpolicies
      - ciliumnodes
      - ciliumnodeconfigs
      - ciliumcidrgroups
      - ciliuml2announcementpolicies
      - ciliumpodippools
    verbs:
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
      - ciliumendpoints
      - ciliumnodes
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
    verbs:
      - delete
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
      - ciliumnodes/status
    verbs:
      - get
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints/status
      - ciliumendpoints
      - ciliuml2announcementpolicies/status
      - ciliumbgpnodeconfigs/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - ""
    resourceNames:
      - cilium-config
    resources:
      - configmaps
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services/status
    verbs:
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies
      - ciliumclusterwidenetworkpolicies
    verbs:
      - create
      - update
      - deletecollection
      - patch
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies/status
      - ciliumclusterwidenetworkpolicies/status
    verbs:
      - patch
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
      - ciliumidentities
    verbs:
      - delete
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes/status
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumbgppeerconfigs
      - ciliumbgpadvertisements
      - ciliumbgpnodeconfigs
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
      - patch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumbgpclusterconfigs/status
      - ciliumbgppeerconfigs/status
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - create
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resourceNames:
      - ciliumloadbalancerippools.cilium.io
      - ciliumbgppeeringpolicies.cilium.io
      - ciliumbgpclusterconfigs.cilium.io
      - ciliumbgppeerconfigs.cilium.io
      - ciliumbgpadvertisements.cilium.io
      - ciliumbgpnodeconfigs.cilium.io
      - ciliumbgpnodeconfigoverrides.cilium.io
      - ciliumclusterwideenvoyconfigs.cilium.io
      - ciliumclusterwidenetworkpolicies.cilium.io
      - ciliumegressgatewaypolicies.cilium.io
      - ciliumendpoints.cilium.io
      - ciliumendpointslices.cilium.io
      - ciliumenvoyconfigs.cilium.io
      - ciliumexternalworkloads.cilium.io
      - ciliumidentities.cilium.io
      - ciliumlocalredirectpolicies.cilium.io
      - ciliumnetworkpolicies.cilium.io
      - ciliumnodes.cilium.io
      - ciliumnodeconfigs.cilium.io
      - ciliumcidrgroups.cilium.io
      - ciliuml2announcementpolicies.cilium.io
      - ciliumpodippools.cilium.io
    resources:
      - customresourcedefinitions
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumpodippools
      - ciliumbgppeeringpolicies
      - ciliumbgpclusterconfigs
      - ciliumbgpnodeconfigoverrides
      - ciliumbgppeerconfigs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumpodippools
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools/status
    verbs:
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-crossplane: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-admin
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-browse: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-browse
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-edit: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-edit
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-rbac-manager
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions/finalizers
    verbs:
      - update
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - escalate
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
    verbs:
      - bind
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
    verbs:
      - '*'
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-view: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-view
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-admin: "true"
  name: crossplane:aggregate-to-admin
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
      - namespaces
    verbs:
      - '*'
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - rolebindings
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-browse: "true"
  name: crossplane:aggregate-to-browse
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-edit: "true"
  name: crossplane:aggregate-to-edit
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-view: "true"
  name: crossplane:aggregate-to-view
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-allowed-provider-permissions: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane:allowed-provider-permissions
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    crossplane.io/scope: system
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-crossplane: "true"
  name: crossplane:system:aggregate-to-crossplane
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
      - customresourcedefinitions/status
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
      - services
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
      - pkg.crossplane.io
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - extensions
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - delete
      - watch
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
rules:
  - apiGroups:
      - ""
      - extensions
      - networking.k8s.io
      - discovery.k8s.io
    resources:
      - ingresses
      - namespaces
      - endpointslices
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-ui
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - componentstatuses
      - endpoints
      - namespaces
      - nodes
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets-unsealer
rules:
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - update
      - delete
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.8.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.13
  name: system:proxmox-cloud-controller-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - create
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - create
      - get
  - apiGroups:
      - ""
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik-traefik
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - nodes
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingressclasses
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.io
    resources:
      - ingressroutes
      - ingressroutetcps
      - ingressrouteudps
      - middlewares
      - middlewaretcps
      - serverstransports
      - serverstransporttcps
      - tlsoptions
      - tlsstores
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-cainjector:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cert-manager-tokenrequest
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-tokenrequest
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-startupapicheck:create-cert
subjects:
  - kind: ServiceAccount
    name: cert-manager-startupapicheck
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-webhook:dynamic-serving
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxmox-csi-plugin-controller
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-controller
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-config-agent
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator-tlsinterception-secrets
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-operator-tlsinterception-secrets
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-tlsinterception-secrets
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-tlsinterception-secrets
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hubble-generate-certs
subjects:
  - kind: ServiceAccount
    name: hubble-generate-certs
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: system:proxmox-cloud-controller-manager:extension-apiserver-authentication-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
  - kind: ServiceAccount
    name: proxmox-cloud-controller-manager
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-key-admin
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-service-proxier
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:authenticated
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-cainjector
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-approve:cert-manager-io
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-approve:cert-manager-io
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-certificates
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificates
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-certificatesigningrequests
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificatesigningrequests
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-challenges
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-challenges
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-clusterissuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-clusterissuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-ingress-shim
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-ingress-shim
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-issuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-issuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-controller-orders
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-orders
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook:subjectaccessreviews
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-webhook:subjectaccessreviews
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium-operator
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane
subjects:
  - kind: ServiceAccount
    name: crossplane
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: crossplane:masters
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-rbac-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-rbac-manager
subjects:
  - kind: ServiceAccount
    name: rbac-manager
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hajimari
subjects:
  - kind: ServiceAccount
    name: hajimari
    namespace: hajimari
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-ui
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hubble-ui
subjects:
  - kind: ServiceAccount
    name: hubble-ui
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oidc-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:olav
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:jiyoung
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxmox-csi-plugin-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxmox-csi-plugin-controller
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-controller
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxmox-csi-plugin-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxmox-csi-plugin-node
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-node
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets-sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sealed-secrets-unsealer
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:proxmox-cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:proxmox-cloud-controller-manager
subjects:
  - kind: ServiceAccount
    name: proxmox-cloud-controller-manager
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik-traefik
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-traefik
subjects:
  - kind: ServiceAccount
    name: traefik
    namespace: traefik
---
apiVersion: v1
data:
  turnserver.conf: "# Coturn TURN SERVER configuration file\n#\n# Boolean values note: where a boolean value is supposed to be used,\n# you can use '0', 'off', 'no', 'false', or 'f' as 'false',\n# and you can use '1', 'on', 'yes', 'true', or 't' as 'true'\n# If the value is missing, then it means 'true' by default.\n#\n\n# Listener interface device (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#listening-device=eth0\n\n# TURN listener port for UDP and TCP (Default: 3478).\n# Note: actually, TLS & DTLS sessions can connect to the\n# \"plain\" TCP & UDP port(s), too - if allowed by configuration.\n#\n#listening-port=3478\n\n# TURN listener port for TLS (Default: 5349).\n# Note: actually, \"plain\" TCP & UDP sessions can connect to the TLS & DTLS\n# port(s), too - if allowed by configuration. The TURN server\n# \"automatically\" recognizes the type of traffic. Actually, two listening\n# endpoints (the \"plain\" one and the \"tls\" one) are equivalent in terms of\n# functionality; but Coturn keeps both endpoints to satisfy the RFC 5766 specs.\n# For secure TCP connections, Coturn currently supports SSL version 3 and\n# TLS version 1.0, 1.1 and 1.2.\n# For secure UDP connections, Coturn supports DTLS version 1.\n#\n#tls-listening-port=5349\n\n# Alternative listening port for UDP and TCP listeners;\n# default (or zero) value means \"listening port plus one\".\n# This is needed for RFC 5780 support\n# (STUN extension specs, NAT behavior discovery). The TURN Server\n# supports RFC 5780 only if it is started with more than one\n# listening IP address of the same family (IPv4 or IPv6).\n# RFC 5780 is supported only by UDP protocol, other protocols\n# are listening to that endpoint only for \"symmetry\".\n#\n#alt-listening-port=0\n\n# Alternative listening port for TLS and DTLS protocols.\n# Default (or zero) value means \"TLS listening port plus one\".\n#\n#alt-tls-listening-port=0\n\n# Some network setups will require using a TCP reverse proxy in front\n# of the STUN server. If the proxy port option is set a single listener\n# is started on the given port that accepts connections using the\n# haproxy proxy protocol v2.\n# (https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)\n#\n#tcp-proxy-port=5555\n\n# Listener IP address of relay server. Multiple listeners can be specified.\n# If no IP(s) specified in the config file or in the command line options,\n# then all IPv4 and IPv6 system IPs will be used for listening.\n#\n#listening-ip=172.17.19.101\n#listening-ip=10.207.21.238\n#listening-ip=2607:f0d0:1002:51::4\n\n# Auxiliary STUN/TURN server listening endpoint.\n# Aux servers have almost full TURN and STUN functionality.\n# The (minor) limitations are:\n#\n# 1) Auxiliary servers do not have alternative ports and\n# they do not support STUN RFC 5780 functionality (CHANGE REQUEST).\n#\n# 2) Auxiliary servers also are never returning ALTERNATIVE-SERVER reply.\n#\n# Valid formats are 1.2.3.4:5555 for IPv4 and [1:2::3:4]:5555 for IPv6.\n#\n# There may be multiple aux-server options, each will be used for listening\n# to client requests.\n#\n#aux-server=172.17.19.110:33478\n#aux-server=[2607:f0d0:1002:51::4]:33478\n\n# (recommended for older Linuxes only)\n# Automatically balance UDP traffic over auxiliary servers (if configured).\n# The load balancing is using the ALTERNATE-SERVER mechanism.\n# The TURN client must support 300 ALTERNATE-SERVER response for this\n# functionality.\n#\n#udp-self-balance\n\n# Relay interface device for relay sockets (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#relay-device=eth1\n\n# Relay address (the local IP address that will be used to relay the\n# packets to the peer).\n# Multiple relay addresses may be used.\n# The same IP(s) can be used as both listening IP(s) and relay IP(s).\n#\n# If no relay IP(s) specified, then the turnserver will apply the default\n# policy: it will decide itself which relay addresses to be used, and it\n# will always be using the client socket IP address as the relay IP address\n# of the TURN session (if the requested relay address family is the same\n# as the family of the client socket).\n#\n#relay-ip=172.17.19.105\n#relay-ip=2607:f0d0:1002:51::5\n\n# For Amazon EC2 users:\n#\n# TURN Server public/private address mapping, if the server is behind NAT.\n# In that situation, if a -X is used in form \"-X <ip>\" then that ip will be reported\n# as relay IP address of all allocations. This scenario works only in a simple case\n# when one single relay address is be used, and no RFC5780 functionality is required.\n# That single relay address must be mapped by NAT to the 'external' IP.\n# The \"external-ip\" value, if not empty, is returned in XOR-RELAYED-ADDRESS field.\n# For that 'external' IP, NAT must forward ports directly (relayed port 12345\n# must be always mapped to the same 'external' port 12345).\n#\n# In more complex case when more than one IP address is involved,\n# that option must be used several times, each entry must\n# have form \"-X <public-ip/private-ip>\", to map all involved addresses.\n# RFC5780 NAT discovery STUN functionality will work correctly,\n# if the addresses are mapped properly, even when the TURN server itself\n# is behind A NAT.\n#\n# By default, this value is empty, and no address mapping is used.\n#\n#external-ip=60.70.80.91\n#\n#OR:\n#\n#external-ip=60.70.80.91/172.17.19.101\n#external-ip=60.70.80.92/172.17.19.102\n\n\n# Number of the relay threads to handle the established connections\n# (in addition to authentication thread and the listener thread).\n# If explicitly set to 0 then application runs relay process in a\n# single thread, in the same thread with the listener process\n# (the authentication thread will still be a separate thread).\n#\n# If this parameter is not set, then the default OS-dependent\n# thread pattern algorithm will be employed. Usually the default\n# algorithm is optimal, so you have to change this option\n# if you want to make some fine tweaks.\n#\n# In the older systems (Linux kernel before 3.9),\n# the number of UDP threads is always one thread per network listening\n# endpoint - including the auxiliary endpoints - unless 0 (zero) or\n# 1 (one) value is set.\n#\n#relay-threads=0\n\n# Lower and upper bounds of the UDP relay endpoints:\n# (default values are 49152 and 65535)\n#\n#min-port=49152\n#max-port=65535\n\n# Uncomment to run TURN server in 'normal' 'moderate' verbose mode.\n# By default the verbose mode is off.\n#verbose\n\n# Uncomment to run TURN server in 'extra' verbose mode.\n# This mode is very annoying and produces lots of output.\n# Not recommended under normal circumstances.\n#\n#Verbose\n\n# Uncomment to use fingerprints in the TURN messages.\n# By default the fingerprints are off.\n#\n#fingerprint\n\n# Uncomment to use long-term credential mechanism.\n# By default no credentials mechanism is used (any user allowed).\n#\n#lt-cred-mech\n\n# This option is the opposite of lt-cred-mech.\n# (TURN Server with no-auth option allows anonymous access).\n# If neither option is defined, and no users are defined,\n# then no-auth is default. If at least one user is defined,\n# in this file, in command line or in usersdb file, then\n# lt-cred-mech is default.\n#\n#no-auth\n\n# Enable prometheus exporter\n# If enabled the turnserver will expose an endpoint with stats on a prometheus format\n# this endpoint is listening on a different port to not conflict with other configurations.\n#\n# You can simply run the turnserver and access the port 9641 and path /metrics\n#\n# For more info on the prometheus exporter and metrics\n# https://prometheus.io/docs/introduction/overview/\n# https://prometheus.io/docs/concepts/data_model/\n#\n#prometheus\n\n# TURN REST API flag.\n# (Time Limited Long Term Credential)\n# Flag that sets a special authorization option that is based upon authentication secret.\n#\n# This feature's purpose is to support \"TURN Server REST API\", see\n# \"TURN REST API\" link in the project's page\n# https://github.com/coturn/coturn/\n#\n# This option is used with timestamp:\n#\n# usercombo -> \"timestamp:userid\"\n# turn user -> usercombo\n# turn password -> base64(hmac(secret key, usercombo))\n#\n# This allows TURN credentials to be accounted for a specific user id.\n# If you don't have a suitable id, then the timestamp alone can be used.\n# This option is enabled by turning on secret-based authentication.\n# The actual value of the secret is defined either by the option static-auth-secret,\n# or can be found in the turn_secret table in the database (see below).\n#\n# Read more about it:\n#  - https://tools.ietf.org/html/draft-uberti-behave-turn-rest-00\n#  - https://www.ietf.org/proceedings/87/slides/slides-87-behave-10.pdf\n#\n# Be aware that use-auth-secret overrides some parts of lt-cred-mech.\n# The use-auth-secret feature depends internally on lt-cred-mech, so if you set\n# this option then it automatically enables lt-cred-mech internally\n# as if you had enabled both.\n#\n# Note that you can use only one auth mechanism at the same time! This is because,\n# both mechanisms conduct username and password validation in different ways.\n#\n# Use either lt-cred-mech or use-auth-secret in the conf\n# to avoid any confusion.\n#\n#use-auth-secret\n\n# 'Static' authentication secret value (a string) for TURN REST API only.\n# If not set, then the turn server\n# will try to use the 'dynamic' value in the turn_secret table\n# in the user database (if present). The database-stored  value can be changed on-the-fly\n# by a separate program, so this is why that mode is considered 'dynamic'.\n#\n#static-auth-secret=north\n\n# Server name used for\n# the oAuth authentication purposes.\n# The default value is the realm name.\n#\n#server-name=blackdow.carleon.gov\n\n# Flag that allows oAuth authentication.\n#\n#oauth\n\n# 'Static' user accounts for the long term credentials mechanism, only.\n# This option cannot be used with TURN REST API.\n# 'Static' user accounts are NOT dynamically checked by the turnserver process,\n# so they can NOT be changed while the turnserver is running.\n#\n#user=username1:key1\n#user=username2:key2\n# OR:\n#user=username1:password1\n#user=username2:password2\n#\n# Keys must be generated by turnadmin utility. The key value depends\n# on user name, realm, and password:\n#\n# Example:\n# $ turnadmin -k -u ninefingers -r north.gov -p youhavetoberealistic\n# Output: 0xbc807ee29df3c9ffa736523fb2c4e8ee\n# ('0x' in the beginning of the key is what differentiates the key from\n# password. If it has 0x then it is a key, otherwise it is a password).\n#\n# The corresponding user account entry in the config file will be:\n#\n#user=ninefingers:0xbc807ee29df3c9ffa736523fb2c4e8ee\n# Or, equivalently, with open clear password (less secure):\n#user=ninefingers:youhavetoberealistic\n#\n\n# SQLite database file name.\n#\n# The default file name is /var/db/turndb or /usr/local/var/db/turndb or\n# /var/lib/turn/turndb.\n#\n#userdb=/var/db/turndb\n\n# PostgreSQL database connection string in the case that you are using PostgreSQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# See http://www.postgresql.org/docs/8.4/static/libpq-connect.html for 8.x PostgreSQL\n# versions connection string format, see\n# http://www.postgresql.org/docs/9.2/static/libpq-connect.html#LIBPQ-CONNSTRING\n# for 9.x and newer connection string formats.\n#\n#psql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> connect_timeout=30\"\n\n# MySQL database connection string in the case that you are using MySQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n#\n# Optional connection string parameters for the secure communications (SSL):\n# ca, capath, cert, key, cipher\n# (see http://dev.mysql.com/doc/refman/5.1/en/ssl-options.html for the\n# command options description).\n#\n# Use the string format below (space separated parameters, all optional):\n#\n#mysql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> port=<port> connect_timeout=<seconds> read_timeout=<seconds>\"\n\n# If you want to use an encrypted password in the MySQL connection string,\n# then set the MySQL password encryption secret key file with this option.\n#\n# Warning: If this option is set, then the mysql password must be set in \"mysql-userdb\" in an encrypted format!\n# If you want to use a cleartext password then do not set this option!\n#\n# This is the file path for the aes encrypted secret key used for password encryption.\n#\n#secret-key-file=/path/\n\n# MongoDB database connection string in the case that you are using MongoDB\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format described at http://hergert.me/docs/mongo-c-driver/mongoc_uri.html\n#\n#mongo-userdb=\"mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\"\n\n# Redis database connection string in the case that you are using Redis\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-userdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Redis status and statistics database connection string, if used (default - empty, no Redis stats DB used).\n# This database keeps allocations status information, and it can be also used for publishing\n# and delivering traffic and allocation event notifications.\n# The connection string has the same parameters as redis-userdb connection string.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-statsdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# The default realm to be used for the users when no explicit\n# origin/realm relationship is found in the database, or if the TURN\n# server is not using any database (just the commands-line settings\n# and the userdb file). Must be used with long-term credentials\n# mechanism or with TURN REST API.\n#\n# Note: If the default realm is not specified, then realm falls back to the host domain name.\n#       If the domain name string is empty, or set to '(None)', then it is initialized as an empty string.\n#\n#realm=mycompany.org\n\n# This flag sets the origin consistency\n# check. Across the session, all requests must have the same\n# main ORIGIN attribute value (if the ORIGIN was\n# initially used by the session).\n#\n#check-origin-consistency\n\n# Per-user allocation quota.\n# default value is 0 (no quota, unlimited number of sessions per user).\n# This option can also be set through the database, for a particular realm.\n#\n#user-quota=0\n\n# Total allocation quota.\n# default value is 0 (no quota).\n# This option can also be set through the database, for a particular realm.\n#\n#total-quota=0\n\n# Max bytes-per-second bandwidth a TURN session is allowed to handle\n# (input and output network streams are treated separately). Anything above\n# that limit will be dropped or temporarily suppressed (within\n# the available buffer limits).\n# This option can also be set through the database, for a particular realm.\n#\n#max-bps=0\n\n#\n# Maximum server capacity.\n# Total bytes-per-second bandwidth the TURN server is allowed to allocate\n# for the sessions, combined (input and output network streams are treated separately).\n#\n#bps-capacity=0\n\n# Uncomment if no UDP client listener is desired.\n# By default UDP client listener is always started.\n#\n#no-udp\n\n# Uncomment if no TCP client listener is desired.\n# By default TCP client listener is always started.\n#\n#no-tcp\n\n# Uncomment if no TLS client listener is desired.\n# By default TLS client listener is always started.\n#\n#no-tls\n\n# Uncomment if no DTLS client listener is desired.\n# By default DTLS client listener is always started.\n#\n#no-dtls\n\n# Uncomment if no UDP relay endpoints are allowed.\n# By default UDP relay endpoints are enabled (like in RFC 5766).\n#\n#no-udp-relay\n\n# Uncomment if no TCP relay endpoints are allowed.\n# By default TCP relay endpoints are enabled (like in RFC 6062).\n#\n#no-tcp-relay\n\n# Uncomment if extra security is desired,\n# with nonce value having a limited lifetime.\n# The nonce value is unique for a session.\n# Set this option to limit the nonce lifetime.\n# Set it to 0 for unlimited lifetime.\n# It defaults to 600 secs (10 min) if no value is provided. After that delay,\n# the client will get 438 error and will have to re-authenticate itself.\n#\n#stale-nonce=600\n\n# Uncomment if you want to set the maximum allocation\n# time before it has to be refreshed.\n# Default is 3600s.\n#\n#max-allocate-lifetime=3600\n\n\n# Uncomment to set the lifetime for the channel.\n# Default value is 600 secs (10 minutes).\n# This value MUST not be changed for production purposes.\n#\n#channel-lifetime=600\n\n# Uncomment to set the permission lifetime.\n# Default to 300 secs (5 minutes).\n# In production this value MUST not be changed,\n# however it can be useful for test purposes.\n#\n#permission-lifetime=300\n\n# Certificate file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#cert=/usr/local/etc/turn_server_cert.pem\n\n# Private key file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#pkey=/usr/local/etc/turn_server_pkey.pem\n\n# Private key file password, if it is in encoded format.\n# This option has no default value.\n#\n#pkey-pwd=...\n\n# Allowed OpenSSL cipher list for TLS/DTLS connections.\n# Default value is \"DEFAULT\".\n#\n#cipher-list=\"DEFAULT\"\n\n# CA file in OpenSSL format.\n# Forces TURN server to verify the client SSL certificates.\n# By default this is not set: there is no default value and the client\n# certificate is not checked.\n#\n# Example:\n#CA-file=/etc/ssh/id_rsa.cert\n\n# Curve name for EC ciphers, if supported by OpenSSL\n# library (TLS and DTLS). The default value is prime256v1,\n# if pre-OpenSSL 1.0.2 is used. With OpenSSL 1.0.2+,\n# an optimal curve will be automatically calculated, if not defined\n# by this option.\n#\n#ec-curve-name=prime256v1\n\n# Use 566 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh566\n\n# Use 1066 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh1066\n\n# Use custom DH TLS key, stored in PEM format in the file.\n# Flags --dh566 and --dh1066 are ignored when the DH key is taken from a file.\n#\n#dh-file=<DH-PEM-file-name>\n\n# Flag to prevent stdout log messages.\n# By default, all log messages go to both stdout and to\n# the configured log file. With this option everything will\n# go to the configured log only (unless the log file itself is stdout).\n#\n#no-stdout-log\n\n# Option to set the log file name.\n# By default, the turnserver tries to open a log file in\n# /var/log, /var/tmp, /tmp and the current directory\n# (Whichever file open operation succeeds first will be used).\n# With this option you can set the definite log file name.\n# The special names are \"stdout\" and \"-\" - they will force everything\n# to the stdout. Also, the \"syslog\" name will force everything to\n# the system log (syslog).\n# In the runtime, the logfile can be reset with the SIGHUP signal\n# to the turnserver process.\n#\n#log-file=/var/tmp/turn.log\n\n# Option to redirect all log output into system log (syslog).\n#\n#syslog\n\n# Set syslog facility for syslog messages\n# Default values is ''.\n#\n#syslog-facility=\"LOG_LOCAL1\"\n\n# This flag means that no log file rollover will be used, and the log file\n# name will be constructed as-is, without PID and date appendage.\n# This option can be used, for example, together with the logrotate tool.\n#\n#simple-log\n\n# Enable full ISO-8601 timestamp in all logs.\n#new-log-timestamp\n\n# Set timestamp format (in strftime(1) format). Depends on new-log-timestamp to be enabled.\n#new-log-timestamp-format \"%FT%T%z\"\n\n# Disabled by default binding logging in verbose log mode to avoid DoS attacks.\n# Enable binding logging and UDP endpoint logs in verbose log mode.\n#log-binding\n\n# Option to set the \"redirection\" mode. The value of this option\n# will be the address of the alternate server for UDP & TCP service in the form of\n# <ip>[:<port>]. The server will send this value in the attribute\n# ALTERNATE-SERVER, with error 300, on ALLOCATE request, to the client.\n# Client will receive only values with the same address family\n# as the client network endpoint address family.\n# See RFC 5389 and RFC 5766 for the description of ALTERNATE-SERVER functionality.\n# The client must use the obtained value for subsequent TURN communications.\n# If more than one --alternate-server option is provided, then the functionality\n# can be more accurately described as \"load-balancing\" than a mere \"redirection\".\n# If the port number is omitted, then the default port\n# number 3478 for the UDP/TCP protocols will be used.\n# Colon (:) characters in IPv6 addresses may conflict with the syntax of\n# the option. To alleviate this conflict, literal IPv6 addresses are enclosed\n# in square brackets in such resource identifiers, for example:\n# [2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478 .\n# Multiple alternate servers can be set. They will be used in the\n# round-robin manner. All servers in the pool are considered of equal weight and\n# the load will be distributed equally. For example, if you have 4 alternate servers,\n# then each server will receive 25% of ALLOCATE requests. A alternate TURN server\n# address can be used more than one time with the alternate-server option, so this\n# can emulate \"weighting\" of the servers.\n#\n# Examples:\n#alternate-server=1.2.3.4:5678\n#alternate-server=11.22.33.44:56789\n#alternate-server=5.6.7.8\n#alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to set alternative server for TLS & DTLS services in form of\n# <ip>:<port>. If the port number is omitted, then the default port\n# number 5349 for the TLS/DTLS protocols will be used. See the previous\n# option for the functionality description.\n#\n# Examples:\n#tls-alternate-server=1.2.3.4:5678\n#tls-alternate-server=11.22.33.44:56789\n#tls-alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to suppress TURN functionality, only STUN requests will be processed.\n# Run as STUN server only, all TURN requests will be ignored.\n# By default, this option is NOT set.\n#\n#stun-only\n\n# Option to hide software version. Enhance security when used in production.\n# Revealing the specific software version of the agent through the\n# SOFTWARE attribute might allow them to become more vulnerable to\n# attacks against software that is known to contain security holes.\n# Implementers SHOULD make usage of the SOFTWARE attribute a\n# configurable option (https://tools.ietf.org/html/rfc5389#section-16.1.2)\n#\n#no-software-attribute\n\n# Option to suppress STUN functionality, only TURN requests will be processed.\n# Run as TURN server only, all STUN requests will be ignored.\n# By default, this option is NOT set.\n#\n#no-stun\n\n# This is the timestamp/username separator symbol (character) in TURN REST API.\n# The default value is ':'.\n#\n#rest-api-separator=:\n\n# Flag that can be used to allow peers on the loopback addresses (127.x.x.x and ::1).\n# This is an extra security measure.\n#\n# (To avoid any security issue that allowing loopback access may raise,\n# the no-loopback-peers option is replaced by allow-loopback-peers.)\n#\n# Allow it only for testing in a development environment!\n# In production it adds a possible security vulnerability, so for security reasons\n# it is not allowed using it together with empty cli-password.\n#\n#allow-loopback-peers\n\n# Flag that can be used to disallow peers on well-known broadcast addresses (224.0.0.0 and above, and FFXX:*).\n# This is an extra security measure.\n#\n#no-multicast-peers\n\n# Option to set the max time, in seconds, allowed for full allocation establishment.\n# Default is 60 seconds.\n#\n#max-allocate-timeout=60\n\n# Option to allow or ban specific ip addresses or ranges of ip addresses.\n# If an ip address is specified as both allowed and denied, then the ip address is\n# considered to be allowed. This is useful when you wish to ban a range of ip\n# addresses, except for a few specific ips within that range.\n#\n# This can be used when you do not want users of the turn server to be able to access\n# machines reachable by the turn server, but would otherwise be unreachable from the\n# internet (e.g. when the turn server is sitting behind a NAT)\n#\n# Examples:\n# denied-peer-ip=83.166.64.0-83.166.95.255\n# allowed-peer-ip=83.166.68.45\n\n# File name to store the pid of the process.\n# Default is /var/run/turnserver.pid (if superuser account is used) or\n# /var/tmp/turnserver.pid .\n#\n#pidfile=\"/var/run/turnserver.pid\"\n\n# Require authentication of the STUN Binding request.\n# By default, the clients are allowed anonymous access to the STUN Binding functionality.\n#\n#secure-stun\n\n# Mobility with ICE (MICE) specs support.\n#\n#mobility\n\n# Allocate Address Family according (DEPRECATED and will be removed in favor of allocation-default-address-family)\n# If enabled then TURN server allocates address family according  the TURN\n# Client <=> Server communication address family.\n# (By default Coturn works according RFC 6156.)\n# !!Warning: Enabling this option breaks RFC6156 section-4.2 (violates use default IPv4)!!\n#\n#keep-address-family\n\n# TURN server allocates address family according TURN client requested address family.\n# If address family not requested explicitly by the client, then it falls back to this default.\n# The standard RFC explicitly define that this default must be IPv4, \n# so use other option values with care! \n# Possible values: \"ipv4\" or \"ipv6\" or \"keep\" \n# \"keep\" sets the allocation default address family according to \n# the TURN client allocation request connection address family.\n#\n#allocation-default-address-family=\"ipv4\"\n#allocation-default-address-family=\"ipv4\"\n\n# User name to run the process. After the initialization, the turnserver process\n# will attempt to change the current user ID to that user.\n#\n#proc-user=<user-name>\n\n# Group name to run the process. After the initialization, the turnserver process\n# will attempt to change the current group ID to that group.\n#\n#proc-group=<group-name>\n\n# Turn OFF the CLI support.\n# By default it is always ON.\n# See also options cli-ip and cli-port.\n#\n#no-cli\n\n#Local system IP address to be used for CLI server endpoint. Default value\n# is 127.0.0.1.\n#\n#cli-ip=127.0.0.1\n\n# CLI server port. Default is 5766.\n#\n#cli-port=5766\n\n# CLI access password. Default is empty (no password).\n# For the security reasons, it is recommended that you use the encrypted\n# form of the password (see the -P command in the turnadmin utility).\n#\n# Secure form for password 'qwerty':\n#\n#cli-password=$5$79a316b350311570$81df9cfb9af7f5e5a76eada31e7097b663a0670f99a3c07ded3f1c8e59c5658a\n#\n# Or unsecure form for the same password:\n#\n#cli-password=qwerty\n\n# Enable Web-admin support on https. By default it is Disabled.\n# If it is enabled it also enables a http a simple static banner page\n# with a small reminder that the admin page is available only on https.\n# Not supported if no-tls option used\n#\n#web-admin\n\n# Local system IP address to be used for Web-admin server endpoint. Default value is 127.0.0.1.\n#\n#web-admin-ip=127.0.0.1\n\n# Web-admin server port. Default is 8080.\n#\n#web-admin-port=8080\n\n# Web-admin server listen on STUN/TURN worker threads\n# By default it is disabled for security reasons! (Not recommended in any production environment!)\n#\n#web-admin-listen-on-workers\n\n# Redirect ACME, i.e. HTTP GET requests matching '^/.well-known/acme-challenge/(.*)' to '<URL>$1'.\n# Default is '', i.e. no special handling for such requests.\n#\n#acme-redirect=http://redirectserver/.well-known/acme-challenge/\n\n# Server relay. NON-STANDARD AND DANGEROUS OPTION.\n# Only for those applications when you want to run\n# server applications on the relay endpoints.\n# This option eliminates the IP permissions check on\n# the packets incoming to the relay endpoints.\n#\n#server-relay\n\n# Maximum number of output sessions in ps CLI command.\n# This value can be changed on-the-fly in CLI. The default value is 256.\n#\n#cli-max-output-sessions\n\n# Set network engine type for the process (for internal purposes).\n#\n#ne=[1|2|3]\n\n# Do not allow an TLS/DTLS version of protocol\n#\n#no-tlsv1\n#no-tlsv1_1\n#no-tlsv1_2\n\n# Disable RFC5780 (NAT behavior discovery).\n#\n# Originally, if there are more than one listener address from the same\n# address family, then by default the NAT behavior discovery feature enabled.\n# This option disables the original behavior, because the NAT behavior\n# discovery adds extra attributes to response, and this increase the\n# possibility of an amplification attack.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-rfc5780\n\n# Disable handling old STUN Binding requests and disable MAPPED-ADDRESS\n# attribute in binding response (use only the XOR-MAPPED-ADDRESS).\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-stun-backward-compatibility\n\n# Only send RESPONSE-ORIGIN attribute in binding response if RFC5780 is enabled.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nresponse-origin-only-with-rfc5780"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r2
    helm.sh/chart: coturn-1.0.1
  name: coturn
  namespace: coturn
---
apiVersion: v1
data:
  valkey-default.conf: "# Valkey configuration file example.\n#\n# Note that in order to read the configuration file, Valkey must be\n# started with the file path as first argument:\n#\n# ./valkey-server /path/to/valkey.conf\n\n# Note on units: when memory size is needed, it is possible to specify\n# it in the usual form of 1k 5GB 4M and so forth:\n#\n# 1k => 1000 bytes\n# 1kb => 1024 bytes\n# 1m => 1000000 bytes\n# 1mb => 1024*1024 bytes\n# 1g => 1000000000 bytes\n# 1gb => 1024*1024*1024 bytes\n#\n# units are case insensitive so 1GB 1Gb 1gB are all the same.\n\n################################## INCLUDES ###################################\n\n# Include one or more other config files here.  This is useful if you\n# have a standard template that goes to all Valkey servers but also need\n# to customize a few per-server settings.  Include files can include\n# other files, so use this wisely.\n#\n# Note that option \"include\" won't be rewritten by command \"CONFIG REWRITE\"\n# from admin or Valkey Sentinel. Since Valkey always uses the last processed\n# line as value of a configuration directive, you'd better put includes\n# at the beginning of this file to avoid overwriting config change at runtime.\n#\n# If instead you are interested in using includes to override configuration\n# options, it is better to use include as the last line.\n#\n# Included paths may contain wildcards. All files matching the wildcards will\n# be included in alphabetical order.\n# Note that if an include path contains a wildcards but no files match it when\n# the server is started, the include statement will be ignored and no error will\n# be emitted.  It is safe, therefore, to include wildcard files from empty\n# directories.\n#\n# include /path/to/local.conf\n# include /path/to/other.conf\n# include /path/to/fragments/*.conf\n#\n\n################################## MODULES #####################################\n\n# Load modules at startup. If the server is not able to load modules\n# it will abort. It is possible to use multiple loadmodule directives.\n#\n# loadmodule /path/to/my_module.so\n# loadmodule /path/to/other_module.so\n\n################################## NETWORK #####################################\n\n# By default, if no \"bind\" configuration directive is specified, Valkey listens\n# for connections from all available network interfaces on the host machine.\n# It is possible to listen to just one or multiple selected interfaces using\n# the \"bind\" configuration directive, followed by one or more IP addresses.\n# Each address can be prefixed by \"-\", which means that valkey will not fail to\n# start if the address is not available. Being not available only refers to\n# addresses that does not correspond to any network interface. Addresses that\n# are already in use will always fail, and unsupported protocols will always BE\n# silently skipped.\n#\n# Examples:\n#\n# bind 192.168.1.100 10.0.0.1     # listens on two specific IPv4 addresses\n# bind 127.0.0.1 ::1              # listens on loopback IPv4 and IPv6\n# bind * -::*                     # like the default, all available interfaces\n#\n# ~~~ WARNING ~~~ If the computer running Valkey is directly exposed to the\n# internet, binding to all the interfaces is dangerous and will expose the\n# instance to everybody on the internet. So by default we uncomment the\n# following bind directive, that will force Valkey to listen only on the\n# IPv4 and IPv6 (if available) loopback interface addresses (this means Valkey\n# will only be able to accept client connections from the same host that it is\n# running on).\n#\n# IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES\n# COMMENT OUT THE FOLLOWING LINE.\n#\n# You will also need to set a password unless you explicitly disable protected\n# mode.\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nbind 127.0.0.1 -::1\n\n# By default, outgoing connections (from replica to primary, from Sentinel to\n# instances, cluster bus, etc.) are not bound to a specific local address. In\n# most cases, this means the operating system will handle that based on routing\n# and the interface through which the connection goes out.\n#\n# Using bind-source-addr it is possible to configure a specific address to bind\n# to, which may also affect how the connection gets routed.\n#\n# Example:\n#\n# bind-source-addr 10.0.0.1\n\n# Protected mode is a layer of security protection, in order to avoid that\n# Valkey instances left open on the internet are accessed and exploited.\n#\n# When protected mode is on and the default user has no password, the server\n# only accepts local connections from the IPv4 address (127.0.0.1), IPv6 address\n# (::1) or Unix domain sockets.\n#\n# By default protected mode is enabled. You should disable it only if\n# you are sure you want clients from other hosts to connect to Valkey\n# even if no authentication is configured.\nprotected-mode yes\n\n# Valkey uses default hardened security configuration directives to reduce the\n# attack surface on innocent users. Therefore, several sensitive configuration\n# directives are immutable, and some potentially-dangerous commands are blocked.\n#\n# Configuration directives that control files that Valkey writes to (e.g., 'dir'\n# and 'dbfilename') and that aren't usually modified during runtime\n# are protected by making them immutable.\n#\n# Commands that can increase the attack surface of Valkey and that aren't usually\n# called by users are blocked by default.\n#\n# These can be exposed to either all connections or just local ones by setting\n# each of the configs listed below to either of these values:\n#\n# no    - Block for any connection (remain immutable)\n# yes   - Allow for any connection (no protection)\n# local - Allow only for local connections. Ones originating from the\n#         IPv4 address (127.0.0.1), IPv6 address (::1) or Unix domain sockets.\n#\n# enable-protected-configs no\n# enable-debug-command no\n# enable-module-command no\n\n# Accept connections on the specified port, default is 6379 (IANA #815344).\n# If port 0 is specified Valkey will not listen on a TCP socket.\nport 6379\n\n# TCP listen() backlog.\n#\n# In high requests-per-second environments you need a high backlog in order\n# to avoid slow clients connection issues. Note that the Linux kernel\n# will silently truncate it to the value of /proc/sys/net/core/somaxconn so\n# make sure to raise both the value of somaxconn and tcp_max_syn_backlog\n# in order to get the desired effect.\ntcp-backlog 511\n\n# Unix socket.\n#\n# Specify the path for the Unix socket that will be used to listen for\n# incoming connections. There is no default, so Valkey will not listen\n# on a unix socket when not specified.\n#\n# unixsocket /run/valkey.sock\n# unixsocketperm 700\n\n# Close the connection after a client is idle for N seconds (0 to disable)\ntimeout 0\n\n# TCP keepalive.\n#\n# If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence\n# of communication. This is useful for two reasons:\n#\n# 1) Detect dead peers.\n# 2) Force network equipment in the middle to consider the connection to be\n#    alive.\n#\n# On Linux, the specified value (in seconds) is the period used to send ACKs.\n# Note that to close the connection the double of the time is needed.\n# On other kernels the period depends on the kernel configuration.\n#\n# A reasonable value for this option is 300 seconds\ntcp-keepalive 300\n\n# Apply OS-specific mechanism to mark the listening socket with the specified\n# ID, to support advanced routing and filtering capabilities.\n#\n# On Linux, the ID represents a connection mark.\n# On FreeBSD, the ID represents a socket cookie ID.\n# On OpenBSD, the ID represents a route table ID.\n#\n# The default value is 0, which implies no marking is required.\n# socket-mark-id 0\n\n################################# TLS/SSL #####################################\n\n# By default, TLS/SSL is disabled. To enable it, the \"tls-port\" configuration\n# directive can be used to define TLS-listening ports. To enable TLS on the\n# default port, use:\n#\n# port 0\n# tls-port 6379\n\n# Configure a X.509 certificate and private key to use for authenticating the\n# server to connected clients, primarys or cluster peers.  These files should be\n# PEM formatted.\n#\n# tls-cert-file valkey.crt\n# tls-key-file valkey.key\n#\n# If the key file is encrypted using a passphrase, it can be included here\n# as well.\n#\n# tls-key-file-pass secret\n\n# Normally Valkey uses the same certificate for both server functions (accepting\n# connections) and client functions (replicating from a primary, establishing\n# cluster bus connections, etc.).\n#\n# Sometimes certificates are issued with attributes that designate them as\n# client-only or server-only certificates. In that case it may be desired to use\n# different certificates for incoming (server) and outgoing (client)\n# connections. To do that, use the following directives:\n#\n# tls-client-cert-file client.crt\n# tls-client-key-file client.key\n#\n# If the key file is encrypted using a passphrase, it can be included here\n# as well.\n#\n# tls-client-key-file-pass secret\n\n# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange,\n# required by older versions of OpenSSL (<3.0). Newer versions do not require\n# this configuration and recommend against it.\n#\n# tls-dh-params-file valkey.dh\n\n# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL\n# clients and peers.  Valkey requires an explicit configuration of at least one\n# of these, and will not implicitly use the system wide configuration.\n#\n# tls-ca-cert-file ca.crt\n# tls-ca-cert-dir /etc/ssl/certs\n\n# By default, clients (including replica servers) on a TLS port are required\n# to authenticate using valid client side certificates.\n#\n# If \"no\" is specified, client certificates are not required and not accepted.\n# If \"optional\" is specified, client certificates are accepted and must be\n# valid if provided, but are not required.\n#\n# tls-auth-clients no\n# tls-auth-clients optional\n\n# By default, a Valkey replica does not attempt to establish a TLS connection\n# with its primary.\n#\n# Use the following directive to enable TLS on replication links.\n#\n# tls-replication yes\n\n# By default, the Valkey Cluster bus uses a plain TCP connection. To enable\n# TLS for the bus protocol, use the following directive:\n#\n# tls-cluster yes\n\n# By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended\n# that older formally deprecated versions are kept disabled to reduce the attack surface.\n# You can explicitly specify TLS versions to support.\n# Allowed values are case insensitive and include \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\",\n# \"TLSv1.3\" (OpenSSL >= 1.1.1) or any combination.\n# To enable only TLSv1.2 and TLSv1.3, use:\n#\n# tls-protocols \"TLSv1.2 TLSv1.3\"\n\n# Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information\n# about the syntax of this string.\n#\n# Note: this configuration applies only to <= TLSv1.2.\n#\n# tls-ciphers DEFAULT:!MEDIUM\n\n# Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more\n# information about the syntax of this string, and specifically for TLSv1.3\n# ciphersuites.\n#\n# tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256\n\n# When choosing a cipher, use the server's preference instead of the client\n# preference. By default, the server follows the client's preference.\n#\n# tls-prefer-server-ciphers yes\n\n# By default, TLS session caching is enabled to allow faster and less expensive\n# reconnections by clients that support it. Use the following directive to disable\n# caching.\n#\n# tls-session-caching no\n\n# Change the default number of TLS sessions cached. A zero value sets the cache\n# to unlimited size. The default size is 20480.\n#\n# tls-session-cache-size 5000\n\n# Change the default timeout of cached TLS sessions. The default timeout is 300\n# seconds.\n#\n# tls-session-cache-timeout 60\n\n################################# GENERAL #####################################\n\n# By default Valkey does not run as a daemon. Use 'yes' if you need it.\n# Note that Valkey will write a pid file in /var/run/valkey.pid when daemonized.\n# When Valkey is supervised by upstart or systemd, this parameter has no impact.\ndaemonize no\n\n# If you run Valkey from upstart or systemd, Valkey can interact with your\n# supervision tree. Options:\n#   supervised no      - no supervision interaction\n#   supervised upstart - signal upstart by putting Valkey into SIGSTOP mode\n#                        requires \"expect stop\" in your upstart job config\n#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET\n#                        on startup, and updating Valkey status on a regular\n#                        basis.\n#   supervised auto    - detect upstart or systemd method based on\n#                        UPSTART_JOB or NOTIFY_SOCKET environment variables\n# Note: these supervision methods only signal \"process is ready.\"\n#       They do not enable continuous pings back to your supervisor.\n#\n# The default is \"no\". To run under upstart/systemd, you can simply uncomment\n# the line below:\n#\n# supervised auto\n\n# If a pid file is specified, Valkey writes it where specified at startup\n# and removes it at exit.\n#\n# When the server runs non daemonized, no pid file is created if none is\n# specified in the configuration. When the server is daemonized, the pid file\n# is used even if not specified, defaulting to \"/var/run/valkey.pid\".\n#\n# Creating a pid file is best effort: if Valkey is not able to create it\n# nothing bad happens, the server will start and run normally.\n#\n# Note that on modern Linux systems \"/run/valkey.pid\" is more conforming\n# and should be used instead.\npidfile /opt/bitnami/valkey/tmp/valkey_6379.pid\n\n# Specify the server verbosity level.\n# This can be one of:\n# debug (a lot of information, useful for development/testing)\n# verbose (many rarely useful info, but not a mess like the debug level)\n# notice (moderately verbose, what you want in production probably)\n# warning (only very important / critical messages are logged)\nloglevel notice\n\n# Specify the log file name. Also the empty string can be used to force\n# Valkey to log on the standard output. Note that if you use standard\n# output for logging but daemonize, logs will be sent to /dev/null\nlogfile \"\"\n\n# To enable logging to the system logger, just set 'syslog-enabled' to yes,\n# and optionally update the other syslog parameters to suit your needs.\n# syslog-enabled no\n\n# Specify the syslog identity.\n# syslog-ident valkey\n\n# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.\n# syslog-facility local0\n\n# To disable the built in crash log, which will possibly produce cleaner core\n# dumps when they are needed, uncomment the following:\n#\n# crash-log-enabled no\n\n# To disable the fast memory check that's run as part of the crash log, which\n# will possibly let valkey terminate sooner, uncomment the following:\n#\n# crash-memcheck-enabled no\n\n# Set the number of databases. The default database is DB 0, you can select\n# a different one on a per-connection basis using SELECT <dbid> where\n# dbid is a number between 0 and 'databases'-1\ndatabases 16\n\n# By default Valkey shows an ASCII art logo only when started to log to the\n# standard output and if the standard output is a TTY and syslog logging is\n# disabled. Basically this means that normally a logo is displayed only in\n# interactive sessions.\n#\n# However it is possible to force the pre-4.0 behavior and always show a\n# ASCII art logo in startup logs by setting the following option to yes.\nalways-show-logo yes\n\n# By default, Valkey modifies the process title (as seen in 'top' and 'ps') to\n# provide some runtime information. It is possible to disable this and leave\n# the process name as executed by setting the following to no.\nset-proc-title yes\n\n# When changing the process title, Valkey uses the following template to construct\n# the modified title.\n#\n# Template variables are specified in curly brackets. The following variables are\n# supported:\n#\n# {title}           Name of process as executed if parent, or type of child process.\n# {listen-addr}     Bind address or '*' followed by TCP or TLS port listening on, or\n#                   Unix socket if only that's available.\n# {server-mode}     Special mode, i.e. \"[sentinel]\" or \"[cluster]\".\n# {port}            TCP port listening on, or 0.\n# {tls-port}        TLS port listening on, or 0.\n# {unixsocket}      Unix domain socket listening on, or \"\".\n# {config-file}     Name of configuration file used.\n#\nproc-title-template \"{title} {listen-addr} {server-mode}\"\n\n################################ SNAPSHOTTING  ################################\n\n# Save the DB to disk.\n#\n# save <seconds> <changes> [<seconds> <changes> ...]\n#\n# Valkey will save the DB if the given number of seconds elapsed and it\n# surpassed the given number of write operations against the DB.\n#\n# Snapshotting can be completely disabled with a single empty string argument\n# as in following example:\n#\n# save \"\"\n#\n# Unless specified otherwise, by default Valkey will save the DB:\n#   * After 3600 seconds (an hour) if at least 1 change was performed\n#   * After 300 seconds (5 minutes) if at least 100 changes were performed\n#   * After 60 seconds if at least 10000 changes were performed\n#\n# You can set these explicitly by uncommenting the following line.\n#\n# save 3600 1 300 100 60 10000\nsave 900 1 300 10 60 10000\n# By default Valkey will stop accepting writes if RDB snapshots are enabled\n# (at least one save point) and the latest background save failed.\n# This will make the user aware (in a hard way) that data is not persisting\n# on disk properly, otherwise chances are that no one will notice and some\n# disaster will happen.\n#\n# If the background saving process will start working again Valkey will\n# automatically allow writes again.\n#\n# However if you have setup your proper monitoring of the Valkey server\n# and persistence, you may want to disable this feature so that Valkey will\n# continue to work as usual even if there are problems with disk,\n# permissions, and so forth.\nstop-writes-on-bgsave-error yes\n\n# Compress string objects using LZF when dump .rdb databases?\n# By default compression is enabled as it's almost always a win.\n# If you want to save some CPU in the saving child set it to 'no' but\n# the dataset will likely be bigger if you have compressible values or keys.\nrdbcompression yes\n\n# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.\n# This makes the format more resistant to corruption but there is a performance\n# hit to pay (around 10%) when saving and loading RDB files, so you can disable it\n# for maximum performances.\n#\n# RDB files created with checksum disabled have a checksum of zero that will\n# tell the loading code to skip the check.\nrdbchecksum yes\n\n# Enables or disables full sanitization checks for ziplist and listpack etc when\n# loading an RDB or RESTORE payload. This reduces the chances of a assertion or\n# crash later on while processing commands.\n# Options:\n#   no         - Never perform full sanitization\n#   yes        - Always perform full sanitization\n#   clients    - Perform full sanitization only for user connections.\n#                Excludes: RDB files, RESTORE commands received from the primary\n#                connection, and client connections which have the\n#                skip-sanitize-payload ACL flag.\n# The default should be 'clients' but since it currently affects cluster\n# resharding via MIGRATE, it is temporarily set to 'no' by default.\n#\n# sanitize-dump-payload no\n\n# The filename where to dump the DB\ndbfilename dump.rdb\n\n# Remove RDB files used by replication in instances without persistence\n# enabled. By default this option is disabled, however there are environments\n# where for regulations or other security concerns, RDB files persisted on\n# disk by primarys in order to feed replicas, or stored on disk by replicas\n# in order to load them for the initial synchronization, should be deleted\n# ASAP. Note that this option ONLY WORKS in instances that have both AOF\n# and RDB persistence disabled, otherwise is completely ignored.\n#\n# An alternative (and sometimes better) way to obtain the same effect is\n# to use diskless replication on both primary and replicas instances. However\n# in the case of replicas, diskless is not always an option.\nrdb-del-sync-files no\n\n# The working directory.\n#\n# The DB will be written inside this directory, with the filename specified\n# above using the 'dbfilename' configuration directive.\n#\n# The Append Only File will also be created inside this directory.\n#\n# Note that you must specify a directory here, not a file name.\ndir /bitnami/valkey/data\n\n################################# REPLICATION #################################\n\n# Primary-Replica replication. Use replicaof to make a Valkey instance a copy of\n# another Valkey server. A few things to understand ASAP about Valkey replication.\n#\n#   +------------------+      +---------------+\n#   |      Primary      | ---> |    Replica    |\n#   | (receive writes) |      |  (exact copy) |\n#   +------------------+      +---------------+\n#\n# 1) Valkey replication is asynchronous, but you can configure a primary to\n#    stop accepting writes if it appears to be not connected with at least\n#    a given number of replicas.\n# 2) Valkey replicas are able to perform a partial resynchronization with the\n#    primary if the replication link is lost for a relatively small amount of\n#    time. You may want to configure the replication backlog size (see the next\n#    sections of this file) with a sensible value depending on your needs.\n# 3) Replication is automatic and does not need user intervention. After a\n#    network partition replicas automatically try to reconnect to primarys\n#    and resynchronize with them.\n#\n# replicaof <primaryip> <primaryport>\n\n# If the primary is password protected (using the \"requirepass\" configuration\n# directive below) it is possible to tell the replica to authenticate before\n# starting the replication synchronization process, otherwise the primary will\n# refuse the replica request.\n#\n# primaryauth <primary-password>\n#\n# However this is not enough if you are using Valkey ACLs (for Valkey version\n# 6 or greater), and the default user is not capable of running the PSYNC\n# command and/or other commands needed for replication. In this case it's\n# better to configure a special user to use with replication, and specify the\n# primaryuser configuration as such:\n#\n# primaryuser <username>\n#\n# When primaryuser is specified, the replica will authenticate against its\n# primary using the new AUTH form: AUTH <username> <password>.\n\n# When a replica loses its connection with the primary, or when the replication\n# is still in progress, the replica can act in two different ways:\n#\n# 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will\n#    still reply to client requests, possibly with out of date data, or the\n#    data set may just be empty if this is the first synchronization.\n#\n# 2) If replica-serve-stale-data is set to 'no' the replica will reply with error\n#    \"MASTERDOWN Link with MASTER is down and replica-serve-stale-data is set to 'no'\"\n#    to all data access commands, excluding commands such as:\n#    INFO, REPLICAOF, AUTH, SHUTDOWN, REPLCONF, ROLE, CONFIG, SUBSCRIBE,\n#    UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, COMMAND, POST,\n#    HOST and LATENCY.\n#\nreplica-serve-stale-data yes\n\n# You can configure a replica instance to accept writes or not. Writing against\n# a replica instance may be useful to store some ephemeral data (because data\n# written on a replica will be easily deleted after resync with the primary) but\n# may also cause problems if clients are writing to it because of a\n# misconfiguration.\n#\n# In Vakey by default replicas are read-only.\n#\n# Note: read only replicas are not designed to be exposed to untrusted clients\n# on the internet. It's just a protection layer against misuse of the instance.\n# Still a read only replica exports by default all the administrative commands\n# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve\n# security of read only replicas using 'rename-command' to shadow all the\n# administrative / dangerous commands.\nreplica-read-only yes\n\n# Replication SYNC strategy: disk or socket.\n#\n# New replicas and reconnecting replicas that are not able to continue the\n# replication process just receiving differences, need to do what is called a\n# \"full synchronization\". An RDB file is transmitted from the primary to the\n# replicas.\n#\n# The transmission can happen in two different ways:\n#\n# 1) Disk-backed: The Valkey primary creates a new process that writes the RDB\n#                 file on disk. Later the file is transferred by the parent\n#                 process to the replicas incrementally.\n# 2) Diskless: The Valkey primary creates a new process that directly writes the\n#              RDB file to replica sockets, without touching the disk at all.\n#\n# With disk-backed replication, while the RDB file is generated, more replicas\n# can be queued and served with the RDB file as soon as the current child\n# producing the RDB file finishes its work. With diskless replication instead\n# once the transfer starts, new replicas arriving will be queued and a new\n# transfer will start when the current one terminates.\n#\n# When diskless replication is used, the primary waits a configurable amount of\n# time (in seconds) before starting the transfer in the hope that multiple\n# replicas will arrive and the transfer can be parallelized.\n#\n# With slow disks and fast (large bandwidth) networks, diskless replication\n# works better.\nrepl-diskless-sync no\n\n# When diskless replication is enabled, it is possible to configure the delay\n# the server waits in order to spawn the child that transfers the RDB via socket\n# to the replicas.\n#\n# This is important since once the transfer starts, it is not possible to serve\n# new replicas arriving, that will be queued for the next RDB transfer, so the\n# server waits a delay in order to let more replicas arrive.\n#\n# The delay is specified in seconds, and by default is 5 seconds. To disable\n# it entirely just set it to 0 seconds and the transfer will start ASAP.\nrepl-diskless-sync-delay 5\n\n# When diskless replication is enabled with a delay, it is possible to let\n# the replication start before the maximum delay is reached if the maximum\n# number of replicas expected have connected. Default of 0 means that the\n# maximum is not defined and Valkey will wait the full delay.\nrepl-diskless-sync-max-replicas 0\n\n# -----------------------------------------------------------------------------\n# WARNING: RDB diskless load is experimental. Since in this setup the replica\n# does not immediately store an RDB on disk, it may cause data loss during\n# failovers. RDB diskless load + Valkey modules not handling I/O reads may also\n# cause Valkey to abort in case of I/O errors during the initial synchronization\n# stage with the primary. Use only if you know what you are doing.\n# -----------------------------------------------------------------------------\n#\n# Replica can load the RDB it reads from the replication link directly from the\n# socket, or store the RDB to a file and read that file after it was completely\n# received from the primary.\n#\n# In many cases the disk is slower than the network, and storing and loading\n# the RDB file may increase replication time (and even increase the primary's\n# Copy on Write memory and replica buffers).\n# However, parsing the RDB file directly from the socket may mean that we have\n# to flush the contents of the current database before the full rdb was\n# received. For this reason we have the following options:\n#\n# \"disabled\"    - Don't use diskless load (store the rdb file to the disk first)\n# \"on-empty-db\" - Use diskless load only when it is completely safe.\n# \"swapdb\"      - Keep current db contents in RAM while parsing the data directly\n#                 from the socket. Replicas in this mode can keep serving current\n#                 data set while replication is in progress, except for cases where\n#                 they can't recognize primary as having a data set from same\n#                 replication history.\n#                 Note that this requires sufficient memory, if you don't have it,\n#                 you risk an OOM kill.\nrepl-diskless-load disabled\n\n# Primary send PINGs to its replicas in a predefined interval. It's possible to\n# change this interval with the repl_ping_replica_period option. The default\n# value is 10 seconds.\n#\n# repl-ping-replica-period 10\n\n# The following option sets the replication timeout for:\n#\n# 1) Bulk transfer I/O during SYNC, from the point of view of replica.\n# 2) Primary timeout from the point of view of replicas (data, pings).\n# 3) Replica timeout from the point of view of primary nodes (REPLCONF ACK pings).\n#\n# It is important to make sure that this value is greater than the value\n# specified for repl-ping-replica-period otherwise a timeout will be detected\n# every time there is low traffic between the primary and the replica. The default\n# value is 60 seconds.\n#\n# repl-timeout 60\n\n# Disable TCP_NODELAY on the replica socket after SYNC?\n#\n# If you select \"yes\" Valkey will use a smaller number of TCP packets and\n# less bandwidth to send data to replicas. But this can add a delay for\n# the data to appear on the replica side, up to 40 milliseconds with\n# Linux kernels using a default configuration.\n#\n# If you select \"no\" the delay for data to appear on the replica side will\n# be reduced but more bandwidth will be used for replication.\n#\n# By default we optimize for low latency, but in very high traffic conditions\n# or when the primary and replicas are many hops away, turning this to \"yes\" may\n# be a good idea.\nrepl-disable-tcp-nodelay no\n\n# Set the replication backlog size. The backlog is a buffer that accumulates\n# replica data when replicas are disconnected for some time, so that when a\n# replica wants to reconnect again, often a full resync is not needed, but a\n# partial resync is enough, just passing the portion of data the replica\n# missed while disconnected.\n#\n# The bigger the replication backlog, the longer the replica can endure the\n# disconnect and later be able to perform a partial resynchronization.\n#\n# The backlog is only allocated if there is at least one replica connected.\n#\n# repl-backlog-size 1mb\n\n# After a primary has no connected replicas for some time, the backlog will be\n# freed. The following option configures the amount of seconds that need to\n# elapse, starting from the time the last replica disconnected, for the backlog\n# buffer to be freed.\n#\n# Note that replicas never free the backlog for timeout, since they may be\n# promoted to primarys later, and should be able to correctly \"partially\n# resynchronize\" with other replicas: hence they should always accumulate backlog.\n#\n# A value of 0 means to never release the backlog.\n#\n# repl-backlog-ttl 3600\n\n# The replica priority is an integer number published by Valkey in the INFO\n# output. It is used by Valkey Sentinel in order to select a replica to promote\n# into a primary if the primary is no longer working correctly.\n#\n# A replica with a low priority number is considered better for promotion, so\n# for instance if there are three replicas with priority 10, 100, 25 Sentinel\n# will pick the one with priority 10, that is the lowest.\n#\n# However a special priority of 0 marks the replica as not able to perform the\n# role of primary, so a replica with priority of 0 will never be selected by\n# Valkey Sentinel for promotion.\n#\n# By default the priority is 100.\nreplica-priority 100\n\n# The propagation error behavior controls how Valkey will behave when it is\n# unable to handle a command being processed in the replication stream from a primary\n# or processed while reading from an AOF file. Errors that occur during propagation\n# are unexpected, and can cause data inconsistency. However, there are edge cases\n# in earlier versions of Valkey where it was possible for the server to replicate or persist\n# commands that would fail on future versions. For this reason the default behavior\n# is to ignore such errors and continue processing commands.\n#\n# If an application wants to ensure there is no data divergence, this configuration\n# should be set to 'panic' instead. The value can also be set to 'panic-on-replicas'\n# to only panic when a replica encounters an error on the replication stream. One of\n# these two panic values will become the default value in the future once there are\n# sufficient safety mechanisms in place to prevent false positive crashes.\n#\n# propagation-error-behavior ignore\n\n# Replica ignore disk write errors controls the behavior of a replica when it is\n# unable to persist a write command received from its primary to disk. By default,\n# this configuration is set to 'no' and will crash the replica in this condition.\n# It is not recommended to change this default, however in order to be compatible\n# with older versions of Valkey this config can be toggled to 'yes' which will just\n# log a warning and execute the write command it got from the primary.\n#\n# replica-ignore-disk-write-errors no\n\n# -----------------------------------------------------------------------------\n# By default, Valkey Sentinel includes all replicas in its reports. A replica\n# can be excluded from Valkey Sentinel's announcements. An unannounced replica\n# will be ignored by the 'sentinel replicas <primary>' command and won't be\n# exposed to Valkey Sentinel's clients.\n#\n# This option does not change the behavior of replica-priority. Even with\n# replica-announced set to 'no', the replica can be promoted to primary. To\n# prevent this behavior, set replica-priority to 0.\n#\n# replica-announced yes\n\n# It is possible for a primary to stop accepting writes if there are less than\n# N replicas connected, having a lag less or equal than M seconds.\n#\n# The N replicas need to be in \"online\" state.\n#\n# The lag in seconds, that must be <= the specified value, is calculated from\n# the last ping received from the replica, that is usually sent every second.\n#\n# This option does not GUARANTEE that N replicas will accept the write, but\n# will limit the window of exposure for lost writes in case not enough replicas\n# are available, to the specified number of seconds.\n#\n# For example to require at least 3 replicas with a lag <= 10 seconds use:\n#\n# min-replicas-to-write 3\n# min-replicas-max-lag 10\n#\n# Setting one or the other to 0 disables the feature.\n#\n# By default min-replicas-to-write is set to 0 (feature disabled) and\n# min-replicas-max-lag is set to 10.\n\n# A Valkey primary is able to list the address and port of the attached\n# replicas in different ways. For example the \"INFO replication\" section\n# offers this information, which is used, among other tools, by\n# Valkey Sentinel in order to discover replica instances.\n# Another place where this info is available is in the output of the\n# \"ROLE\" command of a primary.\n#\n# The listed IP address and port normally reported by a replica is\n# obtained in the following way:\n#\n#   IP: The address is auto detected by checking the peer address\n#   of the socket used by the replica to connect with the primary.\n#\n#   Port: The port is communicated by the replica during the replication\n#   handshake, and is normally the port that the replica is using to\n#   listen for connections.\n#\n# However when port forwarding or Network Address Translation (NAT) is\n# used, the replica may actually be reachable via different IP and port\n# pairs. The following two options can be used by a replica in order to\n# report to its primary a specific set of IP and port, so that both INFO\n# and ROLE will report those values.\n#\n# There is no need to use both the options if you need to override just\n# the port or the IP address.\n#\n# replica-announce-ip 5.5.5.5\n# replica-announce-port 1234\n\n############################### KEYS TRACKING #################################\n\n# Valkey implements server assisted support for client side caching of values.\n# This is implemented using an invalidation table that remembers, using\n# a radix key indexed by key name, what clients have which keys. In turn\n# this is used in order to send invalidation messages to clients. \n#\n# When tracking is enabled for a client, all the read only queries are assumed\n# to be cached: this will force Valkey to store information in the invalidation\n# table. When keys are modified, such information is flushed away, and\n# invalidation messages are sent to the clients. However if the workload is\n# heavily dominated by reads, Valkey could use more and more memory in order\n# to track the keys fetched by many clients.\n#\n# For this reason it is possible to configure a maximum fill value for the\n# invalidation table. By default it is set to 1M of keys, and once this limit\n# is reached, Valkey will start to evict keys in the invalidation table\n# even if they were not modified, just to reclaim memory: this will in turn\n# force the clients to invalidate the cached values. Basically the table\n# maximum size is a trade off between the memory you want to spend server\n# side to track information about who cached what, and the ability of clients\n# to retain cached objects in memory.\n#\n# If you set the value to 0, it means there are no limits, and Valkey will\n# retain as many keys as needed in the invalidation table.\n# In the \"stats\" INFO section, you can find information about the number of\n# keys in the invalidation table at every given moment.\n#\n# Note: when key tracking is used in broadcasting mode, no memory is used\n# in the server side so this setting is useless.\n#\n# tracking-table-max-keys 1000000\n\n################################## SECURITY ###################################\n\n# Warning: since Valkey is pretty fast, an outside user can try up to\n# 1 million passwords per second against a modern box. This means that you\n# should use very strong passwords, otherwise they will be very easy to break.\n# Note that because the password is really a shared secret between the client\n# and the server, and should not be memorized by any human, the password\n# can be easily a long string from /dev/urandom or whatever, so by using a\n# long and unguessable password no brute force attack will be possible.\n\n# Valkey ACL users are defined in the following format:\n#\n#   user <username> ... acl rules ...\n#\n# For example:\n#\n#   user worker +@list +@connection ~jobs:* on >ffa9203c493aa99\n#\n# The special username \"default\" is used for new connections. If this user\n# has the \"nopass\" rule, then new connections will be immediately authenticated\n# as the \"default\" user without the need of any password provided via the\n# AUTH command. Otherwise if the \"default\" user is not flagged with \"nopass\"\n# the connections will start in not authenticated state, and will require\n# AUTH (or the HELLO command AUTH option) in order to be authenticated and\n# start to work.\n#\n# The ACL rules that describe what a user can do are the following:\n#\n#  on           Enable the user: it is possible to authenticate as this user.\n#  off          Disable the user: it's no longer possible to authenticate\n#               with this user, however the already authenticated connections\n#               will still work.\n#  skip-sanitize-payload    RESTORE dump-payload sanitization is skipped.\n#  sanitize-payload         RESTORE dump-payload is sanitized (default).\n#  +<command>   Allow the execution of that command.\n#               May be used with `|` for allowing subcommands (e.g \"+config|get\")\n#  -<command>   Disallow the execution of that command.\n#               May be used with `|` for blocking subcommands (e.g \"-config|set\")\n#  +@<category> Allow the execution of all the commands in such category\n#               with valid categories are like @admin, @set, @sortedset, ...\n#               and so forth, see the full list in the server.c file where\n#               the Valkey command table is described and defined.\n#               The special category @all means all the commands, but currently\n#               present in the server, and that will be loaded in the future\n#               via modules.\n#  +<command>|first-arg  Allow a specific first argument of an otherwise\n#                        disabled command. It is only supported on commands with\n#                        no sub-commands, and is not allowed as negative form\n#                        like -SELECT|1, only additive starting with \"+\". This\n#                        feature is deprecated and may be removed in the future.\n#  allcommands  Alias for +@all. Note that it implies the ability to execute\n#               all the future commands loaded via the modules system.\n#  nocommands   Alias for -@all.\n#  ~<pattern>   Add a pattern of keys that can be mentioned as part of\n#               commands. For instance ~* allows all the keys. The pattern\n#               is a glob-style pattern like the one of KEYS.\n#               It is possible to specify multiple patterns.\n# %R~<pattern>  Add key read pattern that specifies which keys can be read\n#               from.\n# %W~<pattern>  Add key write pattern that specifies which keys can be\n#               written to.\n#  allkeys      Alias for ~*\n#  resetkeys    Flush the list of allowed keys patterns.\n#  &<pattern>   Add a glob-style pattern of Pub/Sub channels that can be\n#               accessed by the user. It is possible to specify multiple channel\n#               patterns.\n#  allchannels  Alias for &*\n#  resetchannels            Flush the list of allowed channel patterns.\n#  ><password>  Add this password to the list of valid password for the user.\n#               For example >mypass will add \"mypass\" to the list.\n#               This directive clears the \"nopass\" flag (see later).\n#  <<password>  Remove this password from the list of valid passwords.\n#  nopass       All the set passwords of the user are removed, and the user\n#               is flagged as requiring no password: it means that every\n#               password will work against this user. If this directive is\n#               used for the default user, every new connection will be\n#               immediately authenticated with the default user without\n#               any explicit AUTH command required. Note that the \"resetpass\"\n#               directive will clear this condition.\n#  resetpass    Flush the list of allowed passwords. Moreover removes the\n#               \"nopass\" status. After \"resetpass\" the user has no associated\n#               passwords and there is no way to authenticate without adding\n#               some password (or setting it as \"nopass\" later).\n#  reset        Performs the following actions: resetpass, resetkeys, off,\n#               -@all. The user returns to the same state it has immediately\n#               after its creation.\n# (<options>)   Create a new selector with the options specified within the\n#               parentheses and attach it to the user. Each option should be\n#               space separated. The first character must be ( and the last\n#               character must be ).\n# clearselectors            Remove all of the currently attached selectors.\n#                           Note this does not change the \"root\" user permissions,\n#                           which are the permissions directly applied onto the\n#                           user (outside the parentheses).\n#\n# ACL rules can be specified in any order: for instance you can start with\n# passwords, then flags, or key patterns. However note that the additive\n# and subtractive rules will CHANGE MEANING depending on the ordering.\n# For instance see the following example:\n#\n#   user alice on +@all -DEBUG ~* >somepassword\n#\n# This will allow \"alice\" to use all the commands with the exception of the\n# DEBUG command, since +@all added all the commands to the set of the commands\n# alice can use, and later DEBUG was removed. However if we invert the order\n# of two ACL rules the result will be different:\n#\n#   user alice on -DEBUG +@all ~* >somepassword\n#\n# Now DEBUG was removed when alice had yet no commands in the set of allowed\n# commands, later all the commands are added, so the user will be able to\n# execute everything.\n#\n# Basically ACL rules are processed left-to-right.\n#\n# The following is a list of command categories and their meanings:\n# * keyspace - Writing or reading from keys, databases, or their metadata\n#     in a type agnostic way. Includes DEL, RESTORE, DUMP, RENAME, EXISTS, DBSIZE,\n#     KEYS, EXPIRE, TTL, FLUSHALL, etc. Commands that may modify the keyspace,\n#     key or metadata will also have `write` category. Commands that only read\n#     the keyspace, key or metadata will have the `read` category.\n# * read - Reading from keys (values or metadata). Note that commands that don't\n#     interact with keys, will not have either `read` or `write`.\n# * write - Writing to keys (values or metadata)\n# * admin - Administrative commands. Normal applications will never need to use\n#     these. Includes REPLICAOF, CONFIG, DEBUG, SAVE, MONITOR, ACL, SHUTDOWN, etc.\n# * dangerous - Potentially dangerous (each should be considered with care for\n#     various reasons). This includes FLUSHALL, MIGRATE, RESTORE, SORT, KEYS,\n#     CLIENT, DEBUG, INFO, CONFIG, SAVE, REPLICAOF, etc.\n# * connection - Commands affecting the connection or other connections.\n#     This includes AUTH, SELECT, COMMAND, CLIENT, ECHO, PING, etc.\n# * blocking - Potentially blocking the connection until released by another\n#     command.\n# * fast - Fast O(1) commands. May loop on the number of arguments, but not the\n#     number of elements in the key.\n# * slow - All commands that are not Fast.\n# * pubsub - PUBLISH / SUBSCRIBE related\n# * transaction - WATCH / MULTI / EXEC related commands.\n# * scripting - Scripting related.\n# * set - Data type: sets related.\n# * sortedset - Data type: zsets related.\n# * list - Data type: lists related.\n# * hash - Data type: hashes related.\n# * string - Data type: strings related.\n# * bitmap - Data type: bitmaps related.\n# * hyperloglog - Data type: hyperloglog related.\n# * geo - Data type: geo related.\n# * stream - Data type: streams related.\n#\n# For more information about ACL configuration please refer to\n# the Valkey web site at https://valkey.io/docs/topics/acl/\n\n# ACL LOG\n#\n# The ACL Log tracks failed commands and authentication events associated\n# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked\n# by ACLs. The ACL Log is stored in memory. You can reclaim memory with\n# ACL LOG RESET. Define the maximum entry length of the ACL Log below.\nacllog-max-len 128\n\n# Using an external ACL file\n#\n# Instead of configuring users here in this file, it is possible to use\n# a stand-alone file just listing users. The two methods cannot be mixed:\n# if you configure users here and at the same time you activate the external\n# ACL file, the server will refuse to start.\n#\n# The format of the external ACL user file is exactly the same as the\n# format that is used inside valkey.conf to describe users.\n#\n# aclfile /etc/valkey/users.acl\n\n# The option effect will be just setting the password for the default user. \n# Clients will still authenticate using AUTH <password> as usually, or more \n# explicitly with AUTH default <password> if they follow the new protocol: \n# both will work.\n#\n# The requirepass is not compatible with aclfile option and the ACL LOAD\n# command, these will cause requirepass to be ignored.\n#\n# requirepass foobared\n\n# New users are initialized with restrictive permissions by default, via the\n# equivalent of this ACL rule 'off resetkeys -@all'. It\n# is possible to manage access to Pub/Sub channels with ACL rules as well. The\n# default Pub/Sub channels permission if new users is controlled by the\n# acl-pubsub-default configuration directive, which accepts one of these values:\n#\n# allchannels: grants access to all Pub/Sub channels\n# resetchannels: revokes access to all Pub/Sub channels\n#\n# acl-pubsub-default resetchannels\n\n# Command renaming (DEPRECATED).\n#\n# ------------------------------------------------------------------------\n# WARNING: avoid using this option if possible. Instead use ACLs to remove\n# commands from the default user, and put them only in some admin user you\n# create for administrative purposes.\n# ------------------------------------------------------------------------\n#\n# It is possible to change the name of dangerous commands in a shared\n# environment. For instance the CONFIG command may be renamed into something\n# hard to guess so that it will still be available for internal-use tools\n# but not available for general clients.\n#\n# Example:\n#\n# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# It is also possible to completely kill a command by renaming it into\n# an empty string:\n#\n# rename-command CONFIG \"\"\n#\n# Please note that changing the name of commands that are logged into the\n# AOF file or transmitted to replicas may cause problems.\n\n################################### CLIENTS ####################################\n\n# Set the max number of connected clients at the same time. By default\n# this limit is set to 10000 clients, however if the Valkey server is not\n# able to configure the process file limit to allow for the specified limit\n# the max number of allowed clients is set to the current file limit\n# minus 32 (as Valkey reserves a few file descriptors for internal uses).\n#\n# Once the limit is reached Valkey will close all the new connections sending\n# an error 'max number of clients reached'.\n#\n# IMPORTANT: When Valkey Cluster is used, the max number of connections is also\n# shared with the cluster bus: every node in the cluster will use two\n# connections, one incoming and another outgoing. It is important to size the\n# limit accordingly in case of very large clusters.\n#\n# maxclients 10000\n\n############################## MEMORY MANAGEMENT ################################\n\n# Set a memory usage limit to the specified amount of bytes.\n# When the memory limit is reached Valkey will try to remove keys\n# according to the eviction policy selected (see maxmemory-policy).\n#\n# If Valkey can't remove keys according to the policy, or if the policy is\n# set to 'noeviction', Valkey will start to reply with errors to commands\n# that would use more memory, like SET, LPUSH, and so on, and will continue\n# to reply to read-only commands like GET.\n#\n# This option is usually useful when using Valkey as an LRU or LFU cache, or to\n# set a hard memory limit for an instance (using the 'noeviction' policy).\n#\n# WARNING: If you have replicas attached to an instance with maxmemory on,\n# the size of the output buffers needed to feed the replicas are subtracted\n# from the used memory count, so that network problems / resyncs will\n# not trigger a loop where keys are evicted, and in turn the output\n# buffer of replicas is full with DELs of keys evicted triggering the deletion\n# of more keys, and so forth until the database is completely emptied.\n#\n# In short... if you have replicas attached it is suggested that you set a lower\n# limit for maxmemory so that there is some free RAM on the system for replica\n# output buffers (but this is not needed if the policy is 'noeviction').\n#\n# maxmemory <bytes>\n\n# MAXMEMORY POLICY: how Valkey will select what to remove when maxmemory\n# is reached. You can select one from the following behaviors:\n#\n# volatile-lru -> Evict using approximated LRU, only keys with an expire set.\n# allkeys-lru -> Evict any key using approximated LRU.\n# volatile-lfu -> Evict using approximated LFU, only keys with an expire set.\n# allkeys-lfu -> Evict any key using approximated LFU.\n# volatile-random -> Remove a random key having an expire set.\n# allkeys-random -> Remove a random key, any key.\n# volatile-ttl -> Remove the key with the nearest expire time (minor TTL)\n# noeviction -> Don't evict anything, just return an error on write operations.\n#\n# LRU means Least Recently Used\n# LFU means Least Frequently Used\n#\n# Both LRU, LFU and volatile-ttl are implemented using approximated\n# randomized algorithms.\n#\n# Note: with any of the above policies, when there are no suitable keys for\n# eviction, Valkey will return an error on write operations that require\n# more memory. These are usually commands that create new keys, add data or\n# modify existing keys. A few examples are: SET, INCR, HSET, LPUSH, SUNIONSTORE,\n# SORT (due to the STORE argument), and EXEC (if the transaction includes any\n# command that requires memory).\n#\n# The default is:\n#\n# maxmemory-policy noeviction\n\n# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated\n# algorithms (in order to save memory), so you can tune it for speed or\n# accuracy. By default Valkey will check five keys and pick the one that was\n# used least recently, you can change the sample size using the following\n# configuration directive.\n#\n# The default of 5 produces good enough results. 10 Approximates very closely\n# true LRU but costs more CPU. 3 is faster but not very accurate.\n#\n# maxmemory-samples 5\n\n# Eviction processing is designed to function well with the default setting.\n# If there is an unusually large amount of write traffic, this value may need to\n# be increased.  Decreasing this value may reduce latency at the risk of\n# eviction processing effectiveness\n#   0 = minimum latency, 10 = default, 100 = process without regard to latency\n#\n# maxmemory-eviction-tenacity 10\n\n# By default a replica will ignore its maxmemory setting\n# (unless it is promoted to primary after a failover or manually). It means\n# that the eviction of keys will be just handled by the primary, sending the\n# DEL commands to the replica as keys evict in the primary side.\n#\n# This behavior ensures that primarys and replicas stay consistent, and is usually\n# what you want, however if your replica is writable, or you want the replica\n# to have a different memory setting, and you are sure all the writes performed\n# to the replica are idempotent, then you may change this default (but be sure\n# to understand what you are doing).\n#\n# Note that since the replica by default does not evict, it may end using more\n# memory than the one set via maxmemory (there are certain buffers that may\n# be larger on the replica, or data structures may sometimes take more memory\n# and so forth). So make sure you monitor your replicas and make sure they\n# have enough memory to never hit a real out-of-memory condition before the\n# primary hits the configured maxmemory setting.\n#\n# replica-ignore-maxmemory yes\n\n# Valkey reclaims expired keys in two ways: upon access when those keys are\n# found to be expired, and also in background, in what is called the\n# \"active expire key\". The key space is slowly and interactively scanned\n# looking for expired keys to reclaim, so that it is possible to free memory\n# of keys that are expired and will never be accessed again in a short time.\n#\n# The default effort of the expire cycle will try to avoid having more than\n# ten percent of expired keys still in memory, and will try to avoid consuming\n# more than 25% of total memory and to add latency to the system. However\n# it is possible to increase the expire \"effort\" that is normally set to\n# \"1\", to a greater value, up to the value \"10\". At its maximum value the\n# system will use more CPU, longer cycles (and technically may introduce\n# more latency), and will tolerate less already expired keys still present\n# in the system. It's a tradeoff between memory, CPU and latency.\n#\n# active-expire-effort 1\n\n############################# LAZY FREEING ####################################\n\n# Valkey has two primitives to delete keys. One is called DEL and is a blocking\n# deletion of the object. It means that the server stops processing new commands\n# in order to reclaim all the memory associated with an object in a synchronous\n# way. If the key deleted is associated with a small object, the time needed\n# in order to execute the DEL command is very small and comparable to most other\n# O(1) or O(log_N) commands in Valkey. However if the key is associated with an\n# aggregated value containing millions of elements, the server can block for\n# a long time (even seconds) in order to complete the operation.\n#\n# For the above reasons Valkey also offers non blocking deletion primitives\n# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and\n# FLUSHDB commands, in order to reclaim memory in background. Those commands\n# are executed in constant time. Another thread will incrementally free the\n# object in the background as fast as possible.\n#\n# DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.\n# It's up to the design of the application to understand when it is a good\n# idea to use one or the other. However the Valkey server sometimes has to\n# delete keys or flush the whole database as a side effect of other operations.\n# Specifically Valkey deletes objects independently of a user call in the\n# following scenarios:\n#\n# 1) On eviction, because of the maxmemory and maxmemory policy configurations,\n#    in order to make room for new data, without going over the specified\n#    memory limit.\n# 2) Because of expire: when a key with an associated time to live (see the\n#    EXPIRE command) must be deleted from memory.\n# 3) Because of a side effect of a command that stores data on a key that may\n#    already exist. For example the RENAME command may delete the old key\n#    content when it is replaced with another one. Similarly SUNIONSTORE\n#    or SORT with STORE option may delete existing keys. The SET command\n#    itself removes any old content of the specified key in order to replace\n#    it with the specified string.\n# 4) During replication, when a replica performs a full resynchronization with\n#    its primary, the content of the whole database is removed in order to\n#    load the RDB file just transferred.\n#\n# In all the above cases the default is to delete objects in a blocking way,\n# like if DEL was called. However you can configure each case specifically\n# in order to instead release memory in a non-blocking way like if UNLINK\n# was called, using the following configuration directives.\n\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\n\n# It is also possible, for the case when to replace the user code DEL calls\n# with UNLINK calls is not easy, to modify the default behavior of the DEL\n# command to act exactly like UNLINK, using the following configuration\n# directive:\n\nlazyfree-lazy-user-del no\n\n# FLUSHDB, FLUSHALL, SCRIPT FLUSH and FUNCTION FLUSH support both asynchronous and synchronous\n# deletion, which can be controlled by passing the [SYNC|ASYNC] flags into the\n# commands. When neither flag is passed, this directive will be used to determine\n# if the data should be deleted asynchronously.\n\nlazyfree-lazy-user-flush no\n\n################################ THREADED I/O #################################\n\n# Valkey is mostly single threaded, however there are certain threaded\n# operations such as UNLINK, slow I/O accesses and other things that are\n# performed on side threads.\n#\n# Now it is also possible to handle Valkey clients socket reads and writes\n# in different I/O threads. Since especially writing is so slow, normally\n# Valkey users use pipelining in order to speed up the Valkey performances per\n# core, and spawn multiple instances in order to scale more. Using I/O\n# threads it is possible to easily speedup two times Valkey without resorting\n# to pipelining nor sharding of the instance.\n#\n# By default threading is disabled, we suggest enabling it only in machines\n# that have at least 4 or more cores, leaving at least one spare core.\n# Using more than 8 threads is unlikely to help much. We also recommend using\n# threaded I/O only if you actually have performance problems, with Valkey\n# instances being able to use a quite big percentage of CPU time, otherwise\n# there is no point in using this feature.\n#\n# So for instance if you have a four cores boxes, try to use 2 or 3 I/O\n# threads, if you have a 8 cores, try to use 6 threads. In order to\n# enable I/O threads use the following configuration directive:\n#\n# io-threads 4\n#\n# Setting io-threads to 1 will just use the main thread as usual.\n# When I/O threads are enabled, we only use threads for writes, that is\n# to thread the write(2) syscall and transfer the client buffers to the\n# socket. However it is also possible to enable threading of reads and\n# protocol parsing using the following configuration directive, by setting\n# it to yes:\n#\n# io-threads-do-reads no\n#\n# Usually threading reads doesn't help much.\n#\n# NOTE 1: This configuration directive cannot be changed at runtime via\n# CONFIG SET. Also, this feature currently does not work when SSL is\n# enabled.\n#\n# NOTE 2: If you want to test the Valkey speedup using valkey-benchmark, make\n# sure you also run the benchmark itself in threaded mode, using the\n# --threads option to match the number of Valkey threads, otherwise you'll not\n# be able to notice the improvements.\n\n############################ KERNEL OOM CONTROL ##############################\n\n# On Linux, it is possible to hint the kernel OOM killer on what processes\n# should be killed first when out of memory.\n#\n# Enabling this feature makes Valkey actively control the oom_score_adj value\n# for all its processes, depending on their role. The default scores will\n# attempt to have background child processes killed before all others, and\n# replicas killed before primarys.\n#\n# Valkey supports these options:\n#\n# no:       Don't make changes to oom-score-adj (default).\n# yes:      Alias to \"relative\" see below.\n# absolute: Values in oom-score-adj-values are written as is to the kernel.\n# relative: Values are used relative to the initial value of oom_score_adj when\n#           the server starts and are then clamped to a range of -1000 to 1000.\n#           Because typically the initial value is 0, they will often match the\n#           absolute values.\noom-score-adj no\n\n# When oom-score-adj is used, this directive controls the specific values used\n# for primary, replica and background child processes. Values range -2000 to\n# 2000 (higher means more likely to be killed).\n#\n# Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)\n# can freely increase their value, but not decrease it below its initial\n# settings. This means that setting oom-score-adj to \"relative\" and setting the\n# oom-score-adj-values to positive values will always succeed.\noom-score-adj-values 0 200 800\n\n\n#################### KERNEL transparent hugepage CONTROL ######################\n\n# Usually the kernel Transparent Huge Pages control is set to \"madvise\" or\n# or \"never\" by default (/sys/kernel/mm/transparent_hugepage/enabled), in which\n# case this config has no effect. On systems in which it is set to \"always\",\n# valkey will attempt to disable it specifically for the valkey process in order\n# to avoid latency problems specifically with fork(2) and CoW.\n# If for some reason you prefer to keep it enabled, you can set this config to\n# \"no\" and the kernel global to \"always\".\n\ndisable-thp yes\n\n############################## APPEND ONLY MODE ###############################\n\n# By default Valkey asynchronously dumps the dataset on disk. This mode is\n# good enough in many applications, but an issue with the Valkey process or\n# a power outage may result into a few minutes of writes lost (depending on\n# the configured save points).\n#\n# The Append Only File is an alternative persistence mode that provides\n# much better durability. For instance using the default data fsync policy\n# (see later in the config file) Valkey can lose just one second of writes in a\n# dramatic event like a server power outage, or a single write if something\n# wrong with the Valkey process itself happens, but the operating system is\n# still running correctly.\n#\n# AOF and RDB persistence can be enabled at the same time without problems.\n# If the AOF is enabled on startup Valkey will load the AOF, that is the file\n# with the better durability guarantees.\n#\n# Please check https://valkey.io/docs/topics/persistence/ for more information.\n\nappendonly no\n\n# The base name of the append only file.\n#\n# Valkey 7 and newer use a set of append-only files to persist the dataset\n# and changes applied to it. There are two basic types of files in use:\n#\n# - Base files, which are a snapshot representing the complete state of the\n#   dataset at the time the file was created. Base files can be either in\n#   the form of RDB (binary serialized) or AOF (textual commands).\n# - Incremental files, which contain additional commands that were applied\n#   to the dataset following the previous file.\n#\n# In addition, manifest files are used to track the files and the order in\n# which they were created and should be applied.\n#\n# Append-only file names are created by Valkey following a specific pattern.\n# The file name's prefix is based on the 'appendfilename' configuration\n# parameter, followed by additional information about the sequence and type.\n#\n# For example, if appendfilename is set to appendonly.aof, the following file\n# names could be derived:\n#\n# - appendonly.aof.1.base.rdb as a base file.\n# - appendonly.aof.1.incr.aof, appendonly.aof.2.incr.aof as incremental files.\n# - appendonly.aof.manifest as a manifest file.\n\nappendfilename \"appendonly.aof\"\n\n# For convenience, Valkey stores all persistent append-only files in a dedicated\n# directory. The name of the directory is determined by the appenddirname\n# configuration parameter.\n\nappenddirname \"appendonlydir\"\n\n# The fsync() call tells the Operating System to actually write data on disk\n# instead of waiting for more data in the output buffer. Some OS will really flush\n# data on disk, some other OS will just try to do it ASAP.\n#\n# Valkey supports three different modes:\n#\n# no: don't fsync, just let the OS flush the data when it wants. Faster.\n# always: fsync after every write to the append only log. Slow, Safest.\n# everysec: fsync only one time every second. Compromise.\n#\n# The default is \"everysec\", as that's usually the right compromise between\n# speed and data safety. It's up to you to understand if you can relax this to\n# \"no\" that will let the operating system flush the output buffer when\n# it wants, for better performances (but if you can live with the idea of\n# some data loss consider the default persistence mode that's snapshotting),\n# or on the contrary, use \"always\" that's very slow but a bit safer than\n# everysec.\n#\n# If unsure, use \"everysec\".\n\n# appendfsync always\nappendfsync everysec\n# appendfsync no\n\n# When the AOF fsync policy is set to always or everysec, and a background\n# saving process (a background save or AOF log background rewriting) is\n# performing a lot of I/O against the disk, in some Linux configurations\n# Valkey may block too long on the fsync() call. Note that there is no fix for\n# this currently, as even performing fsync in a different thread will block\n# our synchronous write(2) call.\n#\n# In order to mitigate this problem it's possible to use the following option\n# that will prevent fsync() from being called in the main process while a\n# BGSAVE or BGREWRITEAOF is in progress.\n#\n# This means that while another child is saving, the durability of Valkey is\n# the same as \"appendfsync no\". In practical terms, this means that it is\n# possible to lose up to 30 seconds of log in the worst scenario (with the\n# default Linux settings).\n#\n# If you have latency problems turn this to \"yes\". Otherwise leave it as\n# \"no\" that is the safest pick from the point of view of durability.\n\nno-appendfsync-on-rewrite no\n\n# Automatic rewrite of the append only file.\n# Valkey is able to automatically rewrite the log file implicitly calling\n# BGREWRITEAOF when the AOF log size grows by the specified percentage.\n#\n# This is how it works: Valkey remembers the size of the AOF file after the\n# latest rewrite (if no rewrite has happened since the restart, the size of\n# the AOF at startup is used).\n#\n# This base size is compared to the current size. If the current size is\n# bigger than the specified percentage, the rewrite is triggered. Also\n# you need to specify a minimal size for the AOF file to be rewritten, this\n# is useful to avoid rewriting the AOF file even if the percentage increase\n# is reached but it is still pretty small.\n#\n# Specify a percentage of zero in order to disable the automatic AOF\n# rewrite feature.\n\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n# An AOF file may be found to be truncated at the end during the Valkey\n# startup process, when the AOF data gets loaded back into memory.\n# This may happen when the system where Valkey is running\n# crashes, especially when an ext4 filesystem is mounted without the\n# data=ordered option (however this can't happen when Valkey itself\n# crashes or aborts but the operating system still works correctly).\n#\n# Valkey can either exit with an error when this happens, or load as much\n# data as possible (the default now) and start if the AOF file is found\n# to be truncated at the end. The following option controls this behavior.\n#\n# If aof-load-truncated is set to yes, a truncated AOF file is loaded and\n# the Valkey server starts emitting a log to inform the user of the event.\n# Otherwise if the option is set to no, the server aborts with an error\n# and refuses to start. When the option is set to no, the user requires\n# to fix the AOF file using the \"valkey-check-aof\" utility before to restart\n# the server.\n#\n# Note that if the AOF file will be found to be corrupted in the middle\n# the server will still exit with an error. This option only applies when\n# Valkey will try to read more data from the AOF file but not enough bytes\n# will be found.\naof-load-truncated yes\n\n# Valkey can create append-only base files in either RDB or AOF formats. Using\n# the RDB format is always faster and more efficient, and disabling it is only\n# supported for backward compatibility purposes.\naof-use-rdb-preamble yes\n\n# Valkey supports recording timestamp annotations in the AOF to support restoring\n# the data from a specific point-in-time. However, using this capability changes\n# the AOF format in a way that may not be compatible with existing AOF parsers.\naof-timestamp-enabled no\n\n################################ SHUTDOWN #####################################\n\n# Maximum time to wait for replicas when shutting down, in seconds.\n#\n# During shut down, a grace period allows any lagging replicas to catch up with\n# the latest replication offset before the primary exists. This period can\n# prevent data loss, especially for deployments without configured disk backups.\n#\n# The 'shutdown-timeout' value is the grace period's duration in seconds. It is\n# only applicable when the instance has replicas. To disable the feature, set\n# the value to 0.\n#\n# shutdown-timeout 10\n\n# When Valkey receives a SIGINT or SIGTERM, shutdown is initiated and by default\n# an RDB snapshot is written to disk in a blocking operation if save points are configured.\n# The options used on signaled shutdown can include the following values:\n# default:  Saves RDB snapshot only if save points are configured.\n#           Waits for lagging replicas to catch up.\n# save:     Forces a DB saving operation even if no save points are configured.\n# nosave:   Prevents DB saving operation even if one or more save points are configured.\n# now:      Skips waiting for lagging replicas.\n# force:    Ignores any errors that would normally prevent the server from exiting.\n#\n# Any combination of values is allowed as long as \"save\" and \"nosave\" are not set simultaneously.\n# Example: \"nosave force now\"\n#\n# shutdown-on-sigint default\n# shutdown-on-sigterm default\n\n################ NON-DETERMINISTIC LONG BLOCKING COMMANDS #####################\n\n# Maximum time in milliseconds for EVAL scripts, functions and in some cases\n# modules' commands before Valkey can start processing or rejecting other clients.\n#\n# If the maximum execution time is reached Valkey will start to reply to most\n# commands with a BUSY error.\n#\n# In this state Valkey will only allow a handful of commands to be executed.\n# For instance, SCRIPT KILL, FUNCTION KILL, SHUTDOWN NOSAVE and possibly some\n# module specific 'allow-busy' commands.\n#\n# SCRIPT KILL and FUNCTION KILL will only be able to stop a script that did not\n# yet call any write commands, so SHUTDOWN NOSAVE may be the only way to stop\n# the server in the case a write command was already issued by the script when\n# the user doesn't want to wait for the natural termination of the script.\n#\n# The default is 5 seconds. It is possible to set it to 0 or a negative value\n# to disable this mechanism (uninterrupted execution). Note that in the past\n# this config had a different name, which is now an alias, so both of these do\n# the same:\nlua-time-limit 5000\n# busy-reply-threshold 5000\n\n################################ VALKEY CLUSTER  ###############################\n\n# Normal Valkey instances can't be part of a Valkey Cluster; only nodes that are\n# started as cluster nodes can. In order to start a Valkey instance as a\n# cluster node enable the cluster support uncommenting the following:\n#\ncluster-enabled yes\n\n# Every cluster node has a cluster configuration file. This file is not\n# intended to be edited by hand. It is created and updated by Valkey nodes.\n# Every Valkey Cluster node requires a different cluster configuration file.\n# Make sure that instances running in the same system do not have\n# overlapping cluster configuration file names.\n#\ncluster-config-file /bitnami/valkey/data/nodes.conf\n\n# Cluster node timeout is the amount of milliseconds a node must be unreachable\n# for it to be considered in failure state.\n# Most other internal time limits are a multiple of the node timeout.\n#\n# cluster-node-timeout 15000\n\n# The cluster port is the port that the cluster bus will listen for inbound connections on. When set\n# to the default value, 0, it will be bound to the command port + 10000. Setting this value requires\n# you to specify the cluster bus port when executing cluster meet.\n# cluster-port 0\n\n# A replica of a failing primary will avoid to start a failover if its data\n# looks too old.\n#\n# There is no simple way for a replica to actually have an exact measure of\n# its \"data age\", so the following two checks are performed:\n#\n# 1) If there are multiple replicas able to failover, they exchange messages\n#    in order to try to give an advantage to the replica with the best\n#    replication offset (more data from the primary processed).\n#    Replicas will try to get their rank by offset, and apply to the start\n#    of the failover a delay proportional to their rank.\n#\n# 2) Every single replica computes the time of the last interaction with\n#    its primary. This can be the last ping or command received (if the primary\n#    is still in the \"connected\" state), or the time that elapsed since the\n#    disconnection with the primary (if the replication link is currently down).\n#    If the last interaction is too old, the replica will not try to failover\n#    at all.\n#\n# The point \"2\" can be tuned by user. Specifically a replica will not perform\n# the failover if, since the last interaction with the primary, the time\n# elapsed is greater than:\n#\n#   (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period\n#\n# So for example if node-timeout is 30 seconds, and the cluster-replica-validity-factor\n# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the\n# replica will not try to failover if it was not able to talk with the primary\n# for longer than 310 seconds.\n#\n# A large cluster-replica-validity-factor may allow replicas with too old data to failover\n# a primary, while a too small value may prevent the cluster from being able to\n# elect a replica at all.\n#\n# For maximum availability, it is possible to set the cluster-replica-validity-factor\n# to a value of 0, which means, that replicas will always try to failover the\n# primary regardless of the last time they interacted with the primary.\n# (However they'll always try to apply a delay proportional to their\n# offset rank).\n#\n# Zero is the only value able to guarantee that when all the partitions heal\n# the cluster will always be able to continue.\n#\n# cluster-replica-validity-factor 10\n\n# Cluster replicas are able to migrate to orphaned primarys, that are primarys\n# that are left without working replicas. This improves the cluster ability\n# to resist to failures as otherwise an orphaned primary can't be failed over\n# in case of failure if it has no working replicas.\n#\n# Replicas migrate to orphaned primarys only if there are still at least a\n# given number of other working replicas for their old primary. This number\n# is the \"migration barrier\". A migration barrier of 1 means that a replica\n# will migrate only if there is at least 1 other working replica for its primary\n# and so forth. It usually reflects the number of replicas you want for every\n# primary in your cluster.\n#\n# Default is 1 (replicas migrate only if their primarys remain with at least\n# one replica). To disable migration just set it to a very large value or\n# set cluster-allow-replica-migration to 'no'.\n# A value of 0 can be set but is useful only for debugging and dangerous\n# in production.\n#\n# cluster-migration-barrier 1\n\n# Turning off this option allows to use less automatic cluster configuration.\n# It both disables migration to orphaned primarys and migration from primarys\n# that became empty.\n#\n# Default is 'yes' (allow automatic migrations).\n#\n# cluster-allow-replica-migration yes\n\n# By default Valkey Cluster nodes stop accepting queries if they detect there\n# is at least a hash slot uncovered (no available node is serving it).\n# This way if the cluster is partially down (for example a range of hash slots\n# are no longer covered) all the cluster becomes, eventually, unavailable.\n# It automatically returns available as soon as all the slots are covered again.\n#\n# However sometimes you want the subset of the cluster which is working,\n# to continue to accept queries for the part of the key space that is still\n# covered. In order to do so, just set the cluster-require-full-coverage\n# option to no.\n#\n# cluster-require-full-coverage yes\n\n# This option, when set to yes, prevents replicas from trying to failover its\n# primary during primary failures. However the replica can still perform a\n# manual failover, if forced to do so.\n#\n# This is useful in different scenarios, especially in the case of multiple\n# data center operations, where we want one side to never be promoted if not\n# in the case of a total DC failure.\n#\n# cluster-replica-no-failover no\n\n# This option, when set to yes, allows nodes to serve read traffic while the\n# cluster is in a down state, as long as it believes it owns the slots.\n#\n# This is useful for two cases.  The first case is for when an application\n# doesn't require consistency of data during node failures or network partitions.\n# One example of this is a cache, where as long as the node has the data it\n# should be able to serve it.\n#\n# The second use case is for configurations that don't meet the recommended\n# three shards but want to enable cluster mode and scale later. A\n# primary outage in a 1 or 2 shard configuration causes a read/write outage to the\n# entire cluster without this option set, with it set there is only a write outage.\n# Without a quorum of primarys, slot ownership will not change automatically.\n#\n# cluster-allow-reads-when-down no\n\n# This option, when set to yes, allows nodes to serve pubsub shard traffic while\n# the cluster is in a down state, as long as it believes it owns the slots.\n#\n# This is useful if the application would like to use the pubsub feature even when\n# the cluster global stable state is not OK. If the application wants to make sure only\n# one shard is serving a given channel, this feature should be kept as yes.\n#\n# cluster-allow-pubsubshard-when-down yes\n\n# Cluster link send buffer limit is the limit on the memory usage of an individual\n# cluster bus link's send buffer in bytes. Cluster links would be freed if they exceed\n# this limit. This is to primarily prevent send buffers from growing unbounded on links\n# toward slow peers (E.g. PubSub messages being piled up).\n# This limit is disabled by default. Enable this limit when 'mem_cluster_links' INFO field\n# and/or 'send-buffer-allocated' entries in the 'CLUSTER LINKS` command output continuously increase.\n# Minimum limit of 1gb is recommended so that cluster link buffer can fit in at least a single\n# PubSub message by default. (client-query-buffer-limit default value is 1gb)\n#\n# cluster-link-sendbuf-limit 0\n\n# Clusters can configure their announced hostname using this config. This is a common use case for\n# applications that need to use TLS Server Name Indication (SNI) or dealing with DNS based\n# routing. By default this value is only shown as additional metadata in the CLUSTER SLOTS\n# command, but can be changed using 'cluster-preferred-endpoint-type' config. This value is\n# communicated along the clusterbus to all nodes, setting it to an empty string will remove\n# the hostname and also propagate the removal.\n#\n# cluster-announce-hostname \"\"\n\n# Clusters can advertise how clients should connect to them using either their IP address,\n# a user defined hostname, or by declaring they have no endpoint. Which endpoint is\n# shown as the preferred endpoint is set by using the cluster-preferred-endpoint-type\n# config with values 'ip', 'hostname', or 'unknown-endpoint'. This value controls how\n# the endpoint returned for MOVED/ASKING requests as well as the first field of CLUSTER SLOTS.\n# If the preferred endpoint type is set to hostname, but no announced hostname is set, a '?'\n# will be returned instead.\n#\n# When a cluster advertises itself as having an unknown endpoint, it's indicating that\n# the server doesn't know how clients can reach the cluster. This can happen in certain\n# networking situations where there are multiple possible routes to the node, and the\n# server doesn't know which one the client took. In this case, the server is expecting\n# the client to reach out on the same endpoint it used for making the last request, but use\n# the port provided in the response.\n#\n# cluster-preferred-endpoint-type ip\n\n########################## CLUSTER DOCKER/NAT support  ########################\n\n# In certain deployments, Valkey Cluster nodes address discovery fails, because\n# addresses are NAT-ted or because ports are forwarded (the typical case is\n# Docker and other containers).\n#\n# In order to make Valkey Cluster working in such environments, a static\n# configuration where each node knows its public address is needed. The\n# following four options are used for this scope, and are:\n#\n# * cluster-announce-ip\n# * cluster-announce-port\n# * cluster-announce-tls-port\n# * cluster-announce-bus-port\n#\n# Each instructs the node about its address, client ports (for connections\n# without and with TLS) and cluster message bus port. The information is then\n# published in the header of the bus packets so that other nodes will be able to\n# correctly map the address of the node publishing the information.\n#\n# If cluster-tls is set to yes and cluster-announce-tls-port is omitted or set\n# to zero, then cluster-announce-port refers to the TLS port. Note also that\n# cluster-announce-tls-port has no effect if cluster-tls is set to no.\n#\n# If the above options are not used, the normal Valkey Cluster auto-detection\n# will be used instead.\n#\n# Note that when remapped, the bus port may not be at the fixed offset of\n# clients port + 10000, so you can specify any port and bus-port depending\n# on how they get remapped. If the bus-port is not set, a fixed offset of\n# 10000 will be used as usual.\n#\n# Example:\n#\n# cluster-announce-ip 10.1.1.5\n# cluster-announce-tls-port 6379\n# cluster-announce-port 0\n# cluster-announce-bus-port 6380\n\n################################## SLOW LOG ###################################\n\n# The Valkey Slow Log is a system to log queries that exceeded a specified\n# execution time. The execution time does not include the I/O operations\n# like talking with the client, sending the reply and so forth,\n# but just the time needed to actually execute the command (this is the only\n# stage of command execution where the thread is blocked and can not serve\n# other requests in the meantime).\n#\n# You can configure the slow log with two parameters: one tells Valkey\n# what is the execution time, in microseconds, to exceed in order for the\n# command to get logged, and the other parameter is the length of the\n# slow log. When a new command is logged the oldest one is removed from the\n# queue of logged commands.\n\n# The following time is expressed in microseconds, so 1000000 is equivalent\n# to one second. Note that a negative number disables the slow log, while\n# a value of zero forces the logging of every command.\nslowlog-log-slower-than 10000\n\n# There is no limit to this length. Just be aware that it will consume memory.\n# You can reclaim memory used by the slow log with SLOWLOG RESET.\nslowlog-max-len 128\n\n################################ LATENCY MONITOR ##############################\n\n# The Valkey latency monitoring subsystem samples different operations\n# at runtime in order to collect data related to possible sources of\n# latency of a Valkey instance.\n#\n# Via the LATENCY command this information is available to the user that can\n# print graphs and obtain reports.\n#\n# The system only logs operations that were performed in a time equal or\n# greater than the amount of milliseconds specified via the\n# latency-monitor-threshold configuration directive. When its value is set\n# to zero, the latency monitor is turned off.\n#\n# By default latency monitoring is disabled since it is mostly not needed\n# if you don't have latency issues, and collecting data has a performance\n# impact, that while very small, can be measured under big load. Latency\n# monitoring can easily be enabled at runtime using the command\n# \"CONFIG SET latency-monitor-threshold <milliseconds>\" if needed.\nlatency-monitor-threshold 0\n\n################################ LATENCY TRACKING ##############################\n\n# The Valkey extended latency monitoring tracks the per command latencies and enables\n# exporting the percentile distribution via the INFO latencystats command,\n# and cumulative latency distributions (histograms) via the LATENCY command.\n#\n# By default, the extended latency monitoring is enabled since the overhead\n# of keeping track of the command latency is very small.\n# latency-tracking yes\n\n# By default the exported latency percentiles via the INFO latencystats command\n# are the p50, p99, and p999.\n# latency-tracking-info-percentiles 50 99 99.9\n\n############################# EVENT NOTIFICATION ##############################\n\n# Valkey can notify Pub/Sub clients about events happening in the key space.\n#\n# For instance if keyspace events notification is enabled, and a client\n# performs a DEL operation on key \"foo\" stored in the Database 0, two\n# messages will be published via Pub/Sub:\n#\n# PUBLISH __keyspace@0__:foo del\n# PUBLISH __keyevent@0__:del foo\n#\n# It is possible to select the events that Valkey will notify among a set\n# of classes. Every class is identified by a single character:\n#\n#  K     Keyspace events, published with __keyspace@<db>__ prefix.\n#  E     Keyevent events, published with __keyevent@<db>__ prefix.\n#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...\n#  $     String commands\n#  l     List commands\n#  s     Set commands\n#  h     Hash commands\n#  z     Sorted set commands\n#  x     Expired events (events generated every time a key expires)\n#  e     Evicted events (events generated when a key is evicted for maxmemory)\n#  n     New key events (Note: not included in the 'A' class)\n#  t     Stream commands\n#  d     Module key type events\n#  m     Key-miss events (Note: It is not included in the 'A' class)\n#  A     Alias for g$lshzxetd, so that the \"AKE\" string means all the events\n#        (Except key-miss events which are excluded from 'A' due to their\n#         unique nature).\n#\n#  The \"notify-keyspace-events\" takes as argument a string that is composed\n#  of zero or multiple characters. The empty string means that notifications\n#  are disabled.\n#\n#  Example: to enable list and generic events, from the point of view of the\n#           event name, use:\n#\n#  notify-keyspace-events Elg\n#\n#  Example 2: to get the stream of the expired keys subscribing to channel\n#             name __keyevent@0__:expired use:\n#\n#  notify-keyspace-events Ex\n#\n#  By default all notifications are disabled because most users don't need\n#  this feature and the feature has some overhead. Note that if you don't\n#  specify at least one of K or E, no events will be delivered.\nnotify-keyspace-events \"\"\n\n############################### ADVANCED CONFIG ###############################\n\n# Hashes are encoded using a memory efficient data structure when they have a\n# small number of entries, and the biggest entry does not exceed a given\n# threshold. These thresholds can be configured using the following directives.\nhash-max-listpack-entries 512\nhash-max-listpack-value 64\n\n# Lists are also encoded in a special way to save a lot of space.\n# The number of entries allowed per internal list node can be specified\n# as a fixed maximum size or a maximum number of elements.\n# For a fixed maximum size, use -5 through -1, meaning:\n# -5: max size: 64 Kb  <-- not recommended for normal workloads\n# -4: max size: 32 Kb  <-- not recommended\n# -3: max size: 16 Kb  <-- probably not recommended\n# -2: max size: 8 Kb   <-- good\n# -1: max size: 4 Kb   <-- good\n# Positive numbers mean store up to _exactly_ that number of elements\n# per list node.\n# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),\n# but if your use case is unique, adjust the settings as necessary.\nlist-max-listpack-size -2\n\n# Lists may also be compressed.\n# Compress depth is the number of quicklist ziplist nodes from *each* side of\n# the list to *exclude* from compression.  The head and tail of the list\n# are always uncompressed for fast push/pop operations.  Settings are:\n# 0: disable all list compression\n# 1: depth 1 means \"don't start compressing until after 1 node into the list,\n#    going from either the head or tail\"\n#    So: [head]->node->node->...->node->[tail]\n#    [head], [tail] will always be uncompressed; inner nodes will compress.\n# 2: [head]->[next]->node->node->...->node->[prev]->[tail]\n#    2 here means: don't compress head or head->next or tail->prev or tail,\n#    but compress all nodes between them.\n# 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail]\n# etc.\nlist-compress-depth 0\n\n# Sets have a special encoding in just one case: when a set is composed\n# of just strings that happen to be integers in radix 10 in the range\n# of 64 bit signed integers.\n# The following configuration setting sets the limit in the size of the\n# set in order to use this special memory saving encoding.\nset-max-intset-entries 512\n\n# Similarly to hashes and lists, sorted sets are also specially encoded in\n# order to save a lot of space. This encoding is only used when the length and\n# elements of a sorted set are below the following limits:\nzset-max-listpack-entries 128\nzset-max-listpack-value 64\n\n# HyperLogLog sparse representation bytes limit. The limit includes the\n# 16 bytes header. When an HyperLogLog using the sparse representation crosses\n# this limit, it is converted into the dense representation.\n#\n# A value greater than 16000 is totally useless, since at that point the\n# dense representation is more memory efficient.\n#\n# The suggested value is ~ 3000 in order to have the benefits of\n# the space efficient encoding without slowing down too much PFADD,\n# which is O(N) with the sparse encoding. The value can be raised to\n# ~ 10000 when CPU is not a concern, but space is, and the data set is\n# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.\nhll-sparse-max-bytes 3000\n\n# Streams macro node max size / items. The stream data structure is a radix\n# tree of big nodes that encode multiple items inside. Using this configuration\n# it is possible to configure how big a single node can be in bytes, and the\n# maximum number of items it may contain before switching to a new node when\n# appending new stream entries. If any of the following settings are set to\n# zero, the limit is ignored, so for instance it is possible to set just a\n# max entries limit by setting max-bytes to 0 and max-entries to the desired\n# value.\nstream-node-max-bytes 4096\nstream-node-max-entries 100\n\n# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in\n# order to help rehashing the main Valkey hash table (the one mapping top-level\n# keys to values). The hash table implementation Valkey uses (see dict.c)\n# performs a lazy rehashing: the more operation you run into a hash table\n# that is rehashing, the more rehashing \"steps\" are performed, so if the\n# server is idle the rehashing is never complete and some more memory is used\n# by the hash table.\n#\n# The default is to use this millisecond 10 times every second in order to\n# actively rehash the main dictionaries, freeing memory when possible.\n#\n# If unsure:\n# use \"activerehashing no\" if you have hard latency requirements and it is\n# not a good thing in your environment that Valkey can reply from time to time\n# to queries with 2 milliseconds delay.\n#\n# use \"activerehashing yes\" if you don't have such hard requirements but\n# want to free memory asap when possible.\nactiverehashing yes\n\n# The client output buffer limits can be used to force disconnection of clients\n# that are not reading data from the server fast enough for some reason (a\n# common reason is that a Pub/Sub client can't consume messages as fast as the\n# publisher can produce them).\n#\n# The limit can be set differently for the three different classes of clients:\n#\n# normal -> normal clients including MONITOR clients\n# replica -> replica clients\n# pubsub -> clients subscribed to at least one pubsub channel or pattern\n#\n# The syntax of every client-output-buffer-limit directive is the following:\n#\n# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>\n#\n# A client is immediately disconnected once the hard limit is reached, or if\n# the soft limit is reached and remains reached for the specified number of\n# seconds (continuously).\n# So for instance if the hard limit is 32 megabytes and the soft limit is\n# 16 megabytes / 10 seconds, the client will get disconnected immediately\n# if the size of the output buffers reach 32 megabytes, but will also get\n# disconnected if the client reaches 16 megabytes and continuously overcomes\n# the limit for 10 seconds.\n#\n# By default normal clients are not limited because they don't receive data\n# without asking (in a push way), but just after a request, so only\n# asynchronous clients may create a scenario where data is requested faster\n# than it can read.\n#\n# Instead there is a default limit for pubsub and replica clients, since\n# subscribers and replicas receive data in a push fashion.\n#\n# Note that it doesn't make sense to set the replica clients output buffer\n# limit lower than the repl-backlog-size config (partial sync will succeed\n# and then replica will get disconnected).\n# Such a configuration is ignored (the size of repl-backlog-size will be used).\n# This doesn't have memory consumption implications since the replica client\n# will share the backlog buffers memory.\n#\n# Both the hard or the soft limit can be disabled by setting them to zero.\nclient-output-buffer-limit normal 0 0 0\nclient-output-buffer-limit replica 256mb 64mb 60\nclient-output-buffer-limit pubsub 32mb 8mb 60\n\n# Client query buffers accumulate new commands. They are limited to a fixed\n# amount by default in order to avoid that a protocol desynchronization (for\n# instance due to a bug in the client) will lead to unbound memory usage in\n# the query buffer. However you can configure it here if you have very special\n# needs, such us huge multi/exec requests or alike.\n#\n# client-query-buffer-limit 1gb\n\n# In some scenarios client connections can hog up memory leading to OOM\n# errors or data eviction. To avoid this we can cap the accumulated memory\n# used by all client connections (all pubsub and normal clients). Once we\n# reach that limit connections will be dropped by the server freeing up\n# memory. The server will attempt to drop the connections using the most\n# memory first. We call this mechanism \"client eviction\".\n#\n# Client eviction is configured using the maxmemory-clients setting as follows:\n# 0 - client eviction is disabled (default)\n#\n# A memory value can be used for the client eviction threshold,\n# for example:\n# maxmemory-clients 1g\n#\n# A percentage value (between 1% and 100%) means the client eviction threshold\n# is based on a percentage of the maxmemory setting. For example to set client\n# eviction at 5% of maxmemory:\n# maxmemory-clients 5%\n\n# In the Valkey protocol, bulk requests, that are, elements representing single\n# strings, are normally limited to 512 mb. However you can change this limit\n# here, but must be 1mb or greater\n#\n# proto-max-bulk-len 512mb\n\n# Valkey calls an internal function to perform many background tasks, like\n# closing connections of clients in timeout, purging expired keys that are\n# never requested, and so forth.\n#\n# Not all tasks are performed with the same frequency, but Valkey checks for\n# tasks to perform according to the specified \"hz\" value.\n#\n# By default \"hz\" is set to 10. Raising the value will use more CPU when\n# Valkey is idle, but at the same time will make Valkey more responsive when\n# there are many keys expiring at the same time, and timeouts may be\n# handled with more precision.\n#\n# The range is between 1 and 500, however a value over 100 is usually not\n# a good idea. Most users should use the default of 10 and raise this up to\n# 100 only in environments where very low latency is required.\nhz 10\n\n# Normally it is useful to have an HZ value which is proportional to the\n# number of clients connected. This is useful in order, for instance, to\n# avoid too many clients are processed for each background task invocation\n# in order to avoid latency spikes.\n#\n# Since the default HZ value by default is conservatively set to 10, Valkey\n# offers, and enables by default, the ability to use an adaptive HZ value\n# which will temporarily raise when there are many connected clients.\n#\n# When dynamic HZ is enabled, the actual configured HZ will be used\n# as a baseline, but multiples of the configured HZ value will be actually\n# used as needed once more clients are connected. In this way an idle\n# instance will use very little CPU time while a busy instance will be\n# more responsive.\ndynamic-hz yes\n\n# When a child rewrites the AOF file, if the following option is enabled\n# the file will be fsync-ed every 4 MB of data generated. This is useful\n# in order to commit the file to the disk more incrementally and avoid\n# big latency spikes.\naof-rewrite-incremental-fsync yes\n\n# When valkey saves RDB file, if the following option is enabled\n# the file will be fsync-ed every 4 MB of data generated. This is useful\n# in order to commit the file to the disk more incrementally and avoid\n# big latency spikes.\nrdb-save-incremental-fsync yes\n\n# Valkey LFU eviction (see maxmemory setting) can be tuned. However it is a good\n# idea to start with the default settings and only change them after investigating\n# how to improve the performances and how the keys LFU change over time, which\n# is possible to inspect via the OBJECT FREQ command.\n#\n# There are two tunable parameters in the Valkey LFU implementation: the\n# counter logarithm factor and the counter decay time. It is important to\n# understand what the two parameters mean before changing them.\n#\n# The LFU counter is just 8 bits per key, it's maximum value is 255, so Valkey\n# uses a probabilistic increment with logarithmic behavior. Given the value\n# of the old counter, when a key is accessed, the counter is incremented in\n# this way:\n#\n# 1. A random number R between 0 and 1 is extracted.\n# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).\n# 3. The counter is incremented only if R < P.\n#\n# The default lfu-log-factor is 10. This is a table of how the frequency\n# counter changes with a different number of accesses with different\n# logarithmic factors:\n#\n# +--------+------------+------------+------------+------------+------------+\n# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |\n# +--------+------------+------------+------------+------------+------------+\n# | 0      | 104        | 255        | 255        | 255        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n# | 1      | 18         | 49         | 255        | 255        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n# | 10     | 10         | 18         | 142        | 255        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n# | 100    | 8          | 11         | 49         | 143        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n#\n# NOTE: The above table was obtained by running the following commands:\n#\n#   valkey-benchmark -n 1000000 incr foo\n#   valkey-cli object freq foo\n#\n# NOTE 2: The counter initial value is 5 in order to give new objects a chance\n# to accumulate hits.\n#\n# The counter decay time is the time, in minutes, that must elapse in order\n# for the key counter to be divided by two (or decremented if it has a value\n# less <= 10).\n#\n# The default value for the lfu-decay-time is 1. A special value of 0 means to\n# decay the counter every time it happens to be scanned.\n#\n# lfu-log-factor 10\n# lfu-decay-time 1\n\n########################### ACTIVE DEFRAGMENTATION #######################\n#\n# What is active defragmentation?\n# -------------------------------\n#\n# Active (online) defragmentation allows a Valkey server to compact the\n# spaces left between small allocations and deallocations of data in memory,\n# thus allowing to reclaim back memory.\n#\n# Fragmentation is a natural process that happens with every allocator (but\n# less so with Jemalloc, fortunately) and certain workloads. Normally a server\n# restart is needed in order to lower the fragmentation, or at least to flush\n# away all the data and create it again.\n#\n# Basically when the fragmentation is over a certain level (see the\n# configuration options below) Valkey will start to create new copies of the\n# values in contiguous memory regions by exploiting certain specific Jemalloc\n# features (in order to understand if an allocation is causing fragmentation\n# and to allocate it in a better place), and at the same time, will release the\n# old copies of the data. This process, repeated incrementally for all the keys\n# will cause the fragmentation to drop back to normal values.\n#\n# Important things to understand:\n#\n# 1. This feature is disabled by default, and only works if you compiled Valkey\n#    to use the copy of Jemalloc we ship with the source code of Valkey.\n#    This is the default with Linux builds.\n#\n# 2. You never need to enable this feature if you don't have fragmentation\n#    issues.\n#\n# 3. Once you experience fragmentation, you can enable this feature when\n#    needed with the command \"CONFIG SET activedefrag yes\".\n#\n# The configuration parameters are able to fine tune the behavior of the\n# defragmentation process. If you are not sure about what they mean it is\n# a good idea to leave the defaults untouched.\n\n# Active defragmentation is disabled by default\n# activedefrag no\n\n# Minimum amount of fragmentation waste to start active defrag\n# active-defrag-ignore-bytes 100mb\n\n# Minimum percentage of fragmentation to start active defrag\n# active-defrag-threshold-lower 10\n\n# Maximum percentage of fragmentation at which we use maximum effort\n# active-defrag-threshold-upper 100\n\n# Minimal effort for defrag in CPU percentage, to be used when the lower\n# threshold is reached\n# active-defrag-cycle-min 1\n\n# Maximal effort for defrag in CPU percentage, to be used when the upper\n# threshold is reached\n# active-defrag-cycle-max 25\n\n# Maximum number of set/hash/zset/list fields that will be processed from\n# the main dictionary scan\n# active-defrag-max-scan-fields 1000\n\n# Jemalloc background thread for purging will be enabled by default\njemalloc-bg-thread yes\n\n# It is possible to pin different threads and processes of Valkey to specific\n# CPUs in your system, in order to maximize the performances of the server.\n# This is useful both in order to pin different Valkey threads in different\n# CPUs, but also in order to make sure that multiple Valkey instances running\n# in the same host will be pinned to different CPUs.\n#\n# Normally you can do this using the \"taskset\" command, however it is also\n# possible to this via Valkey configuration directly, both in Linux and FreeBSD.\n#\n# You can pin the server/IO threads, bio threads, aof rewrite child process, and\n# the bgsave child process. The syntax to specify the cpu list is the same as\n# the taskset command:\n#\n# Set valkey server/io threads to cpu affinity 0,2,4,6:\n# server_cpulist 0-7:2\n#\n# Set bio threads to cpu affinity 1,3:\n# bio_cpulist 1,3\n#\n# Set aof rewrite child process to cpu affinity 8,9,10,11:\n# aof_rewrite_cpulist 8-11\n#\n# Set bgsave child process to cpu affinity 1,10,11\n# bgsave_cpulist 1,10-11\n\n# In some cases valkey will emit warnings and even refuse to start if it detects\n# that the system is in bad state, it is possible to suppress these warnings\n# by setting the following config which takes a space delimited list of warnings\n# to suppress\n#\n# ignore-warnings ARM64-COW-BUG"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster-default
  namespace: gitea
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/sh
    set -e
    if [ ! -z "$VALKEY_PASSWORD" ]; then export REDISCLI_AUTH=$VALKEY_PASSWORD; fi;
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h localhost \
        -p $VALKEY_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/sh
    set -e

    VALKEY_STATUS_FILE=/tmp/.valkey_cluster_check
    if [ ! -z "$VALKEY_PASSWORD" ]; then export REDISCLI_AUTH=$VALKEY_PASSWORD; fi;
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h localhost \
        -p $VALKEY_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
    if [ ! -f "$VALKEY_STATUS_FILE" ]; then
      response=$(
        timeout -s 15 $1 \
        valkey-cli \
          -h localhost \
          -p $VALKEY_PORT_NUMBER \
          CLUSTER INFO | grep cluster_state | tr -d '[:space:]'
      )
      if [ "$?" -eq "124" ]; then
        echo "Timed out"
        exit 1
      fi
      if [ "$response" != "cluster_state:ok" ]; then
        echo "$response"
        exit 1
      else
        touch "$VALKEY_STATUS_FILE"
      fi
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster-scripts
  namespace: gitea
---
apiVersion: v1
data:
  config.yaml: |-
    customApps:
    - apps:
      - icon: https://brands.home-assistant.io/homeassistant/icon.png
        name: Home Assistant
        url: https://homeassistant.homelab.olav.ninja
      group: Applications
    darkTheme: tron
    defaultEnable: false
    globalBookmarks:
    - bookmarks:
      - name: Hubble
        url: https://hubble.homelab.olav.ninja
      - name: Keycloak
        url: https://keycloak.homelab.olav.ninja
      - name: Netbird
        url: https://netbird.homelab.olav.ninja
      group: Utilities
    - bookmarks:
      - name: OpenWrt
        url: http://192.168.0.1
      - name: Proxmox
        url: https://proxmox.homelab.olav.ninja:8006
      group: Infrastructure
    - bookmarks:
      - name: Cloudflare Dashboard
        url: https://dash.cloudflare.com
      - name: Github Repo
        url: https://github.com/olav-st/homelab
      group: External
    instanceName: null
    lightTheme: gazette
    name: Olav
    namespaceSelector:
      any: true
      matchNames:
      - media
    title: Homelab
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari-settings
  namespace: hajimari
---
apiVersion: v1
data:
  immich-config.yaml: |
    placeholder: foo
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: immich
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-immich-config
  namespace: immich
---
apiVersion: v1
data:
  create-extensions.sql: |
    CREATE EXTENSION cube;
    CREATE EXTENSION earthdistance;
    CREATE EXTENSION vectors;
  immich-superuser.sql: "ALTER USER immich WITH SUPERUSER; \n"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql-init-scripts
  namespace: immich
---
apiVersion: v1
data:
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-configuration
  namespace: immich
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-health
  namespace: immich
---
apiVersion: v1
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-scripts
  namespace: immich
---
apiVersion: v1
data:
  JAVA_OPTS_APPEND: -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
  KC_BOOTSTRAP_ADMIN_USERNAME: admin
  KC_CACHE_CONFIG_FILE: cache-ispn.xml
  KC_CACHE_STACK: kubernetes
  KC_CACHE_TYPE: ispn
  KEYCLOAK_DATABASE_HOST: keycloak-postgresql
  KEYCLOAK_DATABASE_NAME: bitnami_keycloak
  KEYCLOAK_DATABASE_PORT: "5432"
  KEYCLOAK_DATABASE_USER: bn_keycloak
  KEYCLOAK_ENABLE_HTTPS: "false"
  KEYCLOAK_ENABLE_STATISTICS: "false"
  KEYCLOAK_HOSTNAME: https://keycloak.homelab.olav.ninja/
  KEYCLOAK_HOSTNAME_STRICT: "false"
  KEYCLOAK_HTTP_PORT: "8080"
  KEYCLOAK_LOG_LEVEL: INFO
  KEYCLOAK_LOG_OUTPUT: default
  KEYCLOAK_PRODUCTION: "true"
  KEYCLOAK_PROXY_HEADERS: xforwarded
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak-env-vars
  namespace: keycloak
---
apiVersion: v1
data:
  agent-not-ready-taint-key: node.cilium.io/agent-not-ready
  arping-refresh-period: 30s
  auto-direct-node-routes: "false"
  bpf-distributed-lru: "false"
  bpf-events-drop-enabled: "true"
  bpf-events-policy-verdict-enabled: "true"
  bpf-events-trace-enabled: "true"
  bpf-lb-acceleration: disabled
  bpf-lb-algorithm-annotation: "false"
  bpf-lb-external-clusterip: "false"
  bpf-lb-map-max: "65536"
  bpf-lb-mode-annotation: "false"
  bpf-lb-sock: "false"
  bpf-lb-source-range-all-types: "false"
  bpf-map-dynamic-size-ratio: "0.0025"
  bpf-policy-map-max: "16384"
  bpf-root: /sys/fs/bpf
  cgroup-root: /sys/fs/cgroup
  cilium-endpoint-gc-interval: 5m0s
  cluster-id: "0"
  cluster-name: default
  clustermesh-enable-endpoint-sync: "false"
  clustermesh-enable-mcs-api: "false"
  cni-exclusive: "true"
  cni-log-file: /var/run/cilium/cilium-cni.log
  custom-cni-conf: "false"
  datapath-mode: veth
  debug: "false"
  debug-verbose: ""
  default-lb-service-ipam: lbipam
  direct-routing-skip-unreachable: "false"
  dnsproxy-enable-transparent-mode: "true"
  dnsproxy-socket-linger-timeout: "10"
  egress-gateway-reconciliation-trigger-interval: 1s
  enable-auto-protect-node-port-range: "true"
  enable-bpf-clock-probe: "false"
  enable-endpoint-health-checking: "true"
  enable-endpoint-lockdown-on-policy-overflow: "false"
  enable-experimental-lb: "false"
  enable-health-check-loadbalancer-ip: "false"
  enable-health-check-nodeport: "true"
  enable-health-checking: "true"
  enable-hubble: "true"
  enable-internal-traffic-policy: "true"
  enable-ipv4: "true"
  enable-ipv4-big-tcp: "false"
  enable-ipv4-masquerade: "true"
  enable-ipv6: "false"
  enable-ipv6-big-tcp: "false"
  enable-ipv6-masquerade: "true"
  enable-k8s-networkpolicy: "true"
  enable-k8s-terminating-endpoint: "true"
  enable-l2-announcements: "true"
  enable-l2-neigh-discovery: "true"
  enable-l7-proxy: "true"
  enable-lb-ipam: "true"
  enable-local-redirect-policy: "false"
  enable-masquerade-to-route-source: "false"
  enable-metrics: "true"
  enable-node-selector-labels: "false"
  enable-non-default-deny-policies: "true"
  enable-policy: default
  enable-policy-secrets-sync: "true"
  enable-runtime-device-detection: "true"
  enable-sctp: "false"
  enable-source-ip-verification: "true"
  enable-svc-source-range-check: "true"
  enable-tcx: "true"
  enable-vtep: "false"
  enable-well-known-identities: "false"
  enable-xt-socket-fallback: "true"
  envoy-access-log-buffer-size: "4096"
  envoy-base-id: "0"
  envoy-keep-cap-netbindservice: "false"
  external-envoy-proxy: "true"
  health-check-icmp-failure-threshold: "3"
  http-retry-count: "3"
  hubble-disable-tls: "false"
  hubble-export-file-max-backups: "5"
  hubble-export-file-max-size-mb: "10"
  hubble-listen-address: :4244
  hubble-socket-path: /var/run/cilium/hubble.sock
  hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt
  hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt
  hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key
  identity-allocation-mode: crd
  identity-gc-interval: 15m0s
  identity-heartbeat-timeout: 30m0s
  install-no-conntrack-iptables-rules: "false"
  ipam: kubernetes
  ipam-cilium-node-update-rate: 15s
  iptables-random-fully: "false"
  k8s-client-burst: "100"
  k8s-client-qps: "50"
  k8s-require-ipv4-pod-cidr: "false"
  k8s-require-ipv6-pod-cidr: "false"
  kube-proxy-replacement: "true"
  kube-proxy-replacement-healthz-bind-address: ""
  max-connected-clusters: "255"
  mesh-auth-enabled: "true"
  mesh-auth-gc-interval: 5m0s
  mesh-auth-queue-size: "1024"
  mesh-auth-rotated-identities-queue-size: "1024"
  monitor-aggregation: medium
  monitor-aggregation-flags: all
  monitor-aggregation-interval: 5s
  nat-map-stats-entries: "32"
  nat-map-stats-interval: 30s
  node-port-bind-protection: "true"
  nodeport-addresses: ""
  nodes-gc-interval: 5m0s
  operator-api-serve-addr: 127.0.0.1:9234
  operator-prometheus-serve-addr: :9963
  policy-cidr-match-mode: ""
  policy-secrets-namespace: cilium-secrets
  policy-secrets-only-from-secrets-namespace: "true"
  preallocate-bpf-maps: "false"
  procfs: /host/proc
  proxy-connect-timeout: "2"
  proxy-idle-timeout-seconds: "60"
  proxy-initial-fetch-timeout: "30"
  proxy-max-concurrent-retries: "128"
  proxy-max-connection-duration-seconds: "0"
  proxy-max-requests-per-connection: "0"
  proxy-xff-num-trusted-hops-egress: "0"
  proxy-xff-num-trusted-hops-ingress: "0"
  remove-cilium-node-taints: "true"
  routing-mode: tunnel
  service-no-backend-response: reject
  set-cilium-is-up-condition: "true"
  set-cilium-node-taints: "true"
  synchronize-k8s-nodes: "true"
  tofqdns-dns-reject-response-code: refused
  tofqdns-enable-dns-compression: "true"
  tofqdns-endpoint-max-ip-per-hostname: "1000"
  tofqdns-idle-connection-grace-period: 0s
  tofqdns-max-deferred-connection-deletes: "10000"
  tofqdns-proxy-response-max-delay: 100ms
  tunnel-protocol: vxlan
  tunnel-source-port-range: 0-0
  unmanaged-pod-watcher-interval: "15"
  vtep-cidr: ""
  vtep-endpoint: ""
  vtep-mac: ""
  vtep-mask: ""
  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
---
apiVersion: v1
data:
  bootstrap-config.json: |
    {"admin":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/admin.sock"}}},"applicationLogConfig":{"logFormat":{"textFormat":"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v"}},"bootstrapExtensions":[{"name":"envoy.bootstrap.internal_listener","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener"}}],"dynamicResources":{"cdsConfig":{"apiConfigSource":{"apiType":"GRPC","grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}],"setNodeOnFirstMessageOnly":true,"transportApiVersion":"V3"},"initialFetchTimeout":"30s","resourceApiVersion":"V3"},"ldsConfig":{"apiConfigSource":{"apiType":"GRPC","grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}],"setNodeOnFirstMessageOnly":true,"transportApiVersion":"V3"},"initialFetchTimeout":"30s","resourceApiVersion":"V3"}},"node":{"cluster":"ingress-cluster","id":"host~127.0.0.1~no-id~localdomain"},"overloadManager":{"resourceMonitors":[{"name":"envoy.resource_monitors.global_downstream_max_connections","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig","max_active_downstream_connections":"50000"}}]},"staticResources":{"clusters":[{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"ingress-cluster","type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"egress-cluster-tls","transportSocket":{"name":"cilium.tls_wrapper","typedConfig":{"@type":"type.googleapis.com/cilium.UpstreamTlsWrapperContext"}},"type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"upstreamHttpProtocolOptions":{},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"egress-cluster","type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"ingress-cluster-tls","transportSocket":{"name":"cilium.tls_wrapper","typedConfig":{"@type":"type.googleapis.com/cilium.UpstreamTlsWrapperContext"}},"type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"upstreamHttpProtocolOptions":{},"useDownstreamProtocolConfig":{}}}},{"connectTimeout":"2s","loadAssignment":{"clusterName":"xds-grpc-cilium","endpoints":[{"lbEndpoints":[{"endpoint":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/xds.sock"}}}}]}]},"name":"xds-grpc-cilium","type":"STATIC","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","explicitHttpConfig":{"http2ProtocolOptions":{}}}}},{"connectTimeout":"2s","loadAssignment":{"clusterName":"/envoy-admin","endpoints":[{"lbEndpoints":[{"endpoint":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/admin.sock"}}}}]}]},"name":"/envoy-admin","type":"STATIC"}],"listeners":[{"address":{"socketAddress":{"address":"0.0.0.0","portValue":9964}},"filterChains":[{"filters":[{"name":"envoy.filters.network.http_connection_manager","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager","httpFilters":[{"name":"envoy.filters.http.router","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}],"internalAddressConfig":{"cidrRanges":[{"addressPrefix":"10.0.0.0","prefixLen":8},{"addressPrefix":"172.16.0.0","prefixLen":12},{"addressPrefix":"192.168.0.0","prefixLen":16},{"addressPrefix":"127.0.0.1","prefixLen":32}]},"routeConfig":{"virtualHosts":[{"domains":["*"],"name":"prometheus_metrics_route","routes":[{"match":{"prefix":"/metrics"},"name":"prometheus_metrics_route","route":{"cluster":"/envoy-admin","prefixRewrite":"/stats/prometheus"}}]}]},"statPrefix":"envoy-prometheus-metrics-listener","streamIdleTimeout":"0s"}}]}],"name":"envoy-prometheus-metrics-listener"},{"address":{"socketAddress":{"address":"127.0.0.1","portValue":9878}},"filterChains":[{"filters":[{"name":"envoy.filters.network.http_connection_manager","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager","httpFilters":[{"name":"envoy.filters.http.router","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}],"internalAddressConfig":{"cidrRanges":[{"addressPrefix":"10.0.0.0","prefixLen":8},{"addressPrefix":"172.16.0.0","prefixLen":12},{"addressPrefix":"192.168.0.0","prefixLen":16},{"addressPrefix":"127.0.0.1","prefixLen":32}]},"routeConfig":{"virtual_hosts":[{"domains":["*"],"name":"health","routes":[{"match":{"prefix":"/healthz"},"name":"health","route":{"cluster":"/envoy-admin","prefixRewrite":"/ready"}}]}]},"statPrefix":"envoy-health-listener","streamIdleTimeout":"0s"}}]}],"name":"envoy-health-listener"}]}}
kind: ConfigMap
metadata:
  name: cilium-envoy-config
  namespace: kube-system
---
apiVersion: v1
data:
  config.yaml: "cluster-name: default\npeer-service: \"hubble-peer.kube-system.svc.cluster.local.:443\"\nlisten-address: :4245\ngops: true\ngops-port: \"9893\"\nretry-timeout: \nsort-buffer-len-max: \nsort-buffer-drain-timeout: \ntls-hubble-client-cert-file: /var/lib/hubble-relay/tls/client.crt\ntls-hubble-client-key-file: /var/lib/hubble-relay/tls/client.key\ntls-hubble-server-ca-files: /var/lib/hubble-relay/tls/hubble-server-ca.crt\n\ndisable-server-tls: true\n"
kind: ConfigMap
metadata:
  name: hubble-relay-config
  namespace: kube-system
---
apiVersion: v1
data:
  nginx.conf: "server {\n    listen       8081;\n    listen       [::]:8081;\n    server_name  localhost;\n    root /app;\n    index index.html;\n    client_max_body_size 1G;\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n\n        location /api {\n            proxy_http_version 1.1;\n            proxy_pass_request_headers on;\n            proxy_pass http://127.0.0.1:8090;\n        }\n        location / {\n            # double `/index.html` is required here \n            try_files $uri $uri/ /index.html /index.html;\n        }\n\n        # Liveness probe\n        location /healthz {\n            access_log off;\n            add_header Content-Type text/plain;\n            return 200 'ok';\n        }\n    }\n}"
kind: ConfigMap
metadata:
  name: hubble-ui-nginx
  namespace: kube-system
---
apiVersion: v1
data:
  management.tmpl.json: |-
    {
        "Stuns": [
            {
                "Proto": "udp",
                "URI": "${NETBIRD_STUN_URI}",
                "Username": "",
                "Password": null
            }
        ],
        "TURNConfig": {
            "Turns": [
                {
                    "Proto": "udp",
                    "URI": "${NETBIRD_TURN_URI}",
                    "Username": "${NETBIRD_TURN_USER}",
                    "Password": "${NETBIRD_TURN_PASSWORD}"
                }
            ],
            "CredentialsTTL": "12h",
            "Secret": "secret",
            "TimeBasedCredentials": false
        },
        "Signal": {
            "Proto": "${NETBIRD_SIGNAL_PROTOCOL}",
            "URI": "${NETBIRD_SIGNAL_URI}",
            "Username": "",
            "Password": null
        },
        "Datadir": "",
        "HttpConfig": {
            "Address": "0.0.0.0:80",
            "AuthAudience": "${NETBIRD_AUTH_AUDIENCE}",
            "AuthUserIDClaim": "${NETBIRD_AUTH_USER_ID_CLAIM:-sub}",
            "CertFile": "${NETBIRD_MGMT_API_CERT_FILE}",
            "CertKey": "${NETBIRD_MGMT_API_CERT_KEY_FILE}",
            "IdpSignKeyRefreshEnabled": true,
            "OIDCConfigEndpoint": "${NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT}"
        },
        "IdpManagerConfig": {
            "ManagerType": "${NETBIRD_IDP_MANAGER_TYPE}",
            "${NETBIRD_IDP_MANAGER_TYPE^}ClientCredentials": {
                "ClientID": "${NETBIRD_IDP_CLIENT_ID}",
                "ClientSecret": "${NETBIRD_IDP_CLIENT_SECRET}",
                "GrantType": "${NETBIRD_IDP_GRANT_TYPE}",
                "Audience": "${NETBIRD_IDP_AUTH0_AUDIENCE}",
                "AuthIssuer": "${NETBIRD_IDP_AUTH0_AUTH_ISSUER}",
                "AdminEndpoint": "${NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT}",
                "TokenEndpoint": "${NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT}"
            }
        },
        "DeviceAuthorizationFlow": {
            "Provider": "${NETBIRD_AUTH_DEVICE_AUTH_PROVIDER}",
            "ProviderConfig": {
                "Audience": "${NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE}",
                "ClientID": "${NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID}",
                "DeviceAuthEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT}",
                "Domain": "${NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY}",
                "TokenEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT}",
                "Scope": "${NETBIRD_AUTH_DEVICE_AUTH_SCOPE}",
                "UseIDToken": ${NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN:-false}
            }
        },
        "Relay": {
            "Addresses": ["${NB_EXPOSED_ADDRESS}"],
            "CredentialsTTL": "24h",
            "Secret": "${NB_AUTH_SECRET}"
        }
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
data:
  .htaccess: |-
    # line below if for Apache 2.4
    <ifModule mod_authz_core.c>
    Require all denied
    </ifModule>
    # line below if for Apache 2.2
    <ifModule !mod_authz_core.c>
    deny from all
    </ifModule>
    # section for Apache 2.2 and 2.4
    <ifModule mod_autoindex.c>
    IndexIgnore *
    </ifModule>
  apache-pretty-urls.config.php: |-
    <?php
    $CONFIG = array (
      'htaccess.RewriteBase' => '/',
    );
  apcu.config.php: |-
    <?php
    $CONFIG = array (
      'memcache.local' => '\OC\Memcache\APCu',
    );
  apps.config.php: |-
    <?php
    $CONFIG = array (
      'apps_paths' => array (
          0 => array (
                  'path'     => OC::$SERVERROOT.'/apps',
                  'url'      => '/apps',
                  'writable' => false,
          ),
          1 => array (
                  'path'     => OC::$SERVERROOT.'/custom_apps',
                  'url'      => '/custom_apps',
                  'writable' => true,
          ),
      ),
    );
  autoconfig.php: |-
    <?php
    $autoconfig_enabled = false;
    if (getenv('SQLITE_DATABASE')) {
        $AUTOCONFIG["dbtype"] = "sqlite";
        $AUTOCONFIG["dbname"] = getenv('SQLITE_DATABASE');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE_FILE') && getenv('MYSQL_USER_FILE') && getenv('MYSQL_PASSWORD_FILE') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG['dbtype'] = 'mysql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('MYSQL_DATABASE_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('MYSQL_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('MYSQL_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE') && getenv('MYSQL_USER') && getenv('MYSQL_PASSWORD') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG["dbtype"] = "mysql";
        $AUTOCONFIG["dbname"] = getenv('MYSQL_DATABASE');
        $AUTOCONFIG["dbuser"] = getenv('MYSQL_USER');
        $AUTOCONFIG["dbpass"] = getenv('MYSQL_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB_FILE') && getenv('POSTGRES_USER_FILE') && getenv('POSTGRES_PASSWORD_FILE') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG['dbtype'] = 'pgsql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('POSTGRES_DB_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('POSTGRES_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('POSTGRES_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB') && getenv('POSTGRES_USER') && getenv('POSTGRES_PASSWORD') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG["dbtype"] = "pgsql";
        $AUTOCONFIG["dbname"] = getenv('POSTGRES_DB');
        $AUTOCONFIG["dbuser"] = getenv('POSTGRES_USER');
        $AUTOCONFIG["dbpass"] = getenv('POSTGRES_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    }
    if ($autoconfig_enabled) {
        $AUTOCONFIG["directory"] = getenv('NEXTCLOUD_DATA_DIR') ?: "/var/www/html/data";
    }
  mycustom.config.php: |-
    <?php
    $CONFIG = array(
      'trusted_proxies' => array('10.0.0.0/8'),
      'default_phone_region' => 'NO',
      'maintenance_window_start' => 1,
      );
  redis.config.php: |-
    <?php
    if (getenv('REDIS_HOST')) {
      $CONFIG = array(
        'memcache.distributed' => '\OC\Memcache\Redis',
        'memcache.locking' => '\OC\Memcache\Redis',
        'redis' => array(
          'host' => getenv('REDIS_HOST'),
          'password' => getenv('REDIS_HOST_PASSWORD_FILE') ? trim(file_get_contents(getenv('REDIS_HOST_PASSWORD_FILE'))) : (string) getenv('REDIS_HOST_PASSWORD'),
        ),
      );

      if (getenv('REDIS_HOST_PORT') !== false) {
        $CONFIG['redis']['port'] = (int) getenv('REDIS_HOST_PORT');
      } elseif (getenv('REDIS_HOST')[0] != '/') {
        $CONFIG['redis']['port'] = 6379;
      }
    }
  reverse-proxy.config.php: |-
    <?php
    $overwriteHost = getenv('OVERWRITEHOST');
    if ($overwriteHost) {
      $CONFIG['overwritehost'] = $overwriteHost;
    }

    $overwriteProtocol = getenv('OVERWRITEPROTOCOL');
    if ($overwriteProtocol) {
      $CONFIG['overwriteprotocol'] = $overwriteProtocol;
    }

    $overwriteCliUrl = getenv('OVERWRITECLIURL');
    if ($overwriteCliUrl) {
      $CONFIG['overwrite.cli.url'] = $overwriteCliUrl;
    }

    $overwriteWebRoot = getenv('OVERWRITEWEBROOT');
    if ($overwriteWebRoot) {
      $CONFIG['overwritewebroot'] = $overwriteWebRoot;
    }

    $overwriteCondAddr = getenv('OVERWRITECONDADDR');
    if ($overwriteCondAddr) {
      $CONFIG['overwritecondaddr'] = $overwriteCondAddr;
    }

    $trustedProxies = getenv('TRUSTED_PROXIES');
    if ($trustedProxies) {
      $CONFIG['trusted_proxies'] = array_filter(array_map('trim', explode(' ', $trustedProxies)));
    }

    $forwardedForHeaders = getenv('FORWARDED_FOR_HEADERS');
    if ($forwardedForHeaders) {
      $CONFIG['forwarded_for_headers'] = array_filter(array_map('trim', explode(' ', $forwardedForHeaders)));
    }
  s3.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_S3_BUCKET')) {
      $use_ssl = getenv('OBJECTSTORE_S3_SSL');
      $use_path = getenv('OBJECTSTORE_S3_USEPATH_STYLE');
      $use_legacyauth = getenv('OBJECTSTORE_S3_LEGACYAUTH');
      $autocreate = getenv('OBJECTSTORE_S3_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => array(
          'class' => '\OC\Files\ObjectStore\S3',
          'arguments' => array(
            'bucket' => getenv('OBJECTSTORE_S3_BUCKET'),
            'region' => getenv('OBJECTSTORE_S3_REGION') ?: '',
            'hostname' => getenv('OBJECTSTORE_S3_HOST') ?: '',
            'port' => getenv('OBJECTSTORE_S3_PORT') ?: '',
            'storageClass' => getenv('OBJECTSTORE_S3_STORAGE_CLASS') ?: '',
            'objectPrefix' => getenv("OBJECTSTORE_S3_OBJECT_PREFIX") ? getenv("OBJECTSTORE_S3_OBJECT_PREFIX") : "urn:oid:",
            'autocreate' => strtolower($autocreate) !== 'false',
            'use_ssl' => strtolower($use_ssl) !== 'false',
            // required for some non Amazon S3 implementations
            'use_path_style' => $use_path == true && strtolower($use_path) !== 'false',
            // required for older protocol versions
            'legacy_auth' => $use_legacyauth == true && strtolower($use_legacyauth) !== 'false'
          )
        )
      );

      if (getenv('OBJECTSTORE_S3_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_KEY')) {
        $CONFIG['objectstore']['arguments']['key'] = getenv('OBJECTSTORE_S3_KEY');
      } else {
        $CONFIG['objectstore']['arguments']['key'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SECRET_FILE')) {
        $CONFIG['objectstore']['arguments']['secret'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SECRET_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SECRET')) {
        $CONFIG['objectstore']['arguments']['secret'] = getenv('OBJECTSTORE_S3_SECRET');
      } else {
        $CONFIG['objectstore']['arguments']['secret'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SSE_C_KEY')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = getenv('OBJECTSTORE_S3_SSE_C_KEY');
      }
    }
  smtp.config.php: |-
    <?php
    if (getenv('SMTP_HOST') && getenv('MAIL_FROM_ADDRESS') && getenv('MAIL_DOMAIN')) {
      $CONFIG = array (
        'mail_smtpmode' => 'smtp',
        'mail_smtphost' => getenv('SMTP_HOST'),
        'mail_smtpport' => getenv('SMTP_PORT') ?: (getenv('SMTP_SECURE') ? 465 : 25),
        'mail_smtpsecure' => getenv('SMTP_SECURE') ?: '',
        'mail_smtpauth' => getenv('SMTP_NAME') && (getenv('SMTP_PASSWORD') || getenv('SMTP_PASSWORD_FILE')),
        'mail_smtpauthtype' => getenv('SMTP_AUTHTYPE') ?: 'LOGIN',
        'mail_smtpname' => getenv('SMTP_NAME') ?: '',
        'mail_from_address' => getenv('MAIL_FROM_ADDRESS'),
        'mail_domain' => getenv('MAIL_DOMAIN'),
      );

      if (getenv('SMTP_PASSWORD_FILE')) {
          $CONFIG['mail_smtppassword'] = trim(file_get_contents(getenv('SMTP_PASSWORD_FILE')));
      } elseif (getenv('SMTP_PASSWORD')) {
          $CONFIG['mail_smtppassword'] = getenv('SMTP_PASSWORD');
      } else {
          $CONFIG['mail_smtppassword'] = '';
      }
    }
  swift.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_SWIFT_URL')) {
        $autocreate = getenv('OBJECTSTORE_SWIFT_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => [
          'class' => 'OC\\Files\\ObjectStore\\Swift',
          'arguments' => [
            'autocreate' => $autocreate == true && strtolower($autocreate) !== 'false',
            'user' => [
              'name' => getenv('OBJECTSTORE_SWIFT_USER_NAME'),
              'password' => getenv('OBJECTSTORE_SWIFT_USER_PASSWORD'),
              'domain' => [
                'name' => (getenv('OBJECTSTORE_SWIFT_USER_DOMAIN')) ?: 'Default',
              ],
            ],
            'scope' => [
              'project' => [
                'name' => getenv('OBJECTSTORE_SWIFT_PROJECT_NAME'),
                'domain' => [
                  'name' => (getenv('OBJECTSTORE_SWIFT_PROJECT_DOMAIN')) ?: 'Default',
                ],
              ],
            ],
            'serviceName' => (getenv('OBJECTSTORE_SWIFT_SERVICE_NAME')) ?: 'swift',
            'region' => getenv('OBJECTSTORE_SWIFT_REGION'),
            'url' => getenv('OBJECTSTORE_SWIFT_URL'),
            'bucket' => getenv('OBJECTSTORE_SWIFT_CONTAINER_NAME'),
          ]
        ]
      );
    }
  upgrade-disable-web.config.php: |-
    <?php
    $CONFIG = array (
      'upgrade.disable-web' => true,
    );
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud-config
  namespace: nextcloud
---
apiVersion: v1
data:
  my.cnf: |-
    [mysqld]
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mariadb
    datadir=/bitnami/mariadb/data
    plugin_dir=/opt/bitnami/mariadb/plugin
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    tmpdir=/opt/bitnami/mariadb/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
    log-error=/opt/bitnami/mariadb/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    long_query_time=10.0

    [client]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mariadb/plugin

    [manager]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
---
apiVersion: v1
data:
  uploadLimit.ini: |-
    upload_max_filesize = 1G
    post_max_size = 1G
    max_input_time = 5400
    max_execution_time = 5400
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud-phpconfig
  namespace: nextcloud
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-root-ca
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-client
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-server
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea
  namespace: gitea
stringData:
  assertions: ""
  config_environment.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    function env2ini::log() {
      printf "${1}\n"
    }

    function env2ini::read_config_to_env() {
      local section="${1}"
      local line="${2}"

      if [[ -z "${line}" ]]; then
        # skip empty line
        return
      fi

      # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line
      local setting="$(awk -F '=' '{print $1}' <<< "${line}" | xargs echo -n)"

      if [[ -z "${setting}" ]]; then
        env2ini::log '  ! invalid setting'
        exit 1
      fi

      local value=''
      local regex="^${setting}(\s*)=(\s*)(.*)"
      if [[ $line =~ $regex ]]; then
        value="${BASH_REMATCH[3]}"
      else
        env2ini::log '  ! invalid setting'
        exit 1
      fi

      env2ini::log "    + '${setting}'"

      if [[ -z "${section}" ]]; then
        export "GITEA____${setting^^}=${value}"                           # '^^' makes the variable content uppercase
        return
      fi

      local masked_section="${section//./_0X2E_}"                            # '//' instructs to replace all matches
      masked_section="${masked_section//-/_0X2D_}"

      export "GITEA__${masked_section^^}__${setting^^}=${value}"        # '^^' makes the variable content uppercase
    }

    function env2ini::reload_preset_envs() {
      env2ini::log "Reloading preset envs..."

      while read -r line; do
        if [[ -z "${line}" ]]; then
          # skip empty line
          return
        fi

        # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line
        local setting="$(awk -F '=' '{print $1}' <<< "${line}" | xargs echo -n)"

        if [[ -z "${setting}" ]]; then
          env2ini::log '  ! invalid setting'
          exit 1
        fi

        local value=''
        local regex="^${setting}(\s*)=(\s*)(.*)"
        if [[ $line =~ $regex ]]; then
          value="${BASH_REMATCH[3]}"
        else
          env2ini::log '  ! invalid setting'
          exit 1
        fi

        env2ini::log "  + '${setting}'"

        export "${setting^^}=${value}"                           # '^^' makes the variable content uppercase
      done < "$TMP_EXISTING_ENVS_FILE"

      rm $TMP_EXISTING_ENVS_FILE
    }


    function env2ini::process_config_file() {
      local config_file="${1}"
      local section="$(basename "${config_file}")"

      if [[ $section == '_generals_' ]]; then
        env2ini::log "  [ini root]"
        section=''
      else
        env2ini::log "  ${section}"
      fi

      while read -r line; do
        env2ini::read_config_to_env "${section}" "${line}"
      done < <(awk 1 "${config_file}")                             # Helm .toYaml trims the trailing new line which breaks line processing; awk 1 ... adds it back while reading
    }

    function env2ini::load_config_sources() {
      local path="${1}"

      if [[ -d "${path}" ]]; then
        env2ini::log "Processing $(basename "${path}")..."

        while read -d '' configFile; do
          env2ini::process_config_file "${configFile}"
        done < <(find "${path}" -type l -not -name '..data' -print0)

        env2ini::log "\n"
      fi
    }

    function env2ini::generate_initial_secrets() {
      # These environment variables will either be
      #   - overwritten with user defined values,
      #   - initially used to set up Gitea
      # Anyway, they won't harm existing app.ini files

      export GITEA__SECURITY__INTERNAL_TOKEN=$(gitea generate secret INTERNAL_TOKEN)
      export GITEA__SECURITY__SECRET_KEY=$(gitea generate secret SECRET_KEY)
      export GITEA__OAUTH2__JWT_SECRET=$(gitea generate secret JWT_SECRET)
      export GITEA__SERVER__LFS_JWT_SECRET=$(gitea generate secret LFS_JWT_SECRET)

      env2ini::log "...Initial secrets generated\n"
    }

    # save existing envs prior to script execution. Necessary to keep order of preexisting and custom envs
    env | (grep -e '^GITEA__' || [[ $? == 1 ]]) > $TMP_EXISTING_ENVS_FILE

    # MUST BE CALLED BEFORE OTHER CONFIGURATION
    env2ini::generate_initial_secrets

    env2ini::load_config_sources "$ENV_TO_INI_MOUNT_POINT/inlines/"
    env2ini::load_config_sources "$ENV_TO_INI_MOUNT_POINT/additionals/"

    # load existing envs to override auto generated envs
    env2ini::reload_preset_envs

    env2ini::log "=== All configuration sources loaded ===\n"

    # safety to prevent rewrite of secret keys if an app.ini already exists
    if [ -f ${GITEA_APP_INI} ]; then
      env2ini::log 'An app.ini file already exists. To prevent overwriting secret keys, these settings are dropped and remain unchanged:'
      env2ini::log '  - security.INTERNAL_TOKEN'
      env2ini::log '  - security.SECRET_KEY'
      env2ini::log '  - oauth2.JWT_SECRET'
      env2ini::log '  - server.LFS_JWT_SECRET'

      unset GITEA__SECURITY__INTERNAL_TOKEN
      unset GITEA__SECURITY__SECRET_KEY
      unset GITEA__OAUTH2__JWT_SECRET
      unset GITEA__SERVER__LFS_JWT_SECRET
    fi

    environment-to-ini -o $GITEA_APP_INI
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-init
  namespace: gitea
stringData:
  configure_gitea.sh: "#!/usr/bin/env bash\n\nset -euo pipefail\n\necho '==== BEGIN GITEA CONFIGURATION ===='\n\n{ # try\n  gitea migrate\n} || { # catch\n  echo \"Gitea migrate might fail due to database connection...This init-container will try again in a few seconds\"\n  exit 1\n}\nfunction test_valkey_connection() {\n  local RETRY=0\n  local MAX=30\n  \n  echo 'Wait for valkey to become avialable...'\n  until [ \"${RETRY}\" -ge \"${MAX}\" ]; do\n    nc -vz -w2 gitea-valkey-cluster-headless.gitea.svc.cluster.local 6379 && break\n    RETRY=$[${RETRY}+1]\n    echo \"...not ready yet (${RETRY}/${MAX})\"\n  done\n\n  if [ \"${RETRY}\" -ge \"${MAX}\" ]; then\n    echo \"Valkey not reachable after '${MAX}' attempts!\"\n    exit 1\n  fi\n}\n\ntest_valkey_connection\nfunction configure_admin_user() {\n  local full_admin_list=$(gitea admin user list --admin)\n  local actual_user_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+Username\\s+Email\\s+IsActive.*)\"\n  if [[ \"${full_admin_list}\" =~ $regex ]]; then\n    actual_user_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_admin_user' was not able to determine the current list of admin users.\"\n    echo \"       Please review the output of 'gitea admin user list --admin' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-gitea/issues.\"\n    echo \"DEBUG: Output of 'gitea admin user list --admin'\"\n    echo \"--\"\n    echo \"${full_admin_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local ACCOUNT_ID=$(echo \"${actual_user_table}\" | grep -E \"\\s+${GITEA_ADMIN_USERNAME}\\s+\" | awk -F \" \" \"{printf \\$1}\")\n  if [[ -z \"${ACCOUNT_ID}\" ]]; then\n    local -a create_args\n    create_args=(--admin --username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\" --email \"gitea@local.domain\")\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = initialOnlyRequireReset ]]; then\n      create_args+=(--must-change-password=true)\n    else\n      create_args+=(--must-change-password=false)\n    fi\n    echo \"No admin user '${GITEA_ADMIN_USERNAME}' found. Creating now...\"\n    gitea admin user create \"${create_args[@]}\"\n    echo '...created.'\n  else\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = keepUpdated ]]; then\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist. Running update to sync password...\"\n      # See https://gitea.com/gitea/helm-gitea/issues/673\n      # --must-change-password argument was added to change-password, defaulting to true, counter to the previous behavior\n      #   which acted as if it were provided with =false. If the argument is present in this version of gitea, then we\n      #   should add it to prevent requiring frequent admin password resets.\n      local -a change_args\n      change_args=(--username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\")\n      if gitea admin user change-password --help | grep -qF -- '--must-change-password'; then\n        change_args+=(--must-change-password=false)\n      fi\n      gitea admin user change-password \"${change_args[@]}\"\n      echo '...password sync done.'\n    else\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist, but update mode is set to '${GITEA_ADMIN_PASSWORD_MODE}'. Skipping.\"\n    fi\n  fi\n}\n\nconfigure_admin_user\n\nfunction configure_ldap() {\n    echo 'no ldap configuration... skipping.'\n}\n\nconfigure_ldap\n\nfunction configure_oauth() {\n  local OAUTH_NAME='keycloak'\n  local full_auth_list=$(gitea admin auth list --vertical-bars)\n  local actual_auth_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+\\|Name\\s+\\|Type\\s+\\|Enabled.*)\"\n  if [[ \"${full_auth_list}\" =~ $regex ]]; then\n    actual_auth_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_oauth' was not able to determine the current list of authentication sources.\"\n    echo \"       Please review the output of 'gitea admin auth list --vertical-bars' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-gitea/issues.\"\n    echo \"DEBUG: Output of 'gitea admin auth list --vertical-bars'\"\n    echo \"--\"\n    echo \"${full_auth_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local AUTH_ID=$(echo \"${actual_auth_table}\" | grep -E \"\\|${OAUTH_NAME}\\s+\\|\" | grep -iE '\\|OAuth2\\s+\\|' | awk -F \" \"  \"{print \\$1}\")\n\n  if [[ -z \"${AUTH_ID}\" ]]; then\n    echo \"No oauth configuration found with name '${OAUTH_NAME}'. Installing it now...\"\n    gitea admin auth add-oauth --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...installed.'\n  else\n    echo \"Existing oauth configuration with name '${OAUTH_NAME}': '${AUTH_ID}'. Running update to sync settings...\"\n    gitea admin auth update-oauth --id \"${AUTH_ID}\" --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...sync settings done.'\n  fi\n}\n\nconfigure_oauth\n\necho '==== END GITEA CONFIGURATION ===='"
  configure_gpg_environment.sh: |
    #!/usr/bin/env bash
    set -eu

    gpg --batch --import "$TMP_RAW_GPG_KEY"
  init_directory_structure.sh: |-
    #!/usr/bin/env bash

    set -euo pipefail
    mkdir -pv /data/git/.ssh
    chmod -Rv 700 /data/git/.ssh
    [ ! -d /data/gitea/conf ] && mkdir -pv /data/gitea/conf

    # prepare temp directory structure
    mkdir -pv "${GITEA_TEMP}"
    chmod -v ug+rwx "${GITEA_TEMP}"
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-inline-config
  namespace: gitea
stringData:
  _generals_: ""
  cache: |-
    ADAPTER=redis
    HOST=redis+cluster://:@gitea-valkey-cluster-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
  database: |-
    DB_TYPE=postgres
    HOST=gitea-postgresql.gitea.svc.cluster.local:5432
    NAME=gitea
    PASSWD=gitea
    USER=gitea
  indexer: ISSUE_INDEXER_TYPE=db
  metrics: ENABLED=false
  queue: |-
    CONN_STR=redis+cluster://:@gitea-valkey-cluster-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
    TYPE=redis
  repository: ROOT=/data/git/gitea-repositories
  security: INSTALL_LOCK=true
  server: |-
    APP_DATA_PATH=/data
    DOMAIN=gitea.homelab.olav.ninja
    ENABLE_PPROF=false
    HTTP_PORT=3000
    LFS_START_SERVER=true
    OFFLINE_MODE=false
    PROTOCOL=http
    ROOT_URL=https://gitea.homelab.olav.ninja
    SSH_DOMAIN=gitea.homelab.olav.ninja
    SSH_LISTEN_PORT=2222
    SSH_PORT=22
    START_SSH_SERVER=true
  service: DISABLE_REGISTRATION=true
  session: |-
    PROVIDER=redis
    PROVIDER_CONFIG=redis+cluster://:@gitea-valkey-cluster-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
type: Opaque
---
apiVersion: v1
kind: Endpoints
metadata:
  name: external-cluster
  namespace: traefik
subsets:
  - addresses:
      - ip: 192.168.0.80
    ports:
      - name: ingress-port
        port: 443
        protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager
  namespace: cert-manager
spec:
  ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cert-manager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
  selector:
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cainjector
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
  selector:
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: webhook
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    io.cilium/lb-ipam-ips: 192.168.0.91
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r2
    helm.sh/chart: coturn-1.0.1
  name: coturn
  namespace: coturn
spec:
  ports:
    - name: tcp
      port: 3478
      protocol: TCP
      targetPort: tcp
    - name: udp
      port: 3478
      protocol: UDP
      targetPort: udp
    - name: tcp-tls
      port: 5349
      protocol: TCP
      targetPort: tcp-tls
    - name: udp-tls
      port: 5349
      protocol: UDP
      targetPort: udp-tls
  selector:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/name: coturn
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    release: crossplane
  name: crossplane-webhooks
  namespace: crossplane
spec:
  ports:
    - port: 9443
      protocol: TCP
      targetPort: 9443
  selector:
    app: crossplane
    release: crossplane
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-http
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: http
      port: 3000
      targetPort: null
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql-hl
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-ssh
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: 2222
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster
  namespace: gitea
spec:
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      protocol: TCP
      targetPort: tcp-redis
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: valkey-cluster
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster-headless
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: tcp-redis
    - name: tcp-redis-bus
      port: 16379
      targetPort: tcp-redis-bus
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: valkey-cluster
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/service: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/name: hajimari
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/service: immich-machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-machine-learning
  namespace: immich
spec:
  ports:
    - name: http
      port: 3003
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: machine-learning
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql-hl
  namespace: immich
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-headless
  namespace: immich
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/service: immich-server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-server
  namespace: immich
spec:
  ports:
    - name: http
      port: 2283
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: server
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  ports:
    - name: http
      nodePort: null
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak-headless
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql-hl
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "9964"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    io.cilium/app: proxy
    k8s-app: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: envoy-metrics
      port: 9964
      protocol: TCP
      targetPort: envoy-metrics
  selector:
    k8s-app: cilium-envoy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-peer
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: hubble-peer
  namespace: kube-system
spec:
  internalTrafficPolicy: Local
  ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
  selector:
    k8s-app: cilium
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-relay
  name: hubble-relay
  namespace: kube-system
spec:
  ports:
    - port: 80
      protocol: TCP
      targetPort: grpc
  selector:
    k8s-app: hubble-relay
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    k8s-app: hubble-ui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-management
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-relay
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  ports:
    - name: https
      port: 80
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-signal
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/name: netbird-dashboard
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud
  namespace: nextcloud
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: nextcloud
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  ports:
    - name: mysql
      nodePort: null
      port: 3306
      protocol: TCP
      targetPort: mysql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: mariadb
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.7.0
    helm.sh/chart: ollama-1.17.0
  name: ollama
  namespace: ollama
spec:
  ports:
    - name: http
      port: 11434
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/name: ollama
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.10
    helm.sh/chart: open-webui-6.16.0
  name: open-webui
  namespace: openwebui
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  ports:
    - name: http
      nodePort: null
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/name: sealed-secrets
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: external-cluster
  namespace: traefik
spec:
  clusterIP: None
  ports:
    - name: ingress-port
      port: 443
      protocol: TCP
      targetPort: 443
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    netbird.io/expose: "true"
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
  namespace: traefik
spec:
  externalTrafficPolicy: Local
  internalTrafficPolicy: Local
  loadBalancerIP: 192.168.0.90
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: ssh
    - name: webpublic
      port: 9443
      protocol: TCP
      targetPort: webpublic
    - name: websecure
      port: 443
      protocol: TCP
      targetPort: websecure
  selector:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/name: traefik
  type: LoadBalancer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: gitea
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: gitea-postgresql
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: immich-library
  namespace: immich
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 150Gi
  storageClassName: proxmox-csi
  volumeName: immich-library
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: immich-postgresql
  namespace: immich
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: immich-postgresql
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: proxmox-csi
  volumeName: netbird-management
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: proxmox-csi
  volumeName: netbird-signal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-data
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 300Gi
  storageClassName: proxmox-csi
  volumeName: nextcloud-data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: nextcloud-mariadb
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.7.0
    helm.sh/chart: ollama-1.17.0
  name: ollama
  namespace: ollama
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 60Gi
  storageClassName: proxmox-csi
  volumeName: ollama
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: open-webui
  namespace: openwebui
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: open-webui
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cert-manager
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cert-manager
        app.kubernetes.io/version: v1.17.2
        helm.sh/chart: cert-manager-v1.17.2
    spec:
      containers:
        - args:
            - --v=2
            - --cluster-resource-namespace=$(POD_NAMESPACE)
            - --leader-election-namespace=cert-manager
            - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.17.2
            - --max-concurrent-challenges=60
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.17.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
            - containerPort: 9403
              name: http-healthz
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cainjector
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cainjector
        app.kubernetes.io/version: v1.17.2
        helm.sh/chart: cert-manager-v1.17.2
    spec:
      containers:
        - args:
            - --v=2
            - --leader-election-namespace=cert-manager
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.17.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-cainjector
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: webhook
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: webhook
        app.kubernetes.io/version: v1.17.2
        helm.sh/chart: cert-manager-v1.17.2
    spec:
      containers:
        - args:
            - --v=2
            - --secure-port=10250
            - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
            - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
            - --dynamic-serving-dns-names=cert-manager-webhook
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.17.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
            - containerPort: 10250
              name: https
              protocol: TCP
            - containerPort: 6080
              name: healthcheck
              protocol: TCP
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r2
    helm.sh/chart: coturn-1.0.1
  name: coturn
  namespace: coturn
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: coturn
      app.kubernetes.io/name: coturn
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: coturn
        app.kubernetes.io/name: coturn
    spec:
      containers:
        - args:
            - --listening-port=3478
            - --tls-listening-port=5349
            - --user=netbird:$$COTURN_USER_netbird_KEY
            - --cert=/usr/local/etc/tls.crt
            - --pkey=/usr/local/etc/tls.key
            - --realm=coturn.homelab.olav.ninja
            - --log-file=stdout
            - --no-software-attribute
            - --no-cli
            - --listening-ip=0.0.0.0
          env:
            - name: COTURN_USER_netbird_KEY
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
          image: coturn/coturn:4.6.3-r2
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            tcpSocket:
              port: tcp
          name: coturn
          ports:
            - containerPort: 3478
              name: tcp
              protocol: TCP
            - containerPort: 3478
              name: udp
              protocol: UDP
            - containerPort: 5349
              name: tcp-tls
              protocol: TCP
            - containerPort: 5349
              name: udp-tls
              protocol: UDP
          readinessProbe:
            tcpSocket:
              port: tcp
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/turnserver.conf
              name: config
              subPath: turnserver.conf
            - mountPath: /usr/local/etc
              name: certs
      securityContext: {}
      serviceAccountName: coturn
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      volumes:
        - configMap:
            name: coturn
          name: config
        - name: certs
          secret:
            secretName: coturn
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    release: crossplane
  name: crossplane
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 1.20.0
        helm.sh/chart: crossplane-1.20.0
        release: crossplane
    spec:
      containers:
        - args:
            - core
            - start
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: LEADER_ELECTION
              value: "true"
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_SERVER_CERTS_DIR
              value: /tls/server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
            - name: TLS_CLIENT_CERTS_DIR
              value: /tls/client
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane
          ports:
            - containerPort: 8081
              name: readyz
            - containerPort: 9443
              name: webhooks
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
          startupProbe:
            failureThreshold: 30
            periodSeconds: 2
            tcpSocket:
              port: readyz
          volumeMounts:
            - mountPath: /cache/xpkg
              name: package-cache
            - mountPath: /cache/xfn
              name: function-cache
            - mountPath: /tls/server
              name: tls-server-certs
            - mountPath: /tls/client
              name: tls-client-certs
      hostNetwork: false
      initContainers:
        - args:
            - core
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: WEBHOOK_SERVICE_NAME
              value: crossplane-webhooks
            - name: WEBHOOK_SERVICE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: WEBHOOK_SERVICE_PORT
              value: "9443"
            - name: TLS_CA_SECRET_NAME
              value: crossplane-root-ca
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: crossplane
      volumes:
        - emptyDir:
            medium: null
            sizeLimit: 20Mi
          name: package-cache
        - emptyDir:
            medium: null
            sizeLimit: 512Mi
          name: function-cache
        - name: tls-server-certs
          secret:
            secretName: crossplane-tls-server
        - name: tls-client-certs
          secret:
            secretName: crossplane-tls-client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane-rbac-manager
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    release: crossplane
  name: crossplane-rbac-manager
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane-rbac-manager
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane-rbac-manager
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 1.20.0
        helm.sh/chart: crossplane-1.20.0
        release: crossplane
    spec:
      containers:
        - args:
            - rbac
            - start
            - --provider-clusterrole=crossplane:allowed-provider-permissions
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: LEADER_ELECTION
              value: "true"
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      initContainers:
        - args:
            - rbac
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: rbac-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: proxmox-csi-plugin
      app.kubernetes.io/name: proxmox-csi-plugin
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: ce080eff0c26b50fe73bf9fcda017c8ad47c1000729fd0c555cfe3535c6d6222
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: proxmox-csi-plugin
        app.kubernetes.io/name: proxmox-csi-plugin
    spec:
      containers:
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --cloud-config=/etc/proxmox/config.yaml
          image: ghcr.io/sergelogvinov/proxmox-csi-controller:v0.11.0
          imagePullPolicy: IfNotPresent
          name: proxmox-csi-plugin-controller
          ports: null
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
            - mountPath: /etc/proxmox/
              name: cloud-config
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --leader-election
            - --default-fstype=ext4
          image: registry.k8s.io/sig-storage/csi-attacher:v4.8.0
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --leader-election
            - --enable-capacity
            - --capacity-ownerref-level=2
            - --default-fstype=ext4
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          image: registry.k8s.io/sig-storage/csi-provisioner:v5.1.0
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --handle-volume-inuse-error=false
            - --leader-election
          image: registry.k8s.io/sig-storage/csi-resizer:v1.13.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
          image: registry.k8s.io/sig-storage/livenessprobe:v2.14.0
          imagePullPolicy: IfNotPresent
          name: liveness-probe
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
      enableServiceLinks: false
      hostAliases: []
      initContainers: []
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroup: 65532
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: proxmox-csi-plugin-controller
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: controller
              app.kubernetes.io/instance: proxmox-csi-plugin
              app.kubernetes.io/name: proxmox-csi-plugin
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
      volumes:
        - emptyDir: {}
          name: socket-dir
        - name: cloud-config
          secret:
            secretName: proxmox-csi-plugin
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: gitea
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: c6548b113d26834ff4e6aa38a9bd40912b13d1b1495004e5b1d1545a2d2bd355
        checksum/oauth_0: 01764dcd9b49e15763176a4daa8f8e10f4902756616d3e834ef1341e80062ad5
      labels:
        app: gitea
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: gitea
        app.kubernetes.io/version: 1.23.8
        helm.sh/chart: gitea-12.0.0
        version: 1.23.8
    spec:
      containers:
        - env:
            - name: SSH_LISTEN_PORT
              value: "2222"
            - name: SSH_PORT
              value: "22"
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMPDIR
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 200
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: gitea
          ports:
            - containerPort: 2222
              name: ssh
            - containerPort: 3000
              name: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      initContainers:
        - command:
            - /usr/sbinx/init_directory_structure.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          name: init-directories
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbinx
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
        - command:
            - /usr/sbinx/config_environment.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMP_EXISTING_ENVS_FILE
              value: /tmp/existing-envs
            - name: ENV_TO_INI_MOUNT_POINT
              value: /env-to-ini-mounts
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          name: init-app-ini
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbinx
              name: config
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
            - mountPath: /env-to-ini-mounts/inlines/
              name: inline-config-sources
        - command:
            - /usr/sbinx/configure_gitea.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
            - name: GITEA_OAUTH_KEY_0
              valueFrom:
                secretKeyRef:
                  key: key
                  name: gitea-oidc-credentials
            - name: GITEA_OAUTH_SECRET_0
              valueFrom:
                secretKeyRef:
                  key: secret
                  name: gitea-oidc-credentials
            - name: GITEA_ADMIN_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD_MODE
              value: keepUpdated
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          name: configure-gitea
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/sbinx
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      securityContext:
        fsGroup: 1000
      terminationGracePeriodSeconds: 60
      volumes:
        - name: init
          secret:
            defaultMode: 110
            secretName: gitea-init
        - name: config
          secret:
            defaultMode: 110
            secretName: gitea
        - name: inline-config-sources
          secret:
            secretName: gitea-inline-config
        - emptyDir: {}
          name: temp
        - name: data
          persistentVolumeClaim:
            claimName: gitea
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: hajimari
      app.kubernetes.io/name: hajimari
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: hajimari
        app.kubernetes.io/name: hajimari
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: TZ
              value: UTC
          image: ghcr.io/toboshii/hajimari:v0.3.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          name: hajimari
          ports:
            - containerPort: 3000
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 0
            periodSeconds: 5
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config/config.yaml
              name: hajimari-settings
              subPath: config.yaml
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: hajimari
      volumes:
        - configMap:
            name: hajimari-settings
          name: hajimari-settings
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-machine-learning
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: machine-learning
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: machine-learning
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql
            - name: DB_PASSWORD
              value: immich
            - name: DB_USERNAME
              value: immich
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
            - name: TRANSFORMERS_CACHE
              value: /cache
          image: ghcr.io/immich-app/immich-machine-learning:v1.132.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-machine-learning
          ports:
            - containerPort: 3003
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 3
              ephemeral-storage: 10Gi
            requests:
              cpu: 10m
              memory: 2Gi
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /cache
              name: cache
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - emptyDir: {}
          name: cache
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-server
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: server
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: 88c813a390a4e10d83e7da3a7df99e9aa98f2f6ce28cc391662822301a54b78e
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: server
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql
            - name: DB_PASSWORD
              value: immich
            - name: DB_USERNAME
              value: immich
            - name: IMMICH_CONFIG_FILE
              value: /config/immich-config.yaml
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
          image: ghcr.io/immich-app/immich-server:v1.132.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-server
          ports:
            - containerPort: 2283
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /usr/src/app/upload
              name: library
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - name: config
          secret:
            secretName: immich-config
        - name: library
          persistentVolumeClaim:
            claimName: immich-library
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: cilium-operator
    app.kubernetes.io/part-of: cilium
    io.cilium/app: operator
    name: cilium-operator
  name: cilium-operator
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      io.cilium/app: operator
      name: cilium-operator
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 100%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/port: "9963"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: cilium-operator
        app.kubernetes.io/part-of: cilium
        io.cilium/app: operator
        name: cilium-operator
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  io.cilium/app: operator
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
            - --debug=$(CILIUM_DEBUG)
          command:
            - cilium-operator-generic
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_DEBUG
              valueFrom:
                configMapKeyRef:
                  key: debug
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/operator-generic:v1.17.3@sha256:8bd38d0e97a955b2d725929d60df09d712fb62b60b930551a29abac2dd92e597
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
          name: cilium-operator
          ports:
            - containerPort: 9963
              hostPort: 9963
              name: prometheus
              protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp/cilium/config-map
              name: cilium-config-path
              readOnly: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      serviceAccountName: cilium-operator
      tolerations:
        - operator: Exists
      volumes:
        - configMap:
            name: cilium-config
          name: cilium-config-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-relay
  name: hubble-relay
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-relay
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cilium.io/hubble-relay-configmap-checksum: 0aebee6bdee393dd840ea0e068f2efeae387cc07114bb26becb030f0ab1e2397
      labels:
        app.kubernetes.io/name: hubble-relay
        app.kubernetes.io/part-of: cilium
        k8s-app: hubble-relay
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: false
      containers:
        - args:
            - serve
          command:
            - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.17.3@sha256:f8674b5139111ac828a8818da7f2d344b4a5bfbaeb122c5dc9abed3e74000c55
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 12
            grpc:
              port: 4222
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 10
          name: hubble-relay
          ports:
            - containerPort: 4245
              name: grpc
          readinessProbe:
            grpc:
              port: 4222
            timeoutSeconds: 3
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 4222
            initialDelaySeconds: 10
            periodSeconds: 3
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/hubble-relay
              name: config
              readOnly: true
            - mountPath: /var/lib/hubble-relay/tls
              name: tls
              readOnly: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: null
      restartPolicy: Always
      securityContext:
        fsGroup: 65532
      serviceAccountName: hubble-relay
      terminationGracePeriodSeconds: 1
      volumes:
        - configMap:
            items:
              - key: config.yaml
                path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: client.crt
                    - key: tls.key
                      path: client.key
                    - key: ca.crt
                      path: hubble-server-ca.crt
                  name: hubble-relay-client-certs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-ui
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cilium.io/hubble-ui-nginx-configmap-checksum: de069d2597e16e4de004ce684b15d74b2ab6051c717ae073d86199a76d91fcf1
      labels:
        app.kubernetes.io/name: hubble-ui
        app.kubernetes.io/part-of: cilium
        k8s-app: hubble-ui
    spec:
      automountServiceAccountToken: true
      containers:
        - image: quay.io/cilium/hubble-ui:v0.13.2@sha256:9e37c1296b802830834cc87342a9182ccbb71ffebb711971e849221bd9d59392
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
          name: frontend
          ports:
            - containerPort: 8081
              name: http
          readinessProbe:
            httpGet:
              path: /
              port: 8081
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/nginx/conf.d/default.conf
              name: hubble-ui-nginx-conf
              subPath: nginx.conf
            - mountPath: /tmp
              name: tmp-dir
        - env:
            - name: EVENTS_SERVER_PORT
              value: "8090"
            - name: FLOWS_API_ADDR
              value: hubble-relay:80
          image: quay.io/cilium/hubble-ui-backend:v0.13.2@sha256:a034b7e98e6ea796ed26df8f4e71f83fc16465a19d166eff67a03b822c0bfa15
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
            - containerPort: 8090
              name: grpc
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts: null
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: null
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      serviceAccountName: hubble-ui
      volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.8.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.13
  name: proxmox-cloud-controller-manager
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: proxmox-cloud-controller-manager
      app.kubernetes.io/name: proxmox-cloud-controller-manager
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: ce080eff0c26b50fe73bf9fcda017c8ad47c1000729fd0c555cfe3535c6d6222
      labels:
        app.kubernetes.io/instance: proxmox-cloud-controller-manager
        app.kubernetes.io/name: proxmox-cloud-controller-manager
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      containers:
        - args:
            - --v=4
            - --cloud-provider=proxmox
            - --cloud-config=/etc/proxmox/config.yaml
            - --controllers=cloud-node,cloud-node-lifecycle
            - --leader-elect-resource-name=cloud-controller-manager-proxmox
            - --use-service-account-credentials
            - --secure-port=10258
            - --authorization-always-allow-paths=/healthz,/livez,/readyz,/metrics
          image: ghcr.io/sergelogvinov/proxmox-cloud-controller-manager:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 30
            timeoutSeconds: 5
          name: proxmox-cloud-controller-manager
          ports:
            - containerPort: 10258
              name: metrics
              protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /etc/proxmox
              name: cloud-config
              readOnly: true
      enableServiceLinks: false
      initContainers: []
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroup: 10258
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 10258
        runAsNonRoot: true
        runAsUser: 10258
      serviceAccountName: proxmox-cloud-controller-manager
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          operator: Exists
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: proxmox-cloud-controller-manager
              app.kubernetes.io/name: proxmox-cloud-controller-manager
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
      volumes:
        - name: cloud-config
          secret:
            defaultMode: 416
            items:
              - key: config.yaml
                path: config.yaml
            secretName: proxmox-ccm-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netbird-agent
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app: netbird-agent
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: netbird-agent
      labels:
        app: netbird-agent
    spec:
      containers:
        - args:
            - TCP-LISTEN:443,fork
            - TCP:traefik.traefik.svc.cluster.local:443
          image: alpine/socat:1.8.0.3
          name: traefik-proxy
          ports:
            - containerPort: 443
              name: http
              protocol: TCP
        - args:
            - TCP-LISTEN:22,fork
            - TCP:traefik.traefik.svc.cluster.local:22
          image: alpine/socat:1.8.0.3
          name: gitea-ssh-proxy
          ports:
            - containerPort: 22
              name: ssh
              protocol: TCP
        - env:
            - name: CLOUDFLARE_API_TOKEN
              valueFrom:
                secretKeyRef:
                  key: cloudflare-dns-api-token
                  name: cloudflare-api-credentials
            - name: DOMAINS
              value: homelab.olav.ninja,*.homelab.olav.ninja
            - name: IP4_PROVIDER
              value: local.iface:wt0
            - name: IP6_PROVIDER
              value: none
            - name: UPDATE_CRON
              value: '@every 5m'
          image: favonia/cloudflare-ddns:1.15.1
          name: cloudflare-dns-updater
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
        - env:
            - name: NB_SETUP_KEY
              valueFrom:
                secretKeyRef:
                  key: setupKey
                  name: netbird-agent-setup-key
            - name: NB_HOSTNAME
              value: k8s-agent
            - name: NB_LOG_LEVEL
              value: info
            - name: NB_MANAGEMENT_URL
              value: https://netbird.homelab.olav.ninja
            - name: NB_ADMIN_URL
              value: https://netbird.homelab.olav.ninja
            - name: NB_CONFIG
              value: /config/config.json
          image: netbirdio/netbird:0.45.1
          name: netbird-agent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_ADMIN
                - PERFMON
                - BPF
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /config
              name: config
      volumes:
        - emptyDir: {}
          name: config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-management
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: a7d495890f56d73e445f354af57bb002ed4c977911c8422a1650a4a531d69800
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-management
    spec:
      containers:
        - args:
            - --log-level
            - info
            - --log-file
            - console
            - --dns-domain
            - netbird
          image: netbirdio/management:0.45.1
          imagePullPolicy: IfNotPresent
          name: netbird-management
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /var/lib/netbird
              name: management
      initContainers:
        - args:
            - |
              go install github.com/drone/envsubst/cmd/envsubst@latest && envsubst < /tmp/netbird/management.tmpl.json > /etc/netbird/management.json && cat /etc/netbird/management.json
          command:
            - /bin/sh
            - -c
          env:
            - name: NETBIRD_SIGNAL_URI
              value: netbird.homelab.olav.ninja:443
            - name: NETBIRD_SIGNAL_PROTOCOL
              value: https
            - name: NETBIRD_STUN_URI
              value: stun:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_URI
              value: turn:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_USER
              value: netbird
            - name: NETBIRD_TURN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
            - name: NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: NETBIRD_MGMT_API_CERT_FILE
              value: ""
            - name: NETBIRD_MGMT_API_CERT_KEY_FILE
              value: ""
            - name: NETBIRD_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_USER_ID_CLAIM
            - name: NETBIRD_AUTH_DEVICE_AUTH_PROVIDER
              value: hosted
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/auth
            - name: NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
            - name: NETBIRD_AUTH_DEVICE_AUTH_SCOPE
              value: openid
            - name: NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN
              value: "false"
            - name: NETBIRD_IDP_MANAGER_TYPE
              value: keycloak
            - name: NETBIRD_IDP_CLIENT_ID
              value: netbird-backend
            - name: NETBIRD_IDP_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: netbird-backend-oidc-credentials
            - name: NETBIRD_IDP_GRANT_TYPE
              value: client_credentials
            - name: NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/admin/realms/homelab
            - name: NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
          envFrom:
            - secretRef:
                name: netbird-relay-secret
          image: golang:latest
          imagePullPolicy: IfNotPresent
          name: configure
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /tmp/netbird
              name: config-template
      securityContext: {}
      serviceAccountName: netbird-backend-management
      volumes:
        - emptyDir:
            medium: Memory
          name: config
        - configMap:
            name: netbird-backend-management
          name: config-template
        - name: management
          persistentVolumeClaim:
            claimName: netbird-backend-management
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-relay
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-relay
    spec:
      containers:
        - args:
            - --listen-address
            - :80
            - --log-level
            - info
            - --log-file
            - console
          envFrom:
            - secretRef:
                name: netbird-relay-secret
          image: netbirdio/relay:0.45.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: 80
          name: netbird-relay
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: 80
          resources: {}
          securityContext: null
      securityContext: null
      serviceAccountName: netbird-backend-relay
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-signal
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-signal
    spec:
      containers:
        - args:
            - --port
            - "80"
            - --log-level
            - info
            - --log-file
            - console
          image: netbirdio/signal:0.45.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: https
          name: netbird-signal
          ports:
            - containerPort: 80
              name: https
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: https
          resources: {}
          securityContext: null
          volumeMounts:
            - mountPath: /var/lib/netbird
              name: signal
      securityContext: null
      serviceAccountName: netbird-backend-signal
      volumes:
        - name: signal
          persistentVolumeClaim:
            claimName: netbird-backend-signal
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-dashboard
      app.kubernetes.io/name: netbird-dashboard
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-dashboard
        app.kubernetes.io/name: netbird-dashboard
    spec:
      containers:
        - args:
            - |
              sed -i 's/listen \[\:\:\]\:80 default_server\;//g' /etc/nginx/http.d/default.conf && /usr/bin/supervisord -c /etc/supervisord.conf
          command:
            - /bin/sh
            - -c
          env:
            - name: AUTH_AUDIENCE
              value: netbird
            - name: AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: AUTH_CLIENT_ID
              value: netbird
            - name: AUTH_SUPPORTED_SCOPES
              value: openid profile email offline_access netbird-api
            - name: USE_AUTH0
              value: "false"
            - name: NETBIRD_MGMT_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
            - name: NETBIRD_MGMT_GRPC_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
          image: netbirdio/dashboard:v2.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /
              port: http
          name: netbird-dashboard
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources: {}
          securityContext: {}
      securityContext: {}
      serviceAccountName: netbird-dashboard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud
  namespace: nextcloud
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: app
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: nextcloud
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204
        nextcloud-config-hash: 93a9742eed1608d05f159d9fa1830f38ea212bcc9440dceccdf0528d6ed6aefc
        php-config-hash: ef6725a34482d427b584b8ddff54804f14e7c98827ae18ff7642f7cf0dd254af
      labels:
        app.kubernetes.io/component: app
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/name: nextcloud
    spec:
      containers:
        - env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: MYSQL_HOST
              value: nextcloud-mariadb
            - name: MYSQL_DATABASE
              value: nextcloud
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
          image: nextcloud:30.0.10-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nextcloud
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
        - command:
            - /cron.sh
          env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: MYSQL_HOST
              value: nextcloud-mariadb
            - name: MYSQL_DATABASE
              value: nextcloud
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
          image: nextcloud:30.0.10-apache
          imagePullPolicy: IfNotPresent
          name: nextcloud-cron
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
      initContainers:
        - command:
            - sh
            - -c
            - until mysql --host=nextcloud-mariadb --user=${MYSQL_USER} --password=${MYSQL_PASSWORD} --execute="SELECT 1;"; do echo waiting for mysql; sleep 2; done;
          env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          name: mariadb-isalive
          resources: {}
          securityContext: {}
      securityContext:
        fsGroup: 33
      volumes:
        - name: nextcloud-main
          persistentVolumeClaim:
            claimName: nextcloud-data
        - configMap:
            name: nextcloud-config
          name: nextcloud-config
        - configMap:
            name: nextcloud-phpconfig
          name: nextcloud-phpconfig
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.7.0
    helm.sh/chart: ollama-1.17.0
  name: ollama
  namespace: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: ollama
      app.kubernetes.io/name: ollama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: ollama
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: ollama
        app.kubernetes.io/version: 0.7.0
        helm.sh/chart: ollama-1.17.0
    spec:
      containers:
        - args: null
          env:
            - name: OLLAMA_HOST
              value: 0.0.0.0:11434
          envFrom: null
          image: ollama/ollama:0.7.0-rocm
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: ollama
          ports:
            - containerPort: 11434
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              amd.com/gpu: 1
            requests: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /root/.ollama
              name: ollama-data
      securityContext: {}
      serviceAccountName: ollama
      tolerations: null
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: sealed-secrets
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: sealed-secrets
        app.kubernetes.io/version: 0.29.0
        helm.sh/chart: sealed-secrets-2.5.12
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: sealed-secrets
                    app.kubernetes.io/name: sealed-secrets
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - args:
            - --key-prefix
            - sealed-secrets-key
            - --update-status
            - --key-renew-period
            - "0"
          command:
            - controller
          image: docker.io/bitnami/sealed-secrets-controller:0.29.0-debian-12-r4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: sealed-secrets
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: sealed-secrets
      volumes:
        - emptyDir: {}
          name: empty-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
  namespace: traefik
spec:
  minReadySeconds: 0
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: traefik-traefik
      app.kubernetes.io/name: traefik
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9100"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: traefik-traefik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: traefik
        helm.sh/chart: traefik-35.4.0
    spec:
      automountServiceAccountToken: true
      containers:
        - args:
            - --global.checknewversion
            - --global.sendanonymoususage
            - --entryPoints.metrics.address=:9100/tcp
            - --entryPoints.ssh.address=:2222/tcp
            - --entryPoints.traefik.address=:8080/tcp
            - --entryPoints.web.address=:8000/tcp
            - --entryPoints.webpublic.address=:9443/tcp
            - --entryPoints.websecure.address=:8443/tcp
            - --entryPoints.websecure.asDefault=true
            - --api.dashboard=true
            - --ping=true
            - --metrics.prometheus=true
            - --metrics.prometheus.entrypoint=metrics
            - --providers.kubernetescrd
            - --providers.kubernetescrd.allowEmptyServices=true
            - --providers.kubernetesingress
            - --providers.kubernetesingress.allowEmptyServices=true
            - --providers.kubernetesingress.ingressendpoint.publishedservice=traefik/traefik
            - --entryPoints.webpublic.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.webpublic.http.tls=true
            - --entryPoints.webpublic.transport.respondingTimeouts.readTimeout=0
            - --entryPoints.websecure.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.websecure.http.tls=true
            - --entryPoints.websecure.transport.respondingTimeouts.readTimeout=0
            - --log.level=INFO
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: docker.io/traefik:v3.4.0
          imagePullPolicy: IfNotPresent
          lifecycle: null
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
            - containerPort: 9100
              name: metrics
              protocol: TCP
            - containerPort: 2222
              name: ssh
              protocol: TCP
            - containerPort: 8080
              name: traefik
              protocol: TCP
            - containerPort: 8000
              name: web
              protocol: TCP
            - containerPort: 9443
              name: webpublic
              protocol: TCP
            - containerPort: 8443
              name: websecure
              protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: null
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /tmp
              name: tmp
      hostNetwork: false
      securityContext:
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: traefik
      terminationGracePeriodSeconds: 60
      volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
  serviceName: gitea-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.5.0
        helm.sh/chart: postgresql-16.7.2
      name: gitea-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: gitea
            - name: POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/password
            - name: POSTGRES_POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/postgres-password
            - name: POSTGRES_DATABASE
              value: gitea
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/bitnami/postgresql:15-debian-11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/postgresql/secrets/
              name: postgresql-password
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: gitea-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            secretName: gitea-postgresql
        - emptyDir:
            medium: Memory
          name: dshm
        - name: data
          persistentVolumeClaim:
            claimName: gitea-postgresql
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster
  namespace: gitea
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey-cluster
  serviceName: gitea-valkey-cluster-headless
  template:
    metadata:
      annotations:
        checksum/config: 13a239c0e8e5e8586e3c4418ffe870f2f98fddf2043793275ec8587cc3b7f8a4
        checksum/scripts: 6596a15dbc923287a4ae3b90025f13d760b8a6e37f9f01415ea8edbada6b1dfc
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      labels:
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: valkey-cluster
        app.kubernetes.io/version: 8.1.1
        helm.sh/chart: valkey-cluster-3.0.5
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: valkey-cluster
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - |
              # Backwards compatibility change
              if ! [[ -f /opt/bitnami/valkey/etc/valkey.conf ]]; then
                  echo COPYING FILE
                  cp  /opt/bitnami/valkey/etc/valkey-default.conf /opt/bitnami/valkey/etc/valkey.conf
              fi
              pod_index=($(echo "$POD_NAME" | tr "-" "\n"))
              pod_index="${pod_index[-1]}"
              if [[ "$pod_index" == "0" ]]; then
                export VALKEY_CLUSTER_CREATOR="yes"
                export VALKEY_CLUSTER_REPLICAS="0"
              fi
              /opt/bitnami/scripts/valkey-cluster/entrypoint.sh /opt/bitnami/scripts/valkey-cluster/run.sh
          command:
            - /bin/bash
            - -c
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: VALKEY_NODES
              value: 'gitea-valkey-cluster-0.gitea-valkey-cluster-headless gitea-valkey-cluster-1.gitea-valkey-cluster-headless gitea-valkey-cluster-2.gitea-valkey-cluster-headless '
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: VALKEY_AOF_ENABLED
              value: "yes"
            - name: VALKEY_TLS_ENABLED
              value: "no"
            - name: VALKEY_PORT_NUMBER
              value: "6379"
          image: docker.io/bitnami/valkey-cluster:8.1.1-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /scripts/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: gitea-valkey-cluster
          ports:
            - containerPort: 6379
              name: tcp-redis
            - containerPort: 16379
              name: tcp-redis-bus
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /scripts/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /scripts
              name: scripts
            - mountPath: /bitnami/valkey/data
              name: valkey-data
              subPath: null
            - mountPath: /opt/bitnami/valkey/etc/valkey-default.conf
              name: default-config
              subPath: valkey-default.conf
            - mountPath: /opt/bitnami/valkey/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/valkey/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/valkey/logs
              name: empty-dir
              subPath: app-logs-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: gitea-valkey-cluster
      volumes:
        - configMap:
            defaultMode: 493
            name: gitea-valkey-cluster-scripts
          name: scripts
        - configMap:
            name: gitea-valkey-cluster-default
          name: default-config
        - emptyDir: {}
          name: empty-dir
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
  volumeClaimTemplates:
    - metadata:
        labels:
          app.kubernetes.io/instance: gitea
          app.kubernetes.io/name: valkey-cluster
        name: valkey-data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: postgresql
  serviceName: immich-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: immich
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.0.0
        helm.sh/chart: postgresql-16.0.0
      name: immich-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: immich
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: immich-postgresql
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: postgres-password
                  name: immich-postgresql
            - name: POSTGRES_DATABASE
              value: immich
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "immich" -d "dbname=immich" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "immich" -d "dbname=immich" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 1536Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 1024Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 999
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /docker-entrypoint-initdb.d/
              name: custom-init-scripts
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      initContainers:
        - command:
            - /bin/sh
            - -ec
            - |
              chown 999:999 /bitnami/postgresql
              mkdir -p /bitnami/postgresql/data
              chmod 700 /bitnami/postgresql/data
              find /bitnami/postgresql -mindepth 1 -maxdepth 1 -not -name "conf" -not -name ".snapshot" -not -name "lost+found" | \
                xargs -r chown -R 999:999
              chmod -R 777 /dev/shm
          image: docker.io/bitnami/os-shell:12-debian-12-r30
          imagePullPolicy: IfNotPresent
          name: init-chmod-data
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /bitnami/postgresql
              name: data
            - mountPath: /dev/shm
              name: dshm
      securityContext:
        fsGroup: 999
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: immich-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            name: immich-postgresql-init-scripts
          name: custom-init-scripts
        - emptyDir:
            medium: Memory
          name: dshm
        - name: data
          persistentVolumeClaim:
            claimName: immich-postgresql
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  serviceName: immich-redis-headless
  template:
    metadata:
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 43cdf68c28f3abe25ce017a82f74dbf2437d1900fd69df51a55a3edf6193d141
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: immich
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.5
        helm.sh/chart: redis-19.5.3
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          image: docker.io/bitnami/redis:7.2.5-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /data
              name: redis-data
            - mountPath: /opt/bitnami/redis/mounted-etc
              name: config
            - mountPath: /opt/bitnami/redis/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: immich-redis-master
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: immich-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: immich-redis-health
          name: health
        - configMap:
            name: immich-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: redis-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
  serviceName: keycloak-headless
  template:
    metadata:
      annotations:
        checksum/configmap-env-vars: d80c992feef3a8b41fed394f5fb5434812268953473a2893746fd449c0ae97c7
      labels:
        app.kubernetes.io/app-version: 26.2.5
        app.kubernetes.io/component: keycloak
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: keycloak
        app.kubernetes.io/version: 26.2.5
        helm.sh/chart: keycloak-24.7.3
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: keycloak
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - env:
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KC_BOOTSTRAP_ADMIN_PASSWORD_FILE
              value: /opt/bitnami/keycloak/secrets/pazzword
            - name: KEYCLOAK_DATABASE_PASSWORD_FILE
              value: /opt/bitnami/keycloak/secrets/db-password
            - name: KEYCLOAK_HTTP_RELATIVE_PATH
              value: /
            - name: KC_SPI_ADMIN_REALM
              value: master
          envFrom:
            - configMapRef:
                name: keycloak-env-vars
          image: docker.io/bitnami/keycloak:26.2.5-debian-12-r1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 300
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 5
          name: keycloak
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 7800
              name: discovery
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /realms/master
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /bitnami/keycloak
              name: empty-dir
              subPath: app-volume-dir
            - mountPath: /opt/bitnami/keycloak/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/keycloak/lib/quarkus
              name: empty-dir
              subPath: app-quarkus-dir
            - mountPath: /opt/bitnami/keycloak/data
              name: empty-dir
              subPath: app-data-dir
            - mountPath: /opt/bitnami/keycloak/providers
              name: empty-dir
              subPath: app-providers-dir
            - mountPath: /opt/bitnami/keycloak/themes
              name: empty-dir
              subPath: app-themes-dir
            - mountPath: /opt/bitnami/keycloak/secrets
              name: keycloak-secrets
      enableServiceLinks: true
      initContainers:
        - args:
            - -ec
            - |
              . /opt/bitnami/scripts/liblog.sh

              info "Copying writable dirs to empty dir"
              # In order to not break the application functionality we need to make some
              # directories writable, so we need to copy it to an empty dir volume
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/data /emptydir/app-data-dir
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/providers /emptydir/app-providers-dir
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/themes /emptydir/app-themes-dir
              info "Copy operation completed"
          command:
            - /bin/bash
          image: docker.io/bitnami/keycloak:26.2.5-debian-12-r1
          imagePullPolicy: IfNotPresent
          name: prepare-write-dirs
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /emptydir
              name: empty-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: keycloak
      volumes:
        - emptyDir: {}
          name: empty-dir
        - name: keycloak-secrets
          projected:
            sources:
              - secret:
                  name: keycloak-admin-password
              - secret:
                  items:
                    - key: password
                      path: db-password
                  name: keycloak-db-credentials
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
  serviceName: keycloak-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.4.0
        helm.sh/chart: postgresql-16.6.6
      name: keycloak-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: bn_keycloak
            - name: POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/password
            - name: POSTGRES_POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/postgres-password
            - name: POSTGRES_DATABASE
              value: bitnami_keycloak
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/bitnami/postgresql:17.4.0-debian-12-r17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "bn_keycloak" -d "dbname=bitnami_keycloak" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "bn_keycloak" -d "dbname=bitnami_keycloak" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/postgresql/secrets/
              name: postgresql-password
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: keycloak-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            secretName: keycloak-db-credentials
        - emptyDir:
            medium: Memory
          name: dshm
        - emptyDir: {}
          name: data
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: mariadb
  serviceName: nextcloud-mariadb
  template:
    metadata:
      annotations:
        checksum/configuration: fb262c2f871ddf4c9fa925e29eb7198153cfc48305559085fcf1ef74cf716807
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mariadb
        app.kubernetes.io/version: 11.3.2
        helm.sh/chart: mariadb-18.2.0
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: mariadb
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: mariadb
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MARIADB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-root-password
                  name: nextcloud-database-auth
            - name: MARIADB_USER
              value: nextcloud
            - name: MARIADB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: MARIADB_DATABASE
              value: nextcloud
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mariadb
          ports:
            - containerPort: 3306
              name: mysql
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin ping -uroot -p"${password_aux}"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /bitnami/mariadb
              name: data
            - mountPath: /opt/bitnami/mariadb/conf/my.cnf
              name: config
              subPath: my.cnf
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/mariadb/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/mariadb/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/mariadb/logs
              name: empty-dir
              subPath: app-logs-dir
      initContainers:
        - args:
            - -ec
            - |
              #!/bin/bash

              . /opt/bitnami/scripts/libfs.sh
              # We copy the logs folder because it has symlinks to stdout and stderr
              if ! is_dir_empty /opt/bitnami/mariadb/logs; then
                cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir
              fi
          command:
            - /bin/bash
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          imagePullPolicy: IfNotPresent
          name: preserve-logs-symlinks
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /emptydir
              name: empty-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: nextcloud-mariadb
      volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            name: nextcloud-mariadb
          name: config
        - name: data
          persistentVolumeClaim:
            claimName: nextcloud-mariadb
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.10
    helm.sh/chart: open-webui-6.16.0
  name: open-webui
  namespace: openwebui
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: open-webui
      app.kubernetes.io/instance: openwebui
  serviceName: open-webui
  template:
    metadata:
      labels:
        app.kubernetes.io/component: open-webui
        app.kubernetes.io/instance: openwebui
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: 0.6.10
        helm.sh/chart: open-webui-6.16.0
    spec:
      automountServiceAccountToken: false
      containers:
        - env:
            - name: OLLAMA_BASE_URLS
              value: http://ollama.ollama.svc.cluster.local:11434
            - name: OPENAI_API_BASE_URL
              value: https://api.openai.com/v1
            - name: ENABLE_OPENAI_API
              value: "False"
            - name: ENABLE_OAUTH_SIGNUP
              value: "True"
            - name: OAUTH_MERGE_ACCOUNTS_BY_EMAIL
              value: "True"
            - name: OAUTH_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  key: clientId
                  name: openwebui-oidc-credentials
            - name: OAUTH_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: openwebui-oidc-credentials
            - name: OPENID_PROVIDER_URL
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: OAUTH_PROVIDER_NAME
              value: Keycloak
            - name: OAUTH_SCOPES
              value: openid email profile
          image: ghcr.io/open-webui/open-webui:0.6.10
          imagePullPolicy: IfNotPresent
          name: open-webui
          ports:
            - containerPort: 8080
              name: http
          tty: true
          volumeMounts:
            - mountPath: /app/backend/data
              name: data
      enableServiceLinks: false
      initContainers:
        - command:
            - sh
            - -c
            - cp -R -n /app/backend/data/* /tmp/app-data/
          image: ghcr.io/open-webui/open-webui:0.6.10
          imagePullPolicy: IfNotPresent
          name: copy-app-data
          volumeMounts:
            - mountPath: /tmp/app-data
              name: data
      serviceAccountName: open-webui
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: open-webui
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: energisk-cache-prime
  namespace: default
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - args:
                - |
                  URLS=" https://api.energisk.app/price?area=BE&currency=EUR https://api.energisk.app/price?area=BG&currency=BGN https://api.energisk.app/price?area=CH&currency=CHF https://api.energisk.app/price?area=CZ&currency=CZK https://api.energisk.app/price?area=DE&currency=EUR https://api.energisk.app/price?area=DK1&currency=DKK https://api.energisk.app/price?area=DK2&currency=DKK https://api.energisk.app/price?area=EE&currency=EUR https://api.energisk.app/price?area=ES&currency=EUR https://api.energisk.app/price?area=FI&currency=EUR https://api.energisk.app/price?area=FR&currency=EUR https://api.energisk.app/price?area=GR&currency=EUR https://api.energisk.app/price?area=HR&currency=EUR https://api.energisk.app/price?area=HU&currency=HUF https://api.energisk.app/price?area=IE&currency=EUR https://api.energisk.app/price?area=LT&currency=EUR https://api.energisk.app/price?area=LU&currency=EUR https://api.energisk.app/price?area=LV&currency=EUR https://api.energisk.app/price?area=NL&currency=EUR https://api.energisk.app/price?area=NO1&currency=NOK https://api.energisk.app/price?area=NO2&currency=NOK https://api.energisk.app/price?area=NO3&currency=NOK https://api.energisk.app/price?area=NO4&currency=NOK https://api.energisk.app/price?area=NO5&currency=NOK https://api.energisk.app/price?area=NORD&currency=EUR https://api.energisk.app/price?area=PL&currency=PLN https://api.energisk.app/price?area=PT&currency=EUR https://api.energisk.app/price?area=RO&currency=RON https://api.energisk.app/price?area=RS&currency=RSD https://api.energisk.app/price?area=SARD&currency=EUR https://api.energisk.app/price?area=SE1&currency=SEK https://api.energisk.app/price?area=SE2&currency=SEK https://api.energisk.app/price?area=SE3&currency=SEK https://api.energisk.app/price?area=SE4&currency=SEK https://api.energisk.app/price?area=SI&currency=EUR https://api.energisk.app/price?area=SK&currency=EUR https://api.energisk.app/price?area=SICI&currency=EUR https://api.energisk.app/price?area=SUD&currency=EUR "
                  for URL in $URLS; do

                    curl -s -o /dev/null -w '%{url_effective}\t%{http_code}\t%{time_total}s\n' $URL\&date=$(date -d tomorrow +'%Y-%m-%d');
                  done
              command:
                - /bin/sh
                - -c
              image: rockylinux:9-minimal
              name: curl-container
          restartPolicy: Never
  schedule: 0 13 * * *
---
apiVersion: batch/v1
kind: CronJob
metadata:
  labels:
    app.kubernetes.io/name: hubble-generate-certs
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-generate-certs
  name: hubble-generate-certs
  namespace: kube-system
spec:
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            k8s-app: hubble-generate-certs
        spec:
          affinity: null
          automountServiceAccountToken: true
          containers:
            - args:
                - --ca-generate=true
                - --ca-reuse-secret
                - --ca-secret-namespace=kube-system
                - --ca-secret-name=cilium-ca
                - --ca-common-name=Cilium CA
              command:
                - /usr/bin/cilium-certgen
              env:
                - name: CILIUM_CERTGEN_CONFIG
                  value: |
                    certs:
                    - name: hubble-server-certs
                      namespace: kube-system
                      commonName: "*.default.hubble-grpc.cilium.io"
                      hosts:
                      - "*.default.hubble-grpc.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - server auth
                      - client auth
                      validity: 8760h
                    - name: hubble-relay-client-certs
                      namespace: kube-system
                      commonName: "*.hubble-relay.cilium.io"
                      hosts:
                      - "*.hubble-relay.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - client auth
                      validity: 8760h
              image: quay.io/cilium/certgen:v0.2.1@sha256:ab6b1928e9c5f424f6b0f51c68065b9fd85e2f8d3e5f21fbd1a3cb27e6fb9321
              imagePullPolicy: IfNotPresent
              name: certgen
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
          hostNetwork: false
          restartPolicy: OnFailure
          securityContext:
            seccompProfile:
              type: RuntimeDefault
          serviceAccount: hubble-generate-certs
          serviceAccountName: hubble-generate-certs
      ttlSecondsAfterFinished: 1800
  schedule: 0 0 1 */4 *
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster
  namespace: gitea
spec:
  maxUnavailable: 1
  selector:
    matchExpressions:
      - key: job-name
        operator: NotIn
        values:
          - gitea-valkey-cluster-cluster-update
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey-cluster
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: mariadb
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.12
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
---
apiVersion: apiextensions.crossplane.io/v1
kind: CompositeResourceDefinition
metadata:
  name: xoidcclients.oidc.homelab.olav.ninja
  namespace: crossplane
spec:
  group: oidc.homelab.olav.ninja
  names:
    kind: XOidcClient
    plural: xoidcclients
  versions:
    - name: v1alpha1
      referenceable: true
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                baseUrl:
                  description: Default URL to use when the auth server needs to redirect or link back to the client.
                  type: string
                clientId:
                  description: The Client ID for this client, referenced in the URI during authentication and in issued tokens
                  type: string
                clientSecretSecretRef:
                  description: |-
                    The client or client secret registered within the identity provider. This field is able to obtain its value from vault, use $${vault.ID} format.
                    Client Secret.
                  properties:
                    key:
                      description: The key to select.
                      type: string
                    name:
                      description: Name of the secret.
                      type: string
                    namespace:
                      description: Namespace of the secret.
                      type: string
                  required:
                    - key
                    - name
                    - namespace
                  type: object
                defaultScopes:
                  description: The default scopes to be requested when asking for authorization
                  items:
                    type: string
                  type: array
                description:
                  description: The description of this client in the GUI
                  type: string
                displayName:
                  description: The display name of this client in the GUI
                  type: string
                grantTypes:
                  description: A list of grant types that should be enabled for the client
                  items:
                    type: string
                  type: array
                postLogoutRedirectUris:
                  description: A list of valid URIs a browser is permitted to redirect to after a successful logout.
                  items:
                    type: string
                  type: array
                realm:
                  description: The realm this client is attached to
                  type: string
                redirectUris:
                  description: |-
                    A list of valid URIs a browser is permitted to redirect to after a successful login or logout. Simple
                    wildcards in the form of an asterisk can be used here. This attribute must be set if either standard_flow_enabled or implicit_flow_enabled
                    is set to true.
                  items:
                    type: string
                  type: array
                serviceAccountRoles:
                  description: A list of roles to assign to the clients service account
                  items:
                    properties:
                      client:
                        type: string
                      realm:
                        type: string
                      role:
                        type: string
                    type: object
                  type: array
                type:
                  description: Specifies the type of client
                  type: string
                webOrigins:
                  description: |-
                    A list of allowed CORS origins. To permit all valid
                    redirect URIs, add +. Note that this will not include the *
                    wildcard. To permit all origins, explicitly add *.
                  items:
                    type: string
                  type: array
              required:
                - clientId
                - realm
              type: object
          required:
            - spec
          type: object
      served: true
---
apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: keycloak-oidc-client
  namespace: crossplane
spec:
  compositeTypeRef:
    apiVersion: oidc.homelab.olav.ninja/v1alpha1
    kind: XOidcClient
  mode: Pipeline
  pipeline:
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: Client
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}
            spec:
              forProvider:
                name: {{ .observed.composite.resource.spec.displayName }}
                accessType: {{ .observed.composite.resource.spec.type }}
                clientId: {{ .observed.composite.resource.spec.clientId }}
                {{ with .observed.composite.resource.spec.clientSecretSecretRef }}
                clientSecretSecretRef: {{ toYaml . | nindent 6 }}
                {{ end }}
                description: {{ .observed.composite.resource.spec.description }}
                {{ with .observed.composite.resource.spec.baseUrl }}
                baseUrl: {{ . }}
                {{ end }}
                {{ with .observed.composite.resource.spec.redirectUris }}
                validRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.postLogoutRedirectUris }}
                validPostLogoutRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.webOrigins }}
                webOrigins: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ if has "client_credentials" .observed.composite.resource.spec.grantTypes }}
                serviceAccountsEnabled: true
                {{ end }}
                {{ if has "code" .observed.composite.resource.spec.grantTypes }}
                standardFlowEnabled: true
                {{ end }}
                {{ if has "device_code" .observed.composite.resource.spec.grantTypes }}
                oauth2DeviceAuthorizationGrantEnabled: true
                {{ end }}
                {{ if has "password" .observed.composite.resource.spec.grantTypes }}
                directAccessGrantsEnabled: true
                {{- end }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
        kind: GoTemplate
        source: Inline
      step: create-client
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-audience-mapper
            spec:
              forProvider:
                name: Set token audience
                protocol: openid-connect
                protocolMapper: oidc-audience-mapper
                config:
                  included.client.audience: "{{ .observed.composite.resource.spec.clientId }}"
                  id.token.claim: "false"
                  access.token.claim: "true"
                  introspection.token.claim: "true"
                  userinfo.token.claim: "false"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-audience-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-sub-mapper
            spec:
              forProvider:
                name: Username as sub claim
                protocol: openid-connect
                protocolMapper: oidc-usermodel-property-mapper
                config:
                  user.attribute: username
                  id.token.claim: "true"
                  access.token.claim: "true"
                  claim.name: sub
                  userinfo.token.claim: "true"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-usermodel-property-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            {{ if .observed.composite.resource.spec.defaultScopes }}
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientDefaultScopes
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-default-scopes
            spec:
              forProvider:
                {{ with .observed.composite.resource.spec.defaultScopes }}
                defaultScopes: {{ toYaml . | nindent 6 }}
                {{ end }}
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-client-default-scopes
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ range .observed.composite.resource.spec.serviceAccountRoles }}
            ---
            apiVersion: meta.gotemplating.fn.crossplane.io/v1alpha1
            kind: ExtraResources
            requirements:
              client:
                apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
                kind: Client
                matchName: {{ .client }}
              realm:
                apiVersion: realm.keycloak.crossplane.io/v1alpha1
                kind: Realm
                matchName: {{ .realm }}
            {{ end }}
            {{ if and (ne .observed.resources nil) (ne .extraResources nil) }}
            {{ range $i, $serviceAccountRole := .observed.composite.resource.spec.serviceAccountRoles }}
            {{ $client := (index (index $.extraResources "client").items $i).resource }}
            {{ $realm := (index (index $.extraResources "realm").items $i).resource }}
            ---
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientServiceAccountRole
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ $.observed.composite.resource.metadata.name }}-{{ .role }}
            spec:
              forProvider:
                clientId: {{ $client.status.atProvider.id }}
                realmId: {{ $realm.status.atProvider.id }}
                role: {{ $serviceAccountRole.role }}
                serviceAccountUserId: {{ ( index $.observed.resources $.observed.composite.resource.metadata.name ).resource.status.atProvider.serviceAccountUserId | default "null" }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-service-account-role
    - functionRef:
        name: function-auto-ready
      step: automatically-detect-ready-composed-resources
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.11.0
    helm.sh/chart: proxmox-csi-plugin-0.3.7
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: node
      app.kubernetes.io/instance: proxmox-csi-plugin
      app.kubernetes.io/name: proxmox-csi-plugin
  template:
    metadata:
      labels:
        app.kubernetes.io/component: node
        app.kubernetes.io/instance: proxmox-csi-plugin
        app.kubernetes.io/name: proxmox-csi-plugin
    spec:
      containers:
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --node-id=$(NODE_NAME)
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          image: ghcr.io/sergelogvinov/proxmox-csi-node:v0.11.0
          imagePullPolicy: IfNotPresent
          name: proxmox-csi-plugin-node
          resources: {}
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - CHOWN
                - DAC_OVERRIDE
              drop:
                - ALL
            privileged: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
            - mountPath: /var/lib/kubelet
              mountPropagation: Bidirectional
              name: kubelet
            - mountPath: /dev
              name: dev
            - mountPath: /sys
              name: sys
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.proxmox.sinextra.dev/csi.sock
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.13.0
          imagePullPolicy: IfNotPresent
          name: csi-node-driver-registrar
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
            - mountPath: /registration
              name: registration
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
          image: registry.k8s.io/sig-storage/livenessprobe:v2.14.0
          imagePullPolicy: IfNotPresent
          name: liveness-probe
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
      enableServiceLinks: false
      priorityClassName: system-node-critical
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      serviceAccountName: proxmox-csi-plugin-node
      tolerations:
        - effect: NoSchedule
          key: node.kubernetes.io/unschedulable
          operator: Exists
        - effect: NoSchedule
          key: node.kubernetes.io/disk-pressure
          operator: Exists
      volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/csi.proxmox.sinextra.dev/
            type: DirectoryOrCreate
          name: socket
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: kubelet
        - hostPath:
            path: /dev
            type: Directory
          name: dev
        - hostPath:
            path: /sys
            type: Directory
          name: sys
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: amdgpu-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: amdgpu-dp-ds
  template:
    metadata:
      labels:
        name: amdgpu-dp-ds
    spec:
      containers:
        - image: rocm/k8s-device-plugin
          name: amdgpu-dp-cntr
          securityContext:
            capabilities:
              drop:
                - ALL
            privileged: true
          volumeMounts:
            - mountPath: /var/lib/kubelet/device-plugins
              name: dp
            - mountPath: /sys
              name: sys
      nodeSelector:
        kubernetes.io/arch: amd64
      priorityClassName: system-node-critical
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
      volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
          name: dp
        - hostPath:
            path: /sys
          name: sys
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-agent
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  template:
    metadata:
      annotations:
        cilium.io/cilium-configmap-checksum: f44a1b9b9aa3687038dfd8962782ff63b922b65c6e04f629a8dbb091db5e9f29
        container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
        container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      labels:
        app.kubernetes.io/name: cilium-agent
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
          command:
            - cilium-agent
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_CLUSTERMESH_CONFIG
              value: /var/lib/cilium/clustermesh/
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
          image: quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                  - bash
                  - -c
                  - |
                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    # When running in AWS ENI mode, it's likely that 'aws-node' has
                    # had a chance to install SNAT iptables rules. These can result
                    # in dropped traffic, so we should attempt to remove them.
                    # We do it using a 'postStart' hook since this may need to run
                    # for nodes which might have already been init'ed but may still
                    # have dangling rules. This is safe because there are no
                    # dependencies on anything that is part of the startup script
                    # itself, and can be safely run multiple times per node (e.g. in
                    # case of a restart).
                    if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
                    then
                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
                    fi
                    echo 'Done!'
            preStop:
              exec:
                command:
                  - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - CHOWN
                - KILL
                - NET_ADMIN
                - NET_RAW
                - IPC_LOCK
                - SYS_ADMIN
                - SYS_RESOURCE
                - DAC_OVERRIDE
                - FOWNER
                - SETGID
                - SETUID
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /host/proc/sys/net
              name: host-proc-sys-net
            - mountPath: /host/proc/sys/kernel
              name: host-proc-sys-kernel
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
            - mountPath: /var/run/cilium/netns
              mountPropagation: HostToContainer
              name: cilium-netns
            - mountPath: /host/etc/cni/net.d
              name: etc-cni-netd
            - mountPath: /var/lib/cilium/clustermesh
              name: clustermesh-secrets
              readOnly: true
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /var/lib/cilium/tls/hubble
              name: hubble-tls
              readOnly: true
            - mountPath: /tmp
              name: tmp
      hostNetwork: true
      initContainers:
        - command:
            - cilium-dbg
            - build-config
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873
          imagePullPolicy: IfNotPresent
          name: config
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp
              name: tmp
        - command:
            - sh
            - -ec
            - |
              cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
              nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
              rm /hostbin/cilium-sysctlfix
          env:
            - name: BIN_PATH
              value: /opt/cni/bin
          image: quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873
          imagePullPolicy: IfNotPresent
          name: apply-sysctl-overwrites
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - SYS_CHROOT
                - SYS_PTRACE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /hostproc
              name: hostproc
            - mountPath: /hostbin
              name: cni-path
        - args:
            - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
          command:
            - /bin/bash
            - -c
            - --
          image: quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873
          imagePullPolicy: IfNotPresent
          name: mount-bpf-fs
          securityContext:
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              mountPropagation: Bidirectional
              name: bpf-maps
        - command:
            - /init-container.sh
          env:
            - name: CILIUM_ALL_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-state
                  name: cilium-config
                  optional: true
            - name: CILIUM_BPF_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-bpf-state
                  name: cilium-config
                  optional: true
            - name: WRITE_CNI_CONF_WHEN_READY
              valueFrom:
                configMapKeyRef:
                  key: write-cni-conf-when-ready
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_RESOURCE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              mountPropagation: HostToContainer
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
        - command:
            - /install-plugin.sh
          image: quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873
          imagePullPolicy: IfNotPresent
          name: install-cni-binaries
          resources:
            requests:
              cpu: 100m
              memory: 10Mi
          securityContext:
            capabilities:
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-path
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccountName: cilium
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - emptyDir: {}
          name: tmp
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /var/run/netns
            type: DirectoryOrCreate
          name: cilium-netns
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /sys/fs/cgroup
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - name: clustermesh-secrets
          projected:
            defaultMode: 256
            sources:
              - secret:
                  name: cilium-clustermesh
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: common-etcd-client.key
                    - key: tls.crt
                      path: common-etcd-client.crt
                    - key: ca.crt
                      path: common-etcd-client-ca.crt
                  name: clustermesh-apiserver-remote-cert
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: local-etcd-client.key
                    - key: tls.crt
                      path: local-etcd-client.crt
                    - key: ca.crt
                      path: local-etcd-client-ca.crt
                  name: clustermesh-apiserver-local-cert
                  optional: true
        - hostPath:
            path: /proc/sys/net
            type: Directory
          name: host-proc-sys-net
        - hostPath:
            path: /proc/sys/kernel
            type: Directory
          name: host-proc-sys-kernel
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: server.crt
                    - key: tls.key
                      path: server.key
                    - key: ca.crt
                      path: client-ca.crt
                  name: hubble-server-certs
                  optional: true
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium-envoy
    name: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium-envoy
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/cilium-envoy: unconfined
      labels:
        app.kubernetes.io/name: cilium-envoy
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium-envoy
        name: cilium-envoy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: cilium.io/no-schedule
                    operator: NotIn
                    values:
                      - "true"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium-envoy
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --
            - -c /var/run/cilium/envoy/bootstrap-config.json
            - --base-id 0
            - --log-level info
          command:
            - /usr/bin/cilium-envoy-starter
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium-envoy:v1.32.5-1744305768-f9ddca7dcd91f7ca25a505560e655c47d3dec2cf@sha256:a01cadf7974409b5c5c92ace3d6afa298408468ca24cab1cb413c04f89d3d1f9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-envoy
          ports:
            - containerPort: 9964
              hostPort: 9964
              name: envoy-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /var/run/cilium/envoy/artifacts
              name: envoy-artifacts
              readOnly: true
            - mountPath: /var/run/cilium/envoy/
              name: envoy-config
              readOnly: true
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccountName: cilium-envoy
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - hostPath:
            path: /var/run/cilium/envoy/artifacts
            type: DirectoryOrCreate
          name: envoy-artifacts
        - configMap:
            defaultMode: 256
            items:
              - key: bootstrap-config.json
                path: bootstrap-config.json
            name: cilium-envoy-config
          name: envoy-config
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "1"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-startupapicheck
  namespace: cert-manager
spec:
  backoffLimit: 4
  template:
    metadata:
      labels:
        app: startupapicheck
        app.kubernetes.io/component: startupapicheck
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: startupapicheck
        app.kubernetes.io/version: v1.17.2
        helm.sh/chart: cert-manager-v1.17.2
    spec:
      containers:
        - args:
            - check
            - api
            - --wait=5m
            - -v
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-startupapicheck:v1.17.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-startupapicheck
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-startupapicheck
  ttlSecondsAfterFinished: 60
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: cert-manager
spec:
  encryptedData:
    cloudflare-dns-api-token: AgAJ9ChXMAezXNfPI8Qhwn0OzJcKtw95ReqgVgLGFV8x3T4BeZtUTgEJGPBlUliwFJNGMCsQcnasTXe5lyPOiCRxFZk1t1ZrUY++K6ErzT1F74wN3nqQY/Sa1slTFidyAIvbWagOMIKgjPpnvXfqo+nl5IffReTSrahI3oDC6Z3AAAVyIvXhUquYeAuGwXds+CJOUWhZCogPxYQnJf5BZCBMLzg6HGtvOgAaOpNVEP7SG0N8yhIJRGv6k1Dc2mGqGMmOQf3Ok201iPq6NkZE+uC3aO9iQGilK54WKkY/pSaUna7YPZ3FjqHKLx7BgC1w2aWFOTVXHfGRUGwnP7mTOzkSpSVSSJahqGc2O2/Bw0R6QdIZm6NSLqKRGZ6tt9rKc5zxjYSk4AMvDE1v2KZ+2VuhPk+2090sHjYGi2cq7H2u0C4OJkIZU2qhxp8L75mw/dUhX2ZEp8CtnN/PQZATY4EOJB3S9jk3NbOV7tN/uHjhYNpFVTQOx0ZLkPpYtLA8GZibC+K+wqpdiPrrUKGsARoX2t7yEHhBjz/fWkyiV206rkCz9m55eTBAtJH2p7YKt2pdVKWHedJ8Lc5yXIpTpKFn9fbgEbrvgOTzia9V7iidizD1QDcLzLYM9Jf9uwj9+vldkGB6hn5Nx8sgdtR1p8JmbR1A24yIfJmSfAeDSsLIcutjNRkDKvK8TwwxbgZnzSNLWtAioxnDO24rPP58qLgVz8lNz6EVXVd5i5dU5I7BGFuyJlB9/9xb
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: cert-manager
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: coturn
spec:
  encryptedData:
    password: AgBqb41JE43pliaqXSNHlr73oVa2YULlDYLFmhnHWsFdAMGCAOrX/3KY1PJK1nSKILblkVlo4y4zYXk1sS1J/2+4uTUStd9zeqaUbaN145joLDKqIJy9UxUMh2G19idlJmhgZ4IXWYpUQIDKgXExZUmbqOmVrecPbGmTnSAAsR2HwW4N4RUW9CzH2ecbTUXuJ5jVzv1p3HhDRyN+A0nkm9TKZOQVTJm6QWNcLF6jk0mecTZZ7m5nVrCWlN2fToBWIaFEnLwB8jZMsJO0SSbHP6CLLtl+tefHVYZC+NkOP0/98kYzE8T2UoxOPlkKZW4m9hAGII3XzzkP6cy/9QA48p7sQne44IZ2QYLMHN9vJou3cn+uDQMY4SA+nYUcBTRzbWX2j2Gd8fzDFYMY5d7j4/XIEmKFI3HKfaMfvyPxKi6zSdFwrk0zGNvU+Km1hwix6rEK4n3UqxwizzdOM2rA5QFjXnH/JLidLCcxwZKWUyh1haiXZTWxqCoKj7W9xxQohNWwG7bV2Szobw4GP3utkmet5wKGe5ZN2buasre1cyMTa/VHr4FtPT23KGbIl5iaVtXO7VxisRwpCWTBJNA1EdbTibPtYVCWqXhM5vj0XOcM6qU3IlBgSGFXWL9JCLRFRiqCvR4OPGxT9VXxOI2AN04JSoYNEGFpOuV8HglANFrBwb54N0FeHgpZ1PNIXlV3Wp6x8IEyl3U7SXnB96O2jXpe
    username: AgCNW2mqiJHi1sqEj60miMBjc6b8XhzN4CVguWfch3mQQzOAY+zd2Mh57dHqmql7bht+oEOtBenxxfxKQQllY6HkyKXK7SWPPPSL0X20oheQEslNpGEYpkpaFBjyolO1Prrx2NM94oZxcf6rPeccXo0vApZ1E3YV1iIIWsdBDpoOZEA2avLb5jc6yIt8S9sbTGlzCf0F5UeVTM2WifnbRpr7WDU1GRgDID1v2dS8jQOtdu82YYD4ux/+I9fL4It+nyBREvfyUeV9TZphxG26xtQgPcXsMs6R+EwjOPbSxivep3ClQ+QbeLvMDl6LUdOeoh9Bx+MzJG+GWKgsLcpOCfRVh+nS8od8XKQRhGrPwOdyVbzGbStVhoIK3OomFyQt7qokaRMXkkVbmsb4vOx25Y2GYcdThB6ERadpzsVA2dcyoa1yvrYDn9AREBzct6gT8sprT89JTUNC7YdbaS0FTedvzHEvF5INpdj7yVoMgp5Zz1Rh5PEYYelpHcLg6Cm49Og4XkjdDVtrxvXr73HKonftioktFi/uIRg168yo+QRedTnqNa0vlY27tnrwuDFp/nxfN5X4QBLBmwhEAEJG4DfpZUiYRrCd4sStOWP61MdoROE7TaFzs9TU70mLEP4fcJL2zXC5LWeU3C+alvsp6c55lov8NtaBFox2z76ug6zTipTutWuenqLZNvpaDtEq4B/Y8oV+Yjqc
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: coturn
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-admin-user
  namespace: gitea
spec:
  encryptedData:
    password: AgCGBjrd5MQeC0yx5epmouyLnJ6+5cZkaCiyXlyS4r+KSgvh1bSrtL7yyTI3LXh5ix7scx1j9cpzyRaGHVexM9LNM9XWqC7y7GRNJb3opCoDnSGcZ8PwBuLMTg4sxwM+fIQDq5oci4YyR+Jz8RUnslRCSBVr8qIE6xz1tMlpl16o0ni+vsb9q1AjuO9TTIhnctU/IJ54P4NQI7UYHBV29S7dLiY6R5JwYm992EIqFls6GPEiDGKKzHbphZnznTmlPENmM3fnC2UQzvoUlJ/sP1rbN9HM1gT08GCVnj/3M/W3fUb7dDAE7KAFnFAZSNhhQW2+8+5ipwndQy+CkrV1d3s+TE4rFvsuce87Dm9UTRXMB4brYQb480adZWIJJLxKR5fXy/cWeWOw+mO78FiY3szxgu8pSLUWndCBfRAFoDpuVSoQJJfNyL/9KTJI+IcubLHl+xboF4HpWnfKNRLix2JdYJCs4AmsJNAOcYBGa/fdnOdg6YQ7YhDRuQqlNflgA6UM4lta1mUMFBPdP55qTsdCnpYba0Z1uAWLS4EERVNTgPl0aGLlmmvYqsi8vWpu8JpoNiMFKtk8if0+ldSAQ0Z8ozgSGK7jwz0XWNmbhQLeyf1297qIaenWiNo8GACZbvPHvVOZ9At+xq4jLpMoQA4obMv0WBXpiohaHYq7lLTkFMyN1xFa8ABuguvDR8NjisuuB9tgxTY0zMDAdeQ1qkLKcioao5nxeA4Q
    username: AgA4Du7ge2bBzpz+hHKV3E74rVdFAVoGNs7Qts6Jmr0r3E4dNobiqyroocqsfHK65fOWCfnzBdYN1vKeVml66XG7GalcF21JvnYll9KC0qDRGO5qUW67rtZjG6F6BBV5CJTP4+1+VpNOcQxXdBNnj3WV6M78MhL1oYX1IeB/1tRJjDRgKcRJUc5vArSQAUHoykXq9zNwsib0G+J4EL7MaTfNEZgjpF5pgdb63f2TXiELEufCNzYisF5KyYsEa96NexnCKRowBWQ6TtiQunq4RoYZnxNBNvL03mYhEDpGH5DU/SNWdDRKQf9byaaKaGSgxKKks7a+vUBwYJDoWUd01tQOVN/uaEQwSJ/BazLoQjo244wxhu6yDWuZLfwc2VqlFsG53urT2uPwTv7sNhW5L2PUIWKgOaRhMAlaA9MWgwTfQYvvAOBmlV2GsgwY2IxRGiO0uSNeeAWS5cIhXuc+aZ8vNwj5CZR98WTtZ0E6mmyDmUpXZcakDE8W5enuYR6iejN555jlxK3U4/HvMKe9ywNXzG9LxqzLF40Tm55hH0nJHAFsin9aElfAmpeRY9UcmwIoNWfIHTM2RwFCm//S5GxYLt9uNwYPlfRg/DsREYy2qPNeXth05TbM1IND5ho0Qb6WLB3f0MM39N9YdPsv+nCDPr+Xs8uF/E0tHgowj0LdDgfaPlX+eLOQReiChorjhs3lybBN
  template:
    metadata:
      name: gitea-admin-user
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-oidc-credentials
  namespace: gitea
spec:
  encryptedData:
    key: AgA+nsX7ujf+uzZ2N653HXfGosxZT7zuvHTW/8aZAZ6Ll/wtswLCHwx6G3Sr2u6m8L6AaxhSoUE/RO9NEhQs+xCcTc9J5ODI1lsvd0Qiupw7+EZMn+4EBsT/LjypXfpDvhM9DTbyX7hbJxR04ncJW9q518CHqoT5j4pkm5PLglbmocb85j3QUpBNOaNFAe1SnhwrIoleX1q9BjdeOGP5cj8e4zbaMHTUft3NQyJyi0ghZ2CASEngTQWIVNHbElm04F6xOhnL69YY1Gh+2pO8BJLKRvCE2JA7FgBcRBwvTkGt6q11tbtEaKEoFiMqdEQKdb9AnZa8XErvBljIaZmtIRTsmoU6DZTgBXaFDaEcOKvM/NuSdLUz2j1gKmWZSJ6gMnI9blUxdaV8KHozygbBF0zj2d3eqXpmtI/atmXfbvRduuLLo1vhmaQ++P0vcHZyFlAIQTLJdCGwIt5hdK0SL6uChtYDqhYV+KebuFBmI1r1PKDaurqz+KViSIfK+kt152Mf/LVBq63foszybrDIpNyk2nqlYHJnCAeZ9HtvuZ0Zz5CtAl3KIeWFg1HlynTpNn1GJO/A8wdZRoAOr1JbX5REaoU18HS9n3X6Rrujm5SwydU26AbFT1rrJAVYAjUoufNV3e1HTulcwDjUdAAowvMwciKvpdhh48H2XlFOW3+MZOWBSo8ErVOa8/t9sSowzdqJd3Q+Mw==
    secret: AgB4vQvD4KBUOIkPCBL5+Fgb+jwbt2jvNcucCP0UFZrX3jN40bQxOqcElG1Z/6LZhoq2dh+amkpGIh3R/YKJLNeUsE1TIL9W3ThriO+Ng9AH/K6omywljIldSHsPRARjkAiuUKMlx1mEs7TRw1/YACmXKNw77KL5RO4NLtU7VgxyKcLe3JXIAexbNK6W3X8TkDqu9wx3eNduUet4Jt8jEbaMfINwexE0cH7NRVyQbLDT7i7MxJD1PisQWh6XH1ZGkLkxSxh+wYSHwIHSoMgQT3r4vxJ1rmvDCSEPMEBvB/3tLYYTGWAImffRIdnYHxWD/PDAaSqHqSMGeXtFnduv5SOYK4eSayz1p73IwcTvQc8ePUxgwS8uahN7UYRaoWem1Vs/uw63XBetm5PQX+wh/yvnK1NOdFAFIRn7Oh/iQhoy6kDIdzEndR9WZS0HW8IrMj8oFPTgJh1lRtWyXuhLiYiwsLcmMg6U2sPSJVRcnvxqdG+DqKjWL9WQjaP0EWZe3luole2CMfTkIBcPPJxMd9HJCgCcEdHl9Z5WZ3NPrOGb0DETOnTnn9hJPPlts3QJfqvyygNFqIUkSWE2zrDA4GWF2sMBE8bh8yQYGBD0COwxjvyWvfO0WlZ/63sjTwqeiV229MUGVNquD7BKvShGT71b8UFGfZG8rx5h1WiVnexCJ4Rth/u2eRaxptyD6NO0A43RAMoMJiDatL7ml/LMxfLyhDJvG368rYM=
  template:
    metadata:
      name: gitea-oidc-credentials
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  encryptedData:
    password: AgCG5I1alY74KUfK2r0gwZxaTrLD2py2Q7vBPPW+4LQg8KA+lTjYUEhN5BZLHzFRUtHW+KQVqqgxRlKOAsRZn55tgzzqxgmNxlipIuX1fGyb5Ds45azqT45nWfJlzf50r0WByMSjsdvVshkNjFjCXlPfwr2r73x+ZWqRv75Ra+6YsPHGWvkzglMQ8G04T+s2crb2PiqtOeSGsflJazLG5VAf8tddChGKsptPwate0UogTctFGHwhjMsTp/J2ivTqOh+VqPSknNuyJQfzYWHTrEHhNR4dNROy1lxFjA2/bbpUoezx4OHEmMB2iKmhVFycLPcxKG0IOykINLO11chia/sStL/3FQSJw0v99YqmgOl4g6HmkCiUSMQyULm2hdAsfbkgt6mGbSTqFkXS+uKClW5WKU0tzowXWQXZ4tCSC4ODZGDRuWbAsipClQLSknd9plh5FEzsrXu5Gujg+ALFDFef6lv6ukd6e9tOlAB68mMklXUVuas3a7JKSeZgYf4+JucmDyn1pDNpyT7KnOzP2GrQ77vTkfgay3QOZ4zh1CMw+5oxrdRbML0JcDvOTBraOKAu+nVwZCWK5nLmS81qj4fWRvqdiklw457cu9m9DZYJIgcp2OGKddyvyH/O8p5JqkmsRowd6LcmUt9F+/9ibnO3z/L9X1+pQkIfDnyBWdoGgIO1MSQzzsoqDdd+MDOxdZZTV9yYkQ==
    postgres-password: AgCfEmOIW7DTsRFmEUsqOAd1+ASfVPxQfZyIcrq1q9LVXX/FWY6dBTmLBpVRPLF1h3acLoUqqIPLGOK0wPgCbr9waEvM9HKsfhFqzTGvPIVid2r2tuga4u0pi0U6JqidcHROpyQCwE2NJrFiY2r3aGo5wstybYzHGDmTIAU8FfCTf38GzqgdneR3Gike9L9IoUmvEXzGu9edcQYinb5+yJ/wWp2+XA9936HmY8vTsjKlv2VWBiu5ZbWO3YRNYOvKesONDgvGh/3I9yagEqajjlpSEIAffDsXRnf16ihqGaxWk6KUD89yOtuiLo5BR9Z2KQmJ2jHB5T/666dZLMUGRgZBxf8G5wR64hSOaAoXI4HZ6xYYsmamGRrVExphzUNrkNu2wHmUwTW3LT8pXG1IyUAd0MZS8ZUlS3RdB6cyczYvhCuln2ER1iLXSPLjKy8HVhvDBwp8deGdvtgXK6USziDbYAaKvosYe5jZynLuEddCFJ55i+Kfk6hPotnnlltlIA5XA9ncK20L6r+tj6Y6CLYR+eShMAi4TS3d3vmmwcWdaqUevh6v+klpCCSJ8JyIP+YTBBl+0fWVNqQDqtAXHzFeU3IRl2MYCZ1pbaLQQgsORabT3tbYbZ1Ab0re56BnIDykw8zwylr2sOOq68laEYqmiXBKEc/9OZZNUFZx2BHE8cx4ewiTkXiCeH917XZRnC60Nlo2zn85v72N
  template:
    metadata:
      name: gitea-postgresql
      namespace: gitea
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-config
  namespace: immich
spec:
  encryptedData:
    immich-config.yaml: AgAA1uORMAGoH8uX44bzeylc/bxC39WkaKTzVBAsHzEem2iW+FzmGv5amYEBlgWnX8jjSGK4SS22g/n8DcL7GFsrgnrZNlJPUXzJQUD9w2DjjpVaIFrQHvjrnLgFLu9+od78RWXAWds85oiRX6LYCiqVc54Nyc5zbnQ8DHlvw+8VwfpitqWZA38cclJ+sDbB9hCLBPSEDsBlC37p8bQqjPoP2m68QhEpg4Kxrv40k7WxCvrNdNUCAiOh9hRdHvbCqEtleL0dk3aWJ3kGSf6p0rsVPl+cyxlfsFrFg74lN6gJKfgTV4j3mzv296wb1btDVN36JaOTpcdS4aVK10Kg53idBG4vX73eQZFRaPw7ZGi6u2A/c9I13etOMxrCX0y2Vxu2ZnAtnFdkAdlcsjhiX2LdjVYGzmT+ZqOlGp7lJd+gKX9CXhfrJlw3/2lQtRpqqWihIrrdkthm/4mAybayREV+Q/ojEXMmk20XkI6OLsP/ljZjMNA7vCB3tyLnbRfqHacpKbnVp/b1Jth6TOYZnvLhne5i0JmM6RCUiZXitJnxDzlQskFAUKoyBHorS8vYD8lYHqMv4iO2E4Qn38vHnSAaeQOH13Fe5tMJ+U/VPQ4ybawWCapUil9iuGTKwMQ4xbkpyN8z1KyP3y0RZf2w9NXoFeBZR/iSgGFUataEbpXWipGb66ieV0CAqqYGvLHFfk2kTbN9HMRKTcsClCaLOCFQKiMQLXNtxLsIsxPxrb1l1kI1xnW1H87upx1Y3SEzIHZdQgIeVmkVUFaVrFw5/WjqrdVZsxBxGkqvD6lKFY3X1srzcxh1A7LasSuoOJNVxVTFBr4LsQ7tNnT/rHaMZ4YSLwRJUQc6hnNK+4B4GaTX0190B3rion3vM3XsEqU8U3hpVNZupFnJe1Lwsp3Dom9nQga30WJjcD5SDGZQUtuPEjL5//A7uTMGrx4/B3ub2VAc+8LulG7dIAGSO7Ez0drVYdqcGoNQ+EPmY0WxO/CbYoD06QWbqQDgBD/f1ftERTwBGW0IQe6MEWqeTcgFdg==
  template:
    metadata:
      name: immich-config
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-oidc-credentials
  namespace: immich
spec:
  encryptedData:
    clientId: AgAvvCAj4sOLQgLcYpkSUKYld6dWCwEsTxtJ0BRjSOdKU29lh7l7XMkLcqiQJKV36PL/he0YKD1MW/jWcD5oys7ZHtaoDKAfCmTMR8eWGU3UcY+0w0ELn+G6TL0EkgKY6Gmc0JOyguwJmsilD8iyW021NknF+hj/pIak5iLxHOidPbRmNCWqS0b+iM0WLfLs+jrstLOYfs/VA/dftqZMogN5JLQ59edb9DU7Z5DRpZbaqmE7kgLuRR3t8Iiz300S2Tvhx8E2/2gKLQbyVMmZ60KjdtNln6HDhIjwxHsas3iWUXlgh1V2sIh9hDOBS4yWPoGDRYEAUdBJfGLFUYi3K2cQOC2oAAYaU0WTTDRGzQMugPDbKONO+fDVjKT0UJpiBwJ5aoNW244f7qUNDbeGJBM4iyQYit24fsHyL+4jbXjaoN/mjfZuN57h+QV6K5sH+bJOSl0OyH8gT+UNYsIZ/Er1w+AsNMdxB6Laf7rPnxlVsyy6bDLp8mRE8lPjz8f9EPGZIHTkcLTgNdsbDowG2RfvEft+UkRVuk0g8++VxQs9T9jzUgU1CsIESW0mDwl/K64rJm4y6G6wAoqCB4RyS0XHQ0+e0dqpponRKpokqzxZPgfnlay1Hc+L2DnAlD3kYsvAnQdECQMaQ93xh2/18Ym5BSQCBDB5RnElbgJBj0uE3eOJ4CMMftts3b+rFtVi+SOvpkXciAg=
    clientSecret: AgBj7B514brIE0r4g9UmqIo8gnBAHA1G/e/reQH3ATOUz2gAMGmC6VVXhfLam4Nwq3U3s+SwcU+jvVYAu8mmf/2ltUIxvBIfFnk5olM8QlE7eTVQAOdKLbhBmIWN5WhTgP4kIXawny9i1h8ft7anYVpI+PJoAQ0X8OiRbybsb+O8wWDFaJQ/lEwFAww3yNtiltCf3b7wJHI8pjvjJrtHeJRb13+T/sisSSh85QdQdbeSXBFGAo3waPHcbtaTVAmMs74ZTpDw56o9rP8hPtybSN67IcBa4lt/p3Z5OsVQgHa8Q1mHdzb/8wLDY+39UbHIZtxhH46MYDs7BL7G6ddTZUViXYBZRFKBdcFSs5tT2jeWR021stpKosRSgrEv3P5d3QDMYIdsK6FZw+lMAzaT7glqxiYLa6Ot0lYLU/fF1iDJ+76J4u1Py+BqHHGQV4qE+q4WTCXnAHlKzLtoV2MiaZN7NaPSfVl751BJFl1lgXbAievsgGPU0rYDn2q+ZS89QYi3yPqhN/3ICTbOE/iMaHhrEKkBBRmz6rllLqrLcTwJo5AWJAUqSyPXY6Jbm3akT/MJJ06Rj3IulZPN2At2ps3tz7+DVSES/MgP6GmpY9ZpIbiqR28rnNjzqRK3gZ0QNvJrScoeAy+uQTFXjRyRuW/8yNPnkYsMpqDe9cmB88lbV1XJzjtdtKKX4e26nWlJZqaT/S7AeoFdDDwLuRSHs2oZYVvFnosLRG8=
  template:
    metadata:
      name: immich-oidc-credentials
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: immich-postgresql
  namespace: immich
spec:
  encryptedData:
    password: AgBg5CBTb0W5tXw8ibklTQP/1D/NEsliDeV/pHL1Pk39RFkUg3OB0pQbSe7ZVqcGHxyLaaTDr7o8NpDsAZnsJOH5vzpGBrddtkPJNwrSFxd2GAzGTI2SQH/hoQQfp27UpJzEqXiLIHw1rFTOTrJJayeWgwDHf26zc0oIBpbbQlu0o+26Vca7sArnGixX4iU5SS0HF4S2Hhk0CkL/dW18k/xrey+GlFdAtYcww2i5Cd6lxRTC5CIX3dQSqBWWcRy3Q4wOWH3zrQpkt96bSIlW9YrDD57dX7NAwCwrCx8pI6MdNAJEZpdjF7T6WAX+t5L1YzF7ecbtGvw4l3IavFKnvJZn1CCS95cI8xFrTw+igozYwMJB+oxre7E+q4vk20LTYuv+hSCeTIzX8t/5u0XqHW3/6XrqMNVOt8ZA0LIpABqqIgR/vEDsQPWwuehPwNvOhTJNKSBQRQi/SGtHUTkYCQwfLSPRpUV/MK++MaQin1nVy/Ch9dexVuWsR/cdffTwagzs4MPO1DDgfYNZe8sY9hPp3XUDjx03ayCT1rUA8Ymqajl2rRseCMuzdQnM33pizqOKXJys2DHWthr2xi0DCYd96dky/L92eHOC/tSQlyiYYjZGOUIMmHMdJpxkHUIXHJ/F1iTIyY4UMJMP3GkavggEk8WIImOebijYSuK1U1QJ/dlPuE4Q30YbhA7Ns5vj4Mu3IsAmAfg=
    postgres-password: AgAJAAmqRjNDaheSbzBtB39VjcDeSGEFyFnSmw7a3BlBBVqSuh2PXs01AVo483Qhy5VJQCHt/pkbnyw3bNn25OPmYZVdeoiwl6gZFwquY7fxpNANhvado6xeC0PFNhHAqX0M/U9UL5xPIfMRNc6JztvQaFosJD56EPiC3Mx76LrKc7hIDN8JxXDoba8NaUfQmvPeqX0bXucbDgDFv5yJmHWv4xtuTO6a/xwnt5uYn0nmC0ut07ntFeQSA4RqT2ptauwMy5SwM9JaR6fL8RUVQKX24Imk8s70XeVXE2mwdXu3w0fVx3PKlHcL2OwLumUsxhgW68ypeoHASR+B59mnJgMajYEQY+OwXwaaQNe1VAZyvfgwL4KUv7DvnwPgcUJNFzOowqmtfYzoydwKgtRboWlRvTR2sfWjtxgOXPxFLd/fKMtv1+CVbrRCivOGsBk7bG5k2QTraTuGCq3MlwMRm0vdhFVnOqZ79/Y9H6GZiFwxN+7W8w9wRKqRb7FmfLGgUbsf5UVqUpAqbHy82Uq3SvUwQ+f+rxBFN8RJVP2zDFiM8WBsywMU4Rvcl9SFcnsLTunaHalPMJw+AiflsT5X6hvrTo1S8xYujq2U2TltFTVK5SKRBrkmNVm+MWef1TmClN/2tFTqm0qy3qNrPZXdNzUVBpIyo7gB0EPuOJ6YXKKWI42LLavb12DdmDEsrK5VRTWcK0/RiMCH5e8G
  template:
    metadata:
      creationTimestamp: null
      name: immich-postgresql
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: crossplane-keycloak-credentials
  namespace: keycloak
spec:
  encryptedData:
    credentials: AgBTjhI8iY1NIToAtBAbRe9qVikwtCVKzH52DYazQICVjjHOBOmG+XHQ0XSnopit6le+ayC9ls2pp3zRMXacMyd6a9x5qmFcO9N0hTxPa9F9O2tuwAjUBaWWegM+WgMu2DXa/9nYdKi0Kz1HSDy0gnB9Wtlmd4Xjbr/tYpDRVTRKizCgs1LLNr7/o6G/ZV0zrJH0Hxt85cZNPjSIoODu4v69ea/Xq8ioqSC6GvfThFQdwx1CZi/TS1OAHarpSWl+i3RfPEls46iLNhHi3t8tD1YumSCENqpFtQ3B1jMc5kAE4kGtsvA9RdrGp8oDLIurmWOzL7HeYSJILrB6cVo7L+Y9wnI+ovCeyqbHeGIjBnYUUYTW7kvubbhBzzdhwDDVsUk5/3TYV8eQoccvYWPfmOhzIGEmBH7jOdD7TfCvbx1TMv2cuMjTGoE2A0WBE+hHY/OgtlWN+Mgvld83KvMLW8cNp2Q4ZhY+syo5okgkAUUqUbsw17bGpBnGObsipdB2j/PqqGTHlNcqskK46TVsu8MPV2MwWcZ/UfDbiEyA9GVYFB8aZXtoNwhXsRLNVS874K3HbtiMoNNWYaZ3ddairQl8uomhEm/H/33IiRJwElFgKfQvQOr5ak6yoBC75NoFetz4fZLr0XgWgB2RvIckb3+WxjrCmc5IdrlrL221otu70T1bZcvZN1vaDA9V2AseY7/WZJuGDqHpfeARkdXzDJd99LPW1Ww6eGBru1BQQWi0obilE+dVEueJ9neebkmZX07OTU1dOQRVvHFQb7k0U39s646mseV5g62d5J+2dnxE9bwoYdy9XY2Hu/oCqPW3ezgyENy0rjwhJo3ua36P2BSkDGI694N7EEb2GjuZSZKlDCWaAmfEGhj4Fw9z
  template:
    metadata:
      labels:
        type: provider-credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-admin-password
  namespace: keycloak
spec:
  encryptedData:
    pazzword: AgBgtRvU9pbVKgL7wZQnogcTXA+I8XO3BqAzialWSeeX3YpYCBbhNELkTe4BcugoYfL+keC/0D2VqiECisYL4arvUkOmHXKBVUsl97rzSKcS8zSomy4OMx3lQWI6xBuB4vwLzDN6qcl+LfnoIJlzcycpgQhdWPPcEJ3utzp/dhO9LzocCW2xHREgaySYSkuaTBdq/Tj2HZKuaJ7hVNgbO00zjHZd7qAs0OVFoZpJlZyMYdZU0S1r0gL+XaP3v2ARZ3XQEDmmxufLOPWcxYOeJ712q8wFZn5sCJdns31sFCFYI5mN/d6iBvcEfuWLnCz1ET0DC6HgedLID1jB3sCjYnQdz92V2fkDyj6uzqb7X9CTKwn6JvY8Egd40KVbamsLuoiT2aNpFw23gkK3Ajus3H5S9aL3k8zCSgBrHBKkO2TZDER6Fzw39cLBSnRBWzPS7wy10JIfRNPKV9rmGlHWY6y+H4KWkPiIlUOqDGjlfaLPXyD9o0DcTs3rTPF/lScqPtqSlKJCEDwWqudC8UKLn5YSPfECdOnqNCDyj586h+FZk0IxU4MSG7HbLHi+B3/Aev3qibBeMIYSsUYGn+B61YY0INm+AEV+M2Co98WU46S7+zIHpxSK6LxV+ov6MIdyo1tGC4PnFd1bIZ2ZxjlUnwMfE8tFSyWO+963ECVcPx0rXbvEDnzKvPsW4f25ttk0zzjX2WLxOfs6fcOKnoeGnG+U
  template:
    metadata:
      name: keycloak-admin-password
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-db-credentials
  namespace: keycloak
spec:
  encryptedData:
    password: AgA+ujL86YIs5ecAexMZ/CgsEBN5VMSYoXV2C0+OKEbmlaNwqBDzlTbFiCMBLe+RIaLTbJ5Lj14HFrGWqx4SdKpIqwJwxtzYJqnUzsQmgHQrAg17OJHgZbMdtI01F13/LrPjaLi0dmygPzO6xw1/bw8RHd1GiAeVorUmQh0Z7zfw33x3YaSb/KC4lCyn+INYOrPG+Gl7WtwKe0g7lLQbkwIl89A37SGHABxErlmuecev6j8CDNQ/WGugHrz/WJK6Z4bNIItMSi0YUeSL60LniEF9Vdx8FrEZ/VMMvyzHz9Sy8X1ygcTKnQi2woKi8QIXi0t1yRAj9HNHmtojh6iDvFtQVldX+OGmCb52iwaS1gybmzPXEs+++vRG1btosXh2t2PGs0SeCvZul+ZVufOfB/i9pRuu6AvVOAqxGz2Z1bRinZfkbrYlweUv5GwZKFTb+xRBx5Umwv1jJ/zngCEEZ+xdAk5iC6IVhqfclW8qcESagEPgIK0ZPwD7KD2iLi9dy3Ob0nd8+jvlnIr1BmwoPc5pf+mbt50OpGAdMlBGfPaYRFFM95pFs3UEX/E0gM/RRWDzgz8c60fyvnV9ACeRLnDqQhMUp3G8z7P6ZndnvVRw9R1n0mXR7yUabUtju63/tHQpY9muJ0m7occD93W+18mUVDPoiBC/sQzppYtqBNo/CJ96L2D0wky/raUNKQQVaiYw/izlXIquW6kn
    postgres-password: AgChWk610AN5uWVgF6/DlIZZ/GJSFfUZi8s0VPluyo6/iT8yKahc1wWW2hnQ2CIfHAlziq/ksLWElPAgpFYSaQAcOLi/uOEi8K4tR994MAMYMX8zh46ZOm4e7da8E8kS7A96aez2cs5J19VPoGbEwteCqB+sZFby+4qLeRPTL6Fk0q8WppnhZDstYwvMYL2gpTeIDqQlwyg360QTHoFa7BAntdpJcO3eA11tTubhV16ImwV348LYo4x1o5fesgazepAope2KhujKuPjBjqEDfMObJUvTTQuXQswmVLG7N4eeBilo/CEAWt7JS4wDODQM1RxLjiySDqJJBjwKxISORX19Np1iPINmTuJddCiS+i4xo6qp25SbExx9AT94TW6bSZrAruJsPPrm30tc90/nGBRnM/kD0CtfOayudmrkli3NMm4fAJ+qWaNTaU4WAiy0jimO5BrPzK+OlCVR7VRh2r6nr83X4BHAYjY7cAiCNF9ptl4G3EKvYnMawM5I3pGCbyr+0XbihEXurphFso5ElvmhzDonv/rzLF+ta/frpYdN2o2I7UUamh+is6u3iYFl/Zh+k8eZuJxZkdY/h2pha5ZL9tjzniGFQxYbWqMZN5suZDBo/YtItHlhx8Jr1o3m7lwpW299P11DFoo8rPHPUfOxTet9iqCDHTTBGx8ymLqG1fnT91WbJP+JIFHY8O9+HnrY2+Wg5aGpDkg9
    username: AgAPDXqoidAyFnMjYy6AHu9w/IS/IyQF7EjGtb3kFaq86nTLvtdSPBgS2V1cEi/Ko8MI8e0B0TA6LfXfvFm85yTDzazGVCRWVDLWBdrt5FEKqg/gUbBZXvC/AcBCxMZxE+cXXLuICz5ogW0Z+nDhgo343IUTG9QgWKXVPVpp2IPqI7ka0IhwbBo9/Q7kd2UwkMDxzNIqgArr/l25fZ59YsjvASI9qFh6rsnSu4idsLGRFJwo5oKJMUVM1K4BaFz5fllBKijTvueLP1FyOMzYjA3DUdu4fr6dmqbZTZu8pzDr4AJdNsYTN54vaZj6+46DHPH29iSYHYHRMGvk4KcJsIOibTxlIxiXS+WzwuaVfPkdAYFwR0vcuBplsFG0yxhNA//bQCtMQIteF3ofGgLtSXdibaXstkuHCCvWR6Mg4/zN/xuDt5qrQW/cPepjKgxqWSObF95BKKp0Tv9fID1K4MlRLI4g+k6j/nMr8hli7ZMtoaQLUa26nnQU9V7dwK/r6GdgCPVtO1gHm5XPL8z4HB5pa7/pxQdieCHLBV9+TFSaEqxhDReje2ZyeLFehQEYdBqEarxqe8L/CNZIQPPzkDNn6X2qmpPiQFiraBOkYB1Q2yr0j5nj6Xuv1ObGlMCvo4fDVW4aWcOquu4IDL2J7nZkON6qbDnMHVoq7/cif8Pf448lA5iqzTRMIGeUlq31EigqJ2g7LNNGGHmUCQ==
  template:
    metadata:
      name: keycloak-db-credentials
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-initial-passwords
  namespace: keycloak
spec:
  encryptedData:
    jiyoung: AgBcUBmxePlb1zMfM3AuCjb5l94cq0Y/wo4aKaM8Zf64oGxJfsdV62hYasXdgAhXrzOpOwV+gS1pJxlgGZXBhRecfT8pGJ565BpN6BUW3S3tlBzKKz9KQeywdTWSNwro/H+y/ySGxvmBQcK5MrFitbkaWegyh8XiNZi6zwKshIfYYrS67h19bfjmlKREPbMtO4ML0ABm1aK1PvNIk6ootKpJrap+XcBtnKPJXyF80qrOBcQvrN8z+MXzBcP4hvYB9bnaqGdsR70DLH5H4HfBo6HkWdYYO61ymcz1x/rQ3xFATR9+cZYLEQrgvHGOzGbvLXbbNGmTEQf6oouN8p0y2zIFb7s3UtxfhbSvYbx7ynXuZXGzemB87uSH/+hdLcg2zwxM9BpAJvnyml6n5EkXYINIKiOUPc39Xx59DKy/sdhAxNat9XkUY3T+Hjy1POGrHQ7CYvwVKqF57ffpH7T4ZVmbU3PCdqJg0NSfUO8/K4gILareKM3/a5ljHyBBSnajpJDMc2ZI2OtCGPWyhWLkzB5DuiB7NOEkgKnv8fsoAmNuT85VoHrYUAafIUn06rQ+TB3Zbeyc682UQm4A8AuZbjsnbA5lEBkznhXltceUPkjsM9vC4o9NGsdgw2y0IALRDzCiY9QPoiNYDSihSwmdR2qWR+HmH5tRvd+W6y88wwT1uNSChYq5ZhV1WelDKTdmgqd5d20yw0UJgm67JipXEH0=
    olav: AgA3vrrT4IljgWy68cCPOStbZdOqo8dpAVtXvSYTyhogPOJQgW/tC1LOPEkrrUbEqMbeahYJ4NihjmKROaeVyuY7Ll3cbLtocxbGKT46xsaJmwRqj6D/cEeOUyK2F1pCBdUHsIrElknZoLTX78JqtuxTvcA4vEz0RHHcuTTqZfSeXxNteYrWbUfBaY7ZPW4RX5CU+s5cInJLYAS/N7asj9ZB5v84vy/WHZI/AQmO7IBcn2KV3WC84HAxtFNxXQee9MAQiutyLjXoC1tdTWIxZ/mI64qU4cq3Nq0AARcLmEh+Q+7eGLzC3xDxIJ3s0DzgJ11qpgcd491VG78PcyQXvAHDHUTasQoqMqgvgSsTDO3dPP2lPxaEgRjzCXIwxSLUOmQtqHQ5qEXxTtSnSuvL/JQ3jM6Q1Xcc+kzD+4ail2ROu09/pNJFUZwLMS8y140+vH6FZn3CZFLVrKkSxsL04xWCE+DCblHIg1MCW5bt0QvaCqHm0BhiJwInG113XrqNxIvJMzf596G6Qebl6AyhLOBCeoC6iehN0xc2lO6oAl8S8Q4hbmJsUzOam1s3eNlnqROuqjOqj8BY8GlwfyL1fmVpSUg6GrHwwH9Ie+3ZWy9NvNmSrBiCm9v00CGL/6RBkMkHVmDsiIBWrMyNDEmDOAMMmm0AiSWz9TKiyy63PPpE8Cfo6fu9k5+4mGjd6vR5pRF4xu8lOt5Yycg59hzOb9zS
  template:
    metadata:
      name: keycloak-initial-passwords
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: netbird
spec:
  encryptedData:
    cloudflare-dns-api-token: AgBxmCyBQd1Ih5SaHD1CqdjuUOpVu9LgQ1wQbObXappqOJvdI09gKCmNQ+8ucNmoEbA88Of1ZNFIHM4UnvmdrXGRHIjJfR4E5GRQKzcmOb/FiIJ+97rYpm7fcp/S1Qcx2uA9THvhvO61O2UW4LdR/m2mFETs/tdgSdKkv5nRfxW+lWvPsMR/UFPiAJl0q77jUQksTT9QIrEmQ2L5miMtB2XtqtdBDx5m43xmwInl4XJeB5ZMqikonqb8D36bjgNYYgtmxIQ5arJc6W33u01n0rmkg7N6NQz/3lRXAzhluaitlUmYM1S+VbJUDuFUfmkTL0Wbz1Ab1eYoygshhWeAS/bK5ttkp4idfdHWd0zsRcM0ESs4T8ReQlelnii3ggh6SZXp8njIGucPduEeaBI81ODEsh6iS9t6OA0AIv4ffO0yRwA9LUsnDlmiPDf2NZe7ecPivM8uPZHFwhnci6fDp8kxdDGiEn3qzC6kFK3wOShTivtTu8LWieXNGb6PPf2vLU2L8+84MEwEHn5L2oQ/VNIUAGzrZMYHQTlTKajt6QNPxS6SwxSTWAh+nZzIwI6snSFKGZXFbcKisZl+V39JqOxIaPhZvwtOTBx6aDD5NGT6pVy4FAxfUwNTn9iElf4tBLdm/3oSrKaDyOKh6n2QO+uVK+H/jPt/HeRs3Y+MIDHyuEj+RzC/tOcNGZ3usX7yXO8B5W4rJut91dsQYMO+OXvGgEtinR4eVag9gIkKA+w8GPbXvvs4XF56
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-agent-setup-key
  namespace: netbird
spec:
  encryptedData:
    setupKey: AgANdgUUaLKyvercS9O31avaxlyiD7zfL0ect3vj7hkE3hFbfmoarKkTPBx/C6Bdss9PmyBrkx1JsZbXynIev4XyJSqohYawCRMUUy0BLMxqbUwpYEFbQrkJ7m72NKtBJb3qClKW1+6MZCINBmJs0sziDv7Ev/hhWUYILKthjO/W+Z7qoAdbQ65bz89hVaV/Lv1P7Qjj0smYfVrBsYYM+ihhxeX0TDxFo6cyXorA35lRMEIAQJbULnB6UTlowgHkP05FSc5Ta5Ekxztxb5u4lcfa/YaI8ekjU8/DIf9WPjjNNW8tNeFWD95lkSGy2pXUD2ZgjUbhBS+guEhzzaxgayMZhl7xigJQt2uXZilKKfqBJ8c+y84uYPoYuVhiJoKlWhMoAT6vNC9En4oH8lIn4Hvuf840WDt1XFbx8F8jfqlfUnjgJfZDK3ksoiH64kb8S3AIZrgURLeMplIngC+QybVWUwr3mPbtAFTJQsV2whkSd1uelD1A3xsEDHGnBPiwhSFP2thn53/4khRzNJ2atDb6GLOW3BR/OL4HPgJL9HGAYiQqZgEjKZaxpBuOHm2S++S4z6kQmq+0ziIOwfq6XbOJwl2TFdJ/HVclTiHUQyfNFWKP4DBDmZKGyDMxOXJdyAqkMPx4YnPPbuZBdy7fViRuH7bCljluni/aDi/XfUNEAac/SxQIBgcJKTyMsRteiE9Mt92iHQfQhjBjbvNZ64G6S7k6mo/dUWtAAsM240ySkjrLKlo=
  template:
    metadata:
      name: netbird-agent-setup-key
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-backend-oidc-credentials
  namespace: netbird
spec:
  encryptedData:
    clientId: AgCEhhixK/003E5HfsqYAQbGo87FBib3OgmtyIgHbCb8EcscFh03JJMWhrWCbRN7k3kC3q7CY3KAreG0/jMP4jJi9Xb1xRJaX9W0/bxSmFPh392XQfK9H5X32bsI6KyQ0Sm1E6FgN4d1rKdLL2qt2WG/Pvl37bKpQ/n+AEyBpXUcqCxbaSsIqDGiBam5/VVBXNEh94aE3J5hYARNPUgUOOwzxjZLxcPAw6jFKy5yJcXeahZ+1gaJpjv585ENeTlElZqJAc8p+lqw7s9Mf7+EW7FrhiJIv0ZUumGwdpBaNOk+D/64G0CStvdzQ+wXR4XC+SCqDsHZNuAy03nu6EqQUhRmOJI+SP5SUJaMABc9TjxeM9DwCHuaPElURGgiHXdOU8CwWi1kNkgzlojO2CuFgeMqHGrM8Gr87hQI0iSe59NnJVjUMZp3N89UeC5nP9A5fhYLP32rTJMRhIgVJqis9yS9XiVfh/9OXhLOv2klFwnHlIR6cH7YGHOcNna43aAsiuM2YkszGz5JKs9/Q0OJc9wTu3MxKrE7pkMe4U1svNqJra+HiE0ivbq7+R/TTUNYG1E3kk2yx/uF1+YRcMX/hlSLpzJuIeZ+Z2ogx+IkHSbBEFkd8Olh5DsBnUpxsRZPa3sDXNjY49dHFTLPeLY+tsrqgWPyNs6lcOOYN6FhTapaYE/Pg0AB8g9JBa147fLJ04uZ/6bLyZgmjU2+vgYjV+A=
    clientSecret: AgBMGLAe/rx3YoeqFv81ioQC8GI/GcIg2IXiNYOb61l3bVi5VmAT2uV7CmNuU+iIL/FIslgZeLYK1WSWJ7JM9gMzrZuAYFxi23SsYjafWRTWZfTYrQVK0dlp9TCBY3s8C5gujiE08htCmQRGyRnsaFmuxJfwGCzFnPYHKPKJWdgzvrIcY8Jy/bQgvBt3YWZ1264N8rmEb8zzFmn3dPZ2YD4ryXr6lY450VmdxYg/wCx9/8UuHcRqy3FH6UA8pC1Y6w6PsAN68ENobkmAT2aPRLntqMvqnrMaNkJWDx1VUuz8O4F9KfpHdvPko+r6xaeI5cuLaMR9VzGS/7mtADe3zPniPzcjj7sFmX0Wt/QaFGx5JsaetPs4ngw3IQwttPkvRibI31ZGEioJDSwBT3JfSisz/oS7fG1IaRh60zswNBx0CaA0vHJht3bbb/XPdHdoEgoPK6qRXokW7lHIx3OxP3FmSqUKV/51y2LqEIJY+T0DWCI3xXiOAxUKWw0ThThUmtIuEsNFJ/riCgLo0levCrUtTzV/+VxOmo8SDwZlhoIpbh0OOk2YyoPT+bCBDZUDWbxnEkVxL/z9ansvJ7db9j2qzv4b9qknPVDC8eB+BjMHZE+IK43BX5tBAobAck0bGRTN3R5XrDv2oo/bG1weDjj/j0aRFVmp5i3NyIFqSYY7p+d+82p5N2XbWVnynEUogx4RIKnHd/hCV0MLeeNWiyuSXcgTaeb35VMpvIExb22kQQ==
  template:
    metadata:
      name: netbird-backend-oidc-credentials
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-relay-secret
  namespace: netbird
spec:
  encryptedData:
    NB_AUTH_SECRET: AgBwuk9UL1sF+dHVXNjTGY8/FZbHszo+c/lBR/tAcOfwJRDZMxUpDUTZCMAh8dof6Sry5RRSenZiWOM5pUYz9Vzzple+e7UjB//TJCz7LtVr6DAcZE8UHC79pxYiQoRuVPLHnzV5uy7OOrGJfAlKfYxgmYWf9YYDV4P2H86Oiflw/fm7auedQL0N/FMfe/JCYKVnSIss1m33rVYeQS66RMj36uH4mo+ctXwbeKfQFEdMmY2MpEuaLNNp4UKqOMxErHSOIR6iCDjKXNgppSwQaZ0I0Ydqr5NWYrtIpMdf4gJA/+FTCCs/nRJkPZFFUD4897XKXvueQrw3lag6dEJiZPMEl5IUaysq7vjdkBKis6INlkfPUC9K2BAAMQePqZEogYtEQUU3Ahm8JjUD57RQikYKxWv7rvcxlMlzGNGkWBpKT8idjy8gYY0/B8+RuU1XaxZ4xkNN7FrfpfNQ0qxuT1PzPYOmq8q12NmgKCNkPr0G/Tqs4Y2L6P5GFUeFqf0qrDE0XTRbPz6ly4Bic7gMq0r5Oalj8UTmk05rcDbt/1CnWqYkN3d80n23R8NMDbWb634H8gFw9E7r+1VA1qrHZz5/5VuS84yIHe5tMZ9vtok3KXXz9eE83+ucceRSu+ex5GsrnxG2w3sZB7H1ogbpDAKm8Q/BeIKWxessnfL8ZdYA96zwn22ZNgpLu1/L1bxY4NdXxzbHVSXrnQvnu7CFMU5rZ+XrBA==
    NB_EXPOSED_ADDRESS: AgAsz3POoeLHik09MEKr+e52iDi2drsmsJntQHwi0NVjkGAgrz0VnY51VDrGOS9WaXZXAD1iPU1lmIAIzH+MerB6Vcpo5F8zZjn20yRQpT5XaPBztGtMWhHE8vey/xXCssafA8OgwN/201mJBVUBOGJKdIO8NTGmuCaO4LCxbmHP0fiYE24QzkPEhTdIkHjpINQuzgAfuozrPiSWmo7br+z/Y6w5s7n/onUGHUC4scjxkMq35v+5SbLGzfOJ9nyhw6ienhUwF88KezDvxz1hfra0Il8m3PzcqluQK+Q7E2emSZBWaKBTJGI8XfjiJN/tRtp0C72V+5nPSw+AnfdJ2BUGF1lsdgHWoSj4pJhBhKnxtbmWRmWs3WYMD4hzWsAf1Su50cNoLtlyYNUqENcbkrrprLacQbPYwodA9Is92sTCbmWXaAex6GGlRU9Kn9t++IXgwl3GjTUIY59EgQyYSq2yxYhBM+ZfIEY4VDpcQLxScqz/yYiYoYVlJp7cyH1MvZ1sfNnZGkDxlyZpxH0vPuw5n4HloQSwPmbZlCMFJbNXzZTcVG4PM4VCIoUaSQIn/1em1uu1McN9bz19d0Xfx89/C0Qs/RbhDga3vlafTxXN+/efivErI9/1cXLjY3UB2xj97PVejHeRoP+L4oyh5I0zCqcS+LBUrAYhZrwaoRYwHlyBUHr9tOF1JU3+m64h6uxVVuL8BSSUeEeKrbYYt5HsipYd/5PWEMzuLxB7gHPZvWG+aaEa
  template:
    metadata:
      name: netbird-relay-secret
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: netbird
spec:
  encryptedData:
    password: AgCG/s3MyJZ4M4qNyjVMonU+TDY+ocaxyye0tMF5kh7uVSJjHoASnEfKjnPpOC/quWksWploqGTgyHsL0RCY9BOPMpAuE4Pp+53VY6jAvH/NrJZOsqQEdgtdmjchE+3d3vZOCM5tHkjZSClvTGgTRFkXdl5pIWE4+o4WuLGtcPFrB4uLYCwq6UgazoOarKTAaj3oPuqssh4HFGyds8gKNxBHaq6cO3RvcH20P7dvnw005UCDymyVoTjxiuueT5SM48aH7O8aXhRYPbFbLwHsD70Q0UaYRT+UUPWnvpHWpDheHr3dVvWM3nkXxkOwre5Ld75BjfDfIM0/CihULYIOxUi/A5e2HTdauGQCGUSJvfh0z+VwKeVsLe6mBEJ6EKCWpWPDwVtnfEYk2Yohj/Z4O5F0ie2eRhXN+QsdCu+RQAxf47gFiKe0GueN1tVn+/QxGrrv4Lp2Jb1FiMwPSmSd4kJW44cKjd36j5ESrXyifOOwvcC+7eCDFOPfTMqYmloRp4RyXTec/BFzxZZ1ONWRYq0Tni4tlw04CCNsEVy+OYDArWu48v6cfPTAiHjgZBZHyIrRpG6cPOGzcWzJLdkQXkK6kxbUOqLVjPNEW2x71kRqiyAquszYV92FBoE3n8T1Au8GsADuurnN/t1f1B8SFi0BX+dILcMkxmQgVTBk4+946/NT2jRILE4ADCWYCI2CFzK5esvn44rlCzB5TeAPZA/l
    username: AgAr8X32/nMm8HRFPn5EK8rta+TqJtiBsmNKJqRe8EANSWCj/UlVfLSv2g1NSsOYZgorVKgHsS8ROj6K0xBbN13hGIwJ0kFICi9REHTdl5j34ElrCPsbYMwr3ZHjTHqTn1h/k1LS6COpp1lzAtiE9iUHKRu2EPLDJO1E5teRFLIBXMXanWWiLzjzgbqHImmW0p8/75yqs0DK1xYNiAAge7F7KJM9wq3lpRYbJ9oh9fv/hOEAL637NXr/fKtxMYj+hYzXzKSolbVep9iBmltCI5BJJMbsGLFaSa5oSjG+pS+2UB/k/osUiO2TfRpwn04yIiiMh5hmXqsAIN5T8t41l/46hqOvMMzrWfxHHNPGHBdlMuypXMVNrjloBLxbjApD8fNYR+TlkC6QqPEdv2Thd6Fta68JUWhLPyzbVhhGD9roMvY5r1diN4QQmMEfhl1JEumNWYDA7rSi1wrMjLr1XUJe+8WfxfW3f0RzNwXJvFtMYsqnjUJJkjmuswCwpfW5jBkAE2Q5wTdaFdoGCrZZsHUbiT3Vittw2/QUWvVLNvx3DZT3T9D6T7nVuq+3I7ORNFcXYhO8fdwevMhv/1L47n/EkVo/7ZfSj2xIvaDZ4/GTQvshttkOQkGFchFmkbmH/F/NP3/Hha0hk40s8n+1GR/nOsUEnEVBlSDbL8Aolt9ZC5DSJY8ISkPwWcLv8DqdY95MpmC+qgrT
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-admin-user
  namespace: nextcloud
spec:
  encryptedData:
    password: AgA133ceVXtfCLHa+fpkKReprcm1tpKZkbpTwYf4tFydhIIO2QLXRD2CioaSDYsk0aNlQUzXZyQJyMfplLs6G7Q0t/2d8cYVgVmEBcdiXgB3CF80gY4/2newgaCQp1MbCYoc28q4scE1vZvqgRfSGWiO12sxPklMWtrAxgIoNz8myEhKEuy42GfneXDUjfN+LvHFSJugDCVBLE4rc5MkNdB40UO6R54zxL40FQf7azZEMcLDIaUfM+ki4445V00rEMA8J9k9DEdPIXwRKva+osIcbKDuWv3L43fEyP0cfj1g5lgNcm+oX/Kmo+Lp48+jvdYOMQCnWW2A19pBIHlym33bPR/ViMkl/o+ztKiH6UxxNy2FyUZkIxKjL+1h/XctiqiNjJg7zCWEn2Mtue0zN0CMSe37ApzE5DzngsHpLAkvJ0DxHq3PPd3l3TevgeVj0hc2gR0sT08TuT9TRKK/EqCcPHpmeKtnzzruV7X+Vl0nreAXS1T/x2COW4FP8RE4RkXSDxf8OPhDy+MSFX5ortwbGM29yOt/9YU1Sq9uDiCj3OeXkFq0wEHCMsaSVJB255cMTR/a8FNrQFIeHT/UkRVQuSCVKAejpeN3EQnMr6ni9BQWu0BVLTwGa3XXQ94ucnmQtye3TwhIz30Hb/4mLloU5jMYUap3NwBp9cMwOxrKyH2HU0cDARAwgsgJhRA6a+OAGPz4eBwxZpuRza5CPRXNefF3cg==
    username: AgApHO0tUVeG++vsB3O4rLN+RrgTFG4uYNvk3lI8+KMV+n3faNak6j+RwhNtwqwcDO+loVQCfvi+mpW9txEqCfTRQ/xbI7oAHGP2fg5YJw4fHFDdKyH5tAeYE27nF21cAjABFymTqkqyD00A7qlorjljc6ld+KUwS67XL0LFsssuNfZQ9bxDjl/Yk2a+4ykNXNYQnMO7hqvxcrIhq9Dbbr8rOFNYTM6ByhBwaU7R5/QgUJ/52Eyb3Og0C97SSesNQUH2UsjHFVEqdMD7dGmdiAdvlA/5wd1V8yFlaUfPkOkWnCpnkpz4frSIMKJwQcIcDF5/YIfMtb/i4AOcUAv9qW4brlS4iCB37juPfItZkUtXm/iHItEAE3E2lkl9f+t1St1f9KuLh3aNOSgfiwTSU26Iw/850ZUjQKhkSXDw4uqiZ7XJC89dT1aavQfPLugj/5U7vuGRkkC+O04x3N6lLbDOqNC7O8H+sIwqd2AnOvVFvGf9ucnyY5ziNuDgNUL8bJQPd06zGh4u15tbOaat4LJ0DbNQACp4Fln9ivipgLvDb/8+WYSmkX2gL2TPMjzoq/FLtB3jgbhGcCDb0gqDkE/UT19JVHjCoBZAHi4Zds4cSknKiqbO8X5mcK4j4dQNutsA5cAbWasjtC18tCySevkmu/6k3oZutg4OYaoiWA0lHwu8AtFlQJGVMg2m4hEp2vaZ0SRsEQ==
  template:
    metadata:
      name: nextcloud-admin-user
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-database-auth
  namespace: nextcloud
spec:
  encryptedData:
    mariadb-database: AgAJjONCV3vYfnRn9F/Swg7ZPECa3rom6sS/9LVkyJJhhDxuAekF37urjn10HhsKSWHcb3+iqWd3IGXfVnBdBa2eEDdg8SQ/ljyvLqw8NjcaXIRdokUgRQ7NZlmTK6/ANFY8JUV960jseSKsCrxaA/WGBQnGTXf6MI3DrJlfdy3TxNXOYgXVACGh5sHOW/h38lbzLLshYbU9NwJz9KvDHdlDzjV2cF7FsXW91vclK6eNpMoABgkd2daOpwy83FT6mCAxkK9cCHlrsAEVXGMoilqPMVkvcM7VxXyixGeOXN6Ll8X79lkj5ja5uzTADEjdXhQiqyRrFzB/EAJF2LHy1sxgpxbpXCi2Ru/znJj7scomE6TF51VlY81sHxLFUxAgwMgQcnLQh3i1vYru/1NCrdeC9IAWdYCe6wuiOsf9FcHq4rx6N8zlkUFgewXVAWadUEpGad9QUQm//aVjq2kFM1kxRfiL25hoB6ec4WXhDgcy4f7CKtKIgS5IC3hxm3b4XAuuekr0s4G2+XGf1endy/Fwu4gVyFzRfQQx8hU75s2jaktmamaIfYlxuHfEGJnnxSbIFOUGM3sQd88sT/7gEXuNQCDXcsNylSaQNwYjPbu/iTlscMlowlTb32W1iM3IwSvx2xeo0DOMVzFUBqE5fxpG9zyA8U7gte7QQ1RWvRXE7Ll4dsOuHziPANZdMyRml8WCqgmcvtzc2s8=
    mariadb-host: AgBXn4r+i1apjXlWD08Qqx5Jh25Un1b85dgd2gLVrZmZo+PYXyAjQA5DLn0rSTfQJtcwp+8/huL94jxm3q1wXQkh4N5TsB7OItt7LModq6ZntgsDqvN+wCK9mvubBee4SSCKvQolgoI92YkJBBM3kdqN9Qvi/AY7X3zTUk2KjwlCeURVWY+L6kNEtPetNz5zZbMz/LAo6wPBX14ZhZL/48WWTURCyJqj9OHYpoH25yQaSEIaemAHRFh+5KLe7+J2nUmuiViXu2OuxmhwKKZYANAiKfur62Phmbcp0BTJnXjXyVAX/1kqmpkspuBOd1ITYg5HpD28BMHjeVWmoWVZf35eQ3IivD9O/Eg7l0SzBoGIBla4dRXbuKRb0hZON6CVqMzv86vo5zkLtd0n6s9vo1p8qQJ0eUCxHfXzwepRo96trvq+FUwFH9/dGhGan6VZP2RUwcvdY7Tk62fzEA6sqPuR/XFjfn4BrzMjaH+bB1DmtHjz/oeOX97wlCGboj1gJfvTIBF7HjzuDLJ1rcI7UTMTA88RscYMqlXhfG1Ao2ZD32d64q8VibWDgVoJMhskUcePivURV/o1xm74rYp1TxOdNn5TqDDzhYFSjgWcjmpbeqC3A3cxlf3DnjwFnm/daDRBXW/W4261GI0Zks8CTgWs9xaV/sNX0/Nei2o5Mtr1q1k6vm4n57Xg17th9OMS2JML5JshKodn9AWqKOqgvzA2Hw==
    mariadb-password: AgA8r1aajxOhMcQbDLegI4FOUu7lU0hlnWpniUCKz/oAVHf58bxsZBXsXwnF8InhUrrtUk7tEZeWK/YjbbI19IlQRuaJ0CXfbycSOnma/VkVTx2N+U8ODpqcT6btzdQXq9qi4NQkv3c/9JLxqiSCJi7cm/T+x6taSEzoL4Aa/zzJ+yMXZU8sB+mvkIxQMwriHQeQKoxK8AHYTG2WQM9a0tIjLsmXU7GhpekryMRKCaJDdjlSpFZCKO+ym/eCGeqP3oMXV7MRpB0l9qLIzs+GwKigup/x9lqLmHCuxaEeFZlkThLDTiuVXGoIZ+fwEOct2TH1aVKZFQRfRG/EPC5+AJApub2iiFofOJb8SxLHv9DFmfbPz3108vFUN0P3u8x2nZLwCWF9BjvpFrhYxWgr4M9PVtLu8caOBP3Ja9W6jYqawh+7ARXx7wW9GOPA1P9JUbt/7mCuueiuWw1xgchfceU9aZeFkebrhcH4/sK+Favt618Lv7XB8WIT8LK4IjEFs4SNW2rG6p7rYnCvtaL+Fc4YR8J+kkXxlo0LHLN9dM6TSiueorQdkpudxG7IJc/0xIs/h4OTpdPPhkEK76GQHen9X718nFI8oXPraGmyygcdIIBgV964PEMH8EwPZIHgHfddVosO2mupMwgccl8GDNUDTf4MO6hKPUWRb4P+QlUaFqwAw7jvu1psLA65NgWHxUn6FGruI39iPcUB9P0uQd2sSKNabw==
    mariadb-replication-password: AgAu3zmyxd/YLtPdmamJh8Re6VoBPXuwp9zeQTIK4Kd36Un7Ut/nTqxxbcdOESmEg3dVy1fnxIJp4NiLTY/xmkFGK3XpAgu5UOYSQolxzChB6sRC0nVNeeGixt4/tpv2PQocbFNo4XKKWiNZ9fLsEbLDAM/tZPhVamlbqCclQelZ9+PBcHcc0c2NkHqORHF5tW5QSCvOgt7b9zG//X/JGon2fa1ittDrqOr3s6Y7KJwT9YLQXCo0tlLjQd6yPQ7ew+KIwd+ZOlq7istE39o5s4qIRDuTHR+soGaTBcAHjBdVOBaM1naGtSWMuw6pUrRP+KdGOp3Uh1YuwjelmATAIR9+4/Gvvo7hSPlI8JaAV/RwJPlMJTe9MBWDV/fnLYnCWea/vNIj3n4eRkeXHEHW3mqu4K7PXN59Uw5lU8WzHI3Zm+vU6f20tYYA6E4wfD5FP2EwHgCKOEloYm9tji4OhTkERSQGnH3IKNk+Sh+UhDMGOd8V2VZXHoPEsLKKGny6D3FqRKUFzL4+ysjN16p1J6K9wJGwMHhBOabp5f1JhKoKOgLsbXrAEV5gdIzdlo5EoipA5jI1l7k3gcFUIptuxsBTAZwrBoNx29cebj9C/FRuz2kEC7J4LKDHlBHjhlJ3S0yZ9LwQzQk5LKKnwcSCKARAImrih7lvZsAzDAtWfDCstg3lBT6slx/mpvSBzAYhow6vXAI9XA6wxOy+iQUl8++oaM+oEg==
    mariadb-root-password: AgBjgQYehZaqqgIT1yQuE+EWb7o6m1ad8PTkkfSReV5QMDF6p/1/8bTkYxiXDR6VBRNJBKeZAhHHZMw54KRdDlj2JMPPr2VwKV1AJ0Kb1zXr2I+3cMydk93K9UHWx23QzececXBIXsXnry+ah8rEfJqENsrtqJSe6dv+T7cLJX9Sso1zvRqYq5uYvjEx/HmfX0r9q5R2RXhhBnJmBAIOVoh5khSCVsqO4Ah+Xt1VVhmWQWEdJmj/c4EXjqnJq753HlDmkI21N+A3VPbGIQLswkIDzIZeShjYAcZCXkLq3DDwJuN81fPSQZN/2Sp7nUm+HTdyOjTxXhEMbszWKwCW87C2uhEh0QgJb0iyc700/KHX9jV4Y1hwyC7thmSRf4p+O3iqSBgaImADh1BVBcpZM4D1CSjz/dpfcdDIxEAGHsiQDZexMJK9sC5Nx2kXP4pWXLxLc6XidsX7BWzIl7lzcY+rneZUWVtiRUARtSqYmO7l6KLmxUz4FuwPraD5B7S+1iy9kZ26JJ4AMJ+CklHMbR2rxUjcOt8KXkMMcUDaPoHs+gMddof8AKfBZGQkBc9oPnmM4T5yWhyklX9Q6zbk11bAplgQyjEroCWUb4q0ozb1EVI7FdtROGfvYU5R616//zo/AZVbAMncnRVdSkSq6Nmu0eaDs0mUttTf69o9D9yM3oQg4RO3yOTVELUi4n1mohxr9T7heip2aDp3S5DYvGsMWL63BA==
    mariadb-username: AgCVfKxxY8KGPODGmC6S8JHqVOVIvrz+t3mW+wAROGJa2mHeRur+lpt29tQ2a2Cc9dsGDg3s4cQ8xqVcFASkpD2gw/MeMK1dt/teNjYFLuWY/50S0a24nXHMzKwrZPTRu+a+7UrhMysitU9AdJ7uFqvLEftKj6y+zVXnB1OtTDfp5kV7XUq2aF9SVelETxoqUqPBWFUbWvAF0jyVMZ2EZbYOv9UmqD60C8VTA8gd+IDUcycnPfY/5dvz0YzufAxzyAQK0fGuX6WHEsdUqNX1YKdI7GQx1L3qMPgPSw6wlkYR22whwdgenyt63DffRR7mHhUmQeL9I75UUWuozFu/V/20/48lsskmSUCdJHxoiFvGdu0v6dPpfr0XMMQR1O5Uao0XLbjU6Asp9QTMAUxYIr7mdFU2Ae7jliRSviWD/2UOhtT5885Ua3dWOBvbAdsSwi6UEZFvH7GbkJRLDKmeoigCxKA9GmWOACrdCg7vvKVvYLFrct2NBCxpbKwTUw/qkT5KRlxx684lgv60ICYTPLLfxH9WI9MtxTEX94nUh9X8bKi2YkUi1KA0j9xOWXlKN+un1k+fUy8+jszNv+SMU7elCkoS6zFBHmg4uqIe2ItjBx62PR1rQ6SC7cl0NgXYu25ANW/Js9Ougeq7awOEhjj5ndB/OHdtEU5vFVeGc3XZa2TYRBuvkvPD75fTHT7SnxATRra6daehsv4=
  template:
    metadata:
      name: nextcloud-database-auth
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: openwebui-oidc-credentials
  namespace: openwebui
spec:
  encryptedData:
    clientId: AgATZxYet2gz1Of6VtDGIDaStL66v1Z4hJKja87uAnc730z78ePf5irRcv//h5rxJkA7Ge0dB8aAdyUSKwySIRwA67yV52bTPLroyYUuqe3bIRFaUWZJKWWf4gSpwHIXDDKMxhzq67k6fuwvVKT/Y74FgkUGRqmyDF0syvXiZ5dtS/V8wqxlWdS0cng03+JQf468N5LUM2CBW4kYwhfqB9EF2txNKABnAcBC5RVKRMq07VmP2BW645CpqlVCDiRQWeh25GsWDbd6YsA/BgG1uNY+OeN/Kd1TpjVdnMJJG5Kgud376Qmgl8fo7RKB51hnRETUL1yBMc5k/BUZH/deuZJTfvr9JqpWwjr2dAw9DPJvX+fTu2LT7kXQHJbYr74908Yd0JrUCO2yOz4e9DpHvCvWCOlYqN8VDc4D4ZKw7PJgNLPdkue8lNXsifCDARebtGScekXLuWnVfojTxmBPz/fUh/12VT1OJxmt0wGtbV3sPW/XT/abjdMI3/PVU4DCeD/4strplrwoPlehk6tQWtBxIIGD8KPHXiPX6WknNtWVlQ+iiIR6hHTLckCyFZ7f4x9msL8JdFquuv4ICdtJ1RJDajHjEm+GVIomsJKgUW1eN8b1pTptNM6YDPewRi87ASN7988s0semlV8YBtBoqs4p4PA3LX4PABEwBGEurJNVsTDW9HU4W786av3nQ3E8vsXMz1i0ZDNm8dY=
    clientSecret: AgAAGMRkzGpDj+45aIwMQ+DoK1FF9Nlxq7GBYgp8+9HWSdRt5rwQqmMv636bQeGqx3PGzrrCkTTUqzlr1otF7ilrwMWV+mPPjoR+Zz1q0LvlzKmhFngcTtSJJtTT5VQyd62EyxOIuVyNItdJS5SYis4mvrRUBfVnlNIVi6deC+k1s8CiF+EJPOGKeQHV8ozG11ryqK5OqzZNiidd+PT47FEfdVbnNz2dQzPKFK84F8ob9kMgbcQrxW2q1OmTCgbtweQLVq5cKmIBeIuxtomg2PMwyeQNPxwkiCSHzbZ9DA42CzqbvmqyOw4U1Ylz9KeG4AhQZN+mNVVp0pzy4yiOeJ3g9bnUFFO3GjExTxS27IXRkWmhZbR33faOASNeBEgDaiQah7dS4XNCkAPj/cs8tfGkBCfs5vFO5Buc8I0VkZiQGecX8u4sQCX258Y0vIK5fWpc2sBUjypW9w34EaztA5QqNSwcIo1KrIuKBVThqTqE0+RPkJNe73fNpGR8e5bOxjF7truTjSNKpx2VXAizvg92+LvUcd3KmL0NfMHlIeUe2sW6EzbAwhjIK6p98pOl1mo1tk1B638KT6tnHvLyr6Ej0/+XJ8rhcp1LBkQYsAol1eEOej2Hik0G1HO1YerEoiK3TIou15HwpQ2vYLFtbuzyCiuyq422UWbYiclptN3R4B7g70mXbOeY4xY1lOVOSTHYtakHSg/3Mzfhg25wM69c0ycXqMtC
  template:
    metadata:
      name: openwebui-oidc-credentials
      namespace: openwebui
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-setup-key
  namespace: traefik
spec:
  encryptedData:
    setupkey: AgABoc3LquXGAbkQTZyhKvRT+JTSSFemrCPZr+td5H3a/WO3OWLO8peO0K1MMe8LkJ5VjQvarFgAzxD4om7X/9OhnvjwY94sOtr7licFT2pMCQF+HkEcTaNVfWZZA9AUa17FQNM3XsSe2pJaw77fgoWDVguH7Y+inLeFt5vQ/SfnCGrXbCu3aEfjzwT+WgRMq+UNScX9Ry+wY3F4NrToGSR0zwkxHFPcd0jvsCUTFJGwYkM51+JpB7Ju/UTzOIyC3xZQTgzhpFvLwk9XbQqHKYhcP6fVKn5WUEAj93yzSowxfOcvqZTFWtFa+Nu7QgjimqyWmQqvOEzCn7SFj8iSqYEd0ocQp1g0Lv1Avt/2zaVlc7xweHAS68k0iZ5vYBinFCqla1fPmlUQdwydhsnVVnEctwuoaOEt6iztoVkN9u+W1hIn9aoacH1Ag6ieIbSfZBeDDbjV0OxPy8D1BLVQhIsHY8u8IlppYDXrYd6YPvr6Yc/n2+OIgsG8r6hVbdPeSjTfbRAXTPgqpDI3aThLugPHe1oPZW7mmoMhcWOlZkxv/8ciBTx2M0xP0Uz/hiXehyfOGC+XNgNuxm9wXyZ6xAVnCuGNVoTyahSGfNGPVkstWZpYhMtKscZupy6E8j6ObFvKkN2Nasvj0wDYhEAE+daLfovUNpdUhx0uWRg89iNyV2tZmQWBH/yv2QEyMF0qNAUftxVQU22DVHHnNfg/SyvS5BGnYtCJDdOammxEtt9lmSAkSWM=
  template:
    metadata:
      name: netbird-setup-key
      namespace: traefik
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r2
    helm.sh/chart: coturn-1.0.1
  name: coturn
  namespace: coturn
spec:
  dnsNames:
    - coturn.homelab.olav.ninja
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt
  secretName: coturn
  usages:
    - digital signature
    - key encipherment
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: keycloak-jwt-signing-certificate
  namespace: keycloak
spec:
  commonName: homelab
  duration: 8760h
  isCA: false
  issuerRef:
    group: cert-manager.io
    kind: Issuer
    name: self-signed-issuer
  privateKey:
    algorithm: RSA
    size: 2048
  renewBefore: 720h
  secretName: keycloak-jwt-signing-certificate
  subject:
    organizations:
      - homelab
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
  namespace: cert-manager
spec:
  acme:
    email: olav.s.th@gmail.com
    privateKeySecretRef:
      name: letsencrypt-issuer-account-key
    server: https://acme-v02.api.letsencrypt.org/directory
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              key: cloudflare-dns-api-token
              name: cloudflare-api-credentials
            email: olav.s.th@gmail.com
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: self-signed-issuer
  namespace: keycloak
spec:
  selfSigned: {}
---
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: default-l2-announcement-policy
  namespace: kube-system
spec:
  externalIPs: true
  loadBalancerIPs: true
---
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: default-ip-pool
  namespace: kube-system
spec:
  blocks:
    - start: 192.168.0.90
      stop: 192.168.0.99
---
apiVersion: keycloak.crossplane.io/v1beta1
kind: ProviderConfig
metadata:
  name: default
  namespace: crossplane
spec:
  credentials:
    secretRef:
      key: credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    source: Secret
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://upload.wikimedia.org/wikipedia/commons/b/bb/Gitea_Logo.svg
    ingress.kubernetes.io/proxy-body-size: 10000m
    traefik.ingress.kubernetes.io/router.middlewares: gitea-login-redirect-keycloak@kubernetescrd
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea
  namespace: gitea
spec:
  ingressClassName: null
  rules:
    - host: gitea.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: gitea-http
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - gitea.homelab.olav.ninja
      secretName: gitea-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "false"
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  rules:
    - host: homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hajimari
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - homelab.olav.ninja
      secretName: hajimari-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Immich
    hajimari.io/enable: "true"
    hajimari.io/icon: https://user-images.githubusercontent.com/27055614/182044984-2ee6d1ed-c4a7-4331-8a4b-64fcde77fe1f.png
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-server
  namespace: immich
spec:
  rules:
    - host: immich.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: immich-server
                port:
                  number: 2283
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - immich.homelab.olav.ninja
      secretName: immich-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  name: http
            path: /
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak.homelab.olav.ninja-tls
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.middlewares: keycloak-ipallowlist@kubernetescrd
  name: keycloak-admin
  namespace: keycloak
spec:
  ingressClassName: traefik
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  name: http
            path: /admin
            pathType: Prefix
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak.homelab.olav.ninja-tls
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  rules:
    - host: hubble.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hubble-ui
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - hubble.homelab.olav.ninja
      secretName: hubble-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /api
            pathType: Prefix
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /management.ManagementService/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-relay
                port:
                  number: 80
            path: /relay
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-signal
                port:
                  number: 80
            path: /signalexchange.SignalExchange/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-dashboard
                port:
                  number: 80
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://nextcloud.com/wp-content/uploads/2022/08/nextcloud-logo-icon.svg
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud
  namespace: nextcloud
spec:
  rules:
    - host: nextcloud.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: nextcloud
                port:
                  number: 8080
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - nextcloud.homelab.olav.ninja
      secretName: nextcloud-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.7.0
    helm.sh/chart: ollama-1.17.0
  name: ollama
  namespace: ollama
spec:
  rules:
    - host: ollama.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: ollama
                port:
                  number: 11434
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - ollama.homelab.olav.ninja
      secretName: ollama-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://avatars.githubusercontent.com/u/158137808
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.10
    helm.sh/chart: open-webui-6.16.0
  name: open-webui
  namespace: openwebui
spec:
  rules:
    - host: openwebui.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: open-webui
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - openwebui.homelab.olav.ninja
      secretName: openwebui-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
spec:
  controller: traefik.io/ingress-controller
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey-cluster
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-cluster-3.0.5
  name: gitea-valkey-cluster
  namespace: gitea
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
        - port: 16379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey-cluster
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis
  namespace: immich
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 7800
        - port: 8080
  podSelector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 3306
        - port: 3306
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mariadb
      app.kubernetes.io/version: 11.3.2
      helm.sh/chart: mariadb-18.2.0
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: Client
metadata:
  name: builtin-homelab-realm-management
  namespace: keycloak
spec:
  forProvider:
    clientId: realm-management
    realmIdRef:
      name: homelab
  managementPolicies:
    - Observe
  providerConfigRef:
    name: default
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: ClientScope
metadata:
  name: netbird-api
  namespace: netbird
spec:
  forProvider:
    consentScreenText: Netbird Management API
    includeInTokenScope: true
    name: netbird-api
    realmIdRef:
      name: homelab
---
apiVersion: pkg.crossplane.io/v1
kind: Provider
metadata:
  name: provider-keycloak
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/provider-keycloak:v2.1.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-auto-ready
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-auto-ready:v0.5.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-go-templating
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-go-templating:v0.10.0
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: KeystoreRsa
metadata:
  name: jwt-signing-certificate
  namespace: keycloak
spec:
  forProvider:
    active: true
    algorithm: RS256
    certificateSecretRef:
      key: tls.crt
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    enabled: true
    name: jwt-signing-certificate
    priority: 110
    privateKeySecretRef:
      key: tls.key
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    providerId: rsa
    realmIdRef:
      name: homelab
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: Realm
metadata:
  name: homelab
  namespace: keycloak
spec:
  forProvider:
    realm: homelab
---
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: csi.proxmox.sinextra.dev
spec:
  attachRequired: true
  podInfoOnMount: true
  storageCapacity: true
  volumeLifecycleModes:
    - Persistent
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: gitea-ssh
  namespace: gitea
spec:
  entryPoints:
    - ssh
  routes:
    - match: HostSNI(`*`)
      services:
        - name: gitea-ssh
          port: 22
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: external-cluster-ingressroute
  namespace: traefik
spec:
  entryPoints:
    - webpublic
  routes:
    - match: HostSNIRegexp(`jiyoung.cloud|{subdomain:[a-z]+}.jiyoung.cloud`)
      services:
        - name: external-cluster
          port: ingress-port
  tls:
    passthrough: true
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: login-redirect-keycloak
  namespace: gitea
spec:
  redirectRegex:
    permanent: false
    regex: ^(https?://[^/]+)/user/login(\?.*)?$
    replacement: ${1}/user/oauth2/keycloak${2}
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: ipallowlist
  namespace: keycloak
spec:
  ipAllowList:
    sourceRange:
      - 192.168.0.1/24
      - 10.0.0.0/8
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: securityheaders
  namespace: traefik
spec:
  headers:
    customFrameOptionsValue: SAMEORIGIN
    forceSTSHeader: false
    referrerPolicy: same-origin
    sslRedirect: true
    stsPreload: false
    stsSeconds: 15552000
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: jiyoung
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - valueSecretRef:
          key: jiyoung
          name: keycloak-initial-passwords
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: jiyoung
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: olav
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - valueSecretRef:
          key: olav
          name: keycloak-initial-passwords
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: olav
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-test-connection
  namespace: gitea
spec:
  containers:
    - args:
        - gitea-http:3000
      command:
        - wget
      image: busybox:latest
      name: wget
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.7.0
    helm.sh/chart: ollama-1.17.0
  name: ollama-test-connection
  namespace: ollama
spec:
  containers:
    - args:
        - ollama:11434
      command:
        - wget
      image: busybox
      name: wget
  restartPolicy: Never
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /mutate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    rules:
      - apiGroups:
          - cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
        resources:
          - certificaterequests
    sideEffects: None
    timeoutSeconds: 30
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.17.2
    helm.sh/chart: cert-manager-v1.17.2
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /validate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    namespaceSelector:
      matchExpressions:
        - key: cert-manager.io/disable-validation
          operator: NotIn
          values:
            - "true"
    rules:
      - apiGroups:
          - cert-manager.io
          - acme.cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - '*/*'
    sideEffects: None
    timeoutSeconds: 30
