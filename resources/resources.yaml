apiVersion: v1
kind: Namespace
metadata:
  name: bambustudio
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    cdi.kubevirt.io: ""
  name: cdi
---
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
---
apiVersion: v1
kind: Namespace
metadata:
  name: cnpg-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: coturn
---
apiVersion: v1
kind: Namespace
metadata:
  name: crossplane
---
apiVersion: v1
kind: Namespace
metadata:
  name: external-dns
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: Namespace
metadata:
  name: hajimari
---
apiVersion: v1
kind: Namespace
metadata:
  name: homeassistant
---
apiVersion: v1
kind: Namespace
metadata:
  name: immich
---
apiVersion: v1
kind: Namespace
metadata:
  name: keycloak
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: kube-system
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubevirt.io: ""
    pod-security.kubernetes.io/enforce: privileged
  name: kubevirt
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: privileged
  name: longhorn-system
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: netbird
---
apiVersion: v1
kind: Namespace
metadata:
  name: nextcloud
---
apiVersion: v1
kind: Namespace
metadata:
  name: ollama
---
apiVersion: v1
kind: Namespace
metadata:
  name: openwebui
---
apiVersion: v1
kind: Namespace
metadata:
  name: pingvinshare
---
apiVersion: v1
kind: Namespace
metadata:
  name: pubip-operator
---
apiVersion: v1
kind: Namespace
metadata:
  name: sablier
---
apiVersion: v1
kind: Namespace
metadata:
  name: sealed-secrets
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: traefik
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    operator.cdi.kubevirt.io: ""
  name: cdi-operator
  namespace: cdi
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-startupapicheck
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook
  namespace: cert-manager
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-cloudnative-pg
  namespace: cnpg-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: rbac-manager
  namespace: crossplane
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: external-dns
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: external-dns
    app.kubernetes.io/version: 0.19.0
    helm.sh/chart: external-dns-1.19.0
  name: external-dns
  namespace: external-dns
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-primary
  namespace: gitea
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
---
apiVersion: v1
automountServiceAccountToken: true
imagePullSecrets: []
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloakx
    app.kubernetes.io/version: 26.3.3
    helm.sh/chart: keycloakx-7.1.3
  name: keycloak-keycloakx
  namespace: keycloak
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-envoy
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-operator
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-generate-certs
  namespace: kube-system
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  name: hubble-relay
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-ui
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: multus
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    kubevirt.io: ""
  name: kubevirt-operator
  namespace: kubevirt
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-service-account
  namespace: longhorn-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-support-bundle
  namespace: longhorn-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-ui-service-account
  namespace: longhorn-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.12.0
    helm.sh/chart: netbird-dashboard-1.2.0
  name: netbird-dashboard
  namespace: netbird
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator
  namespace: netbird
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-master
  namespace: nextcloud
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.12.2
    helm.sh/chart: ollama-1.30.0
  name: ollama
  namespace: ollama
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: open-webui
    app.kubernetes.io/version: 0.6.32
    helm.sh/chart: open-webui-8.9.0
  name: open-webui
  namespace: openwebui
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-controller
  namespace: pubip-operator
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sablier-sablier
  namespace: sablier
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets
  namespace: sealed-secrets
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-37.1.2
  name: traefik
  namespace: traefik
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: containerized-data-importer
    app.kubernetes.io/component: storage
    app.kubernetes.io/managed-by: cdi-operator
    cdi.kubevirt.io: ""
  name: cdi-operator
  namespace: cdi
rules:
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - rolebindings
      - roles
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
      - configmaps
      - events
      - secrets
      - services
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - apps
    resources:
      - deployments
      - deployments/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
      - routes/custom-host
    verbs:
      - get
      - list
      - watch
      - create
      - update
  - apiGroups:
      - config.openshift.io
    resources:
      - proxies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - servicemonitors
      - prometheusrules
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - create
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - deletecollection
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
      - deletecollection
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - create
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
      - pods
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-cainjector-leader-election
      - cert-manager-cainjector-leader-election-core
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificaterequests
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-tokenrequest
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager-webhook-ca
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-controller
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator-tlsinterception-secrets
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - delete
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-tlsinterception-secrets
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - ""
    resourceNames:
      - hubble-server-certs
      - hubble-relay-client-certs
      - hubble-relay-server-certs
      - hubble-metrics-server-certs
      - hubble-ui-client-certs
    resources:
      - secrets
    verbs:
      - update
  - apiGroups:
      - ""
    resourceNames:
      - cilium-ca
    resources:
      - secrets
    verbs:
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    kubevirt.io: ""
  name: kubevirt-operator
  namespace: kubevirt
rules:
  - apiGroups:
      - ""
    resourceNames:
      - kubevirt-ca
      - kubevirt-export-ca
      - kubevirt-virt-handler-certs
      - kubevirt-virt-handler-server-certs
      - kubevirt-operator-certs
      - kubevirt-virt-api-certs
      - kubevirt-controller-certs
      - kubevirt-exportproxy-certs
    resources:
      - secrets
    verbs:
      - create
      - get
      - list
      - watch
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
      - get
      - list
      - watch
      - patch
      - delete
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
    verbs:
      - create
      - get
      - list
      - watch
      - patch
      - delete
  - apiGroups:
      - route.openshift.io
    resources:
      - routes/custom-host
    verbs:
      - create
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - delete
      - update
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
    verbs:
      - list
      - get
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - list
      - get
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - list
      - get
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - delete
      - update
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resourceNames:
      - kubevirt-export-ca
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn
  namespace: longhorn-system
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - events
      - secrets
      - services
      - endpoints
      - configmaps
      - serviceaccounts
      - persistentvolumeclaims
      - persistentvolumeclaims/status
    verbs:
      - '*'
  - apiGroups:
      - apps
    resources:
      - daemonsets
      - deployments
      - statefulsets
      - replicasets
    verbs:
      - '*'
  - apiGroups:
      - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - '*'
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - '*'
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - '*'
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - roles
      - rolebindings
    verbs:
      - '*'
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator
  namespace: netbird
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-leader-election-role
  namespace: pubip-operator
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - sealed-secrets-key
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - 'http:sealed-secrets:'
      - http:sealed-secrets:http
      - sealed-secrets
    resources:
      - services/proxy
    verbs:
      - create
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    operator.cdi.kubevirt.io: ""
  name: cdi-operator-cluster
rules:
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - clusterroles
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - security.openshift.io
    resources:
      - securitycontextconstraints
    verbs:
      - get
      - list
      - watch
      - update
      - create
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
      - customresourcedefinitions/status
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - cdi.kubevirt.io
      - upload.cdi.kubevirt.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - create
      - list
      - watch
  - apiGroups:
      - admissionregistration.k8s.io
    resourceNames:
      - cdi-api-dataimportcron-validate
      - cdi-api-populator-validate
      - cdi-api-datavolume-validate
      - cdi-api-validate
      - objecttransfer-api-validate
    resources:
      - validatingwebhookconfigurations
    verbs:
      - get
      - update
      - delete
  - apiGroups:
      - admissionregistration.k8s.io
    resourceNames:
      - cdi-api-datavolume-mutate
      - cdi-api-pvc-mutate
    resources:
      - mutatingwebhookconfigurations
    verbs:
      - get
      - update
      - delete
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshots
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - datavolumes
    verbs:
      - list
      - get
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - datasources
    verbs:
      - get
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - volumeclonesources
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - storageprofiles
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - cdis
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - cdiconfigs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - cdis/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - deletecollection
      - patch
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims/finalizers
      - pods/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - create
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
      - csidrivers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - config.openshift.io
    resources:
      - proxies
      - infrastructures
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - config.openshift.io
    resources:
      - clusterversions
    verbs:
      - get
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshots
      - volumesnapshotclasses
      - volumesnapshotcontents
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshots
    verbs:
      - update
      - deletecollection
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - scheduling.k8s.io
    resources:
      - priorityclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - image.openshift.io
    resources:
      - imagestreams
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines/finalizers
    verbs:
      - update
  - apiGroups:
      - forklift.cdi.kubevirt.io
    resources:
      - ovirtvolumepopulators
      - openstackvolumepopulators
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - dataimportcrons
    verbs:
      - get
      - list
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - create
      - update
      - patch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
  name: cert-manager-cluster-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-approve:cert-manager-io
rules:
  - apiGroups:
      - cert-manager.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - approve
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-certificates
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificates/status
      - certificaterequests
      - certificaterequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/finalizers
      - certificaterequests/finalizers
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
    verbs:
      - create
      - delete
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-certificatesigningrequests
rules:
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - certificates.k8s.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - sign
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-challenges
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - challenges/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - httproutes
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - route.openshift.io
    resources:
      - routes/custom-host
    verbs:
      - create
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-clusterissuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - clusterissuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-ingress-shim
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
    verbs:
      - create
      - update
      - delete
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/finalizers
    verbs:
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways
      - httproutes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways/finalizers
      - httproutes/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-issuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - issuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-orders
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - orders/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - create
      - delete
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: cert-manager-edit
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/status
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: cert-manager-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook:subjectaccessreviews
rules:
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - services
      - pods
      - endpoints
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
      - list
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - list
      - watch
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumbgppeeringpolicies
      - ciliumbgpnodeconfigs
      - ciliumbgpadvertisements
      - ciliumbgppeerconfigs
      - ciliumclusterwideenvoyconfigs
      - ciliumclusterwidenetworkpolicies
      - ciliumegressgatewaypolicies
      - ciliumendpoints
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumidentities
      - ciliumlocalredirectpolicies
      - ciliumnetworkpolicies
      - ciliumnodes
      - ciliumnodeconfigs
      - ciliumcidrgroups
      - ciliuml2announcementpolicies
      - ciliumpodippools
    verbs:
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
      - ciliumendpoints
      - ciliumnodes
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
    verbs:
      - delete
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
      - ciliumnodes/status
    verbs:
      - get
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints/status
      - ciliumendpoints
      - ciliuml2announcementpolicies/status
      - ciliumbgpnodeconfigs/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - ""
    resourceNames:
      - cilium-config
    resources:
      - configmaps
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services/status
    verbs:
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies
      - ciliumclusterwidenetworkpolicies
    verbs:
      - create
      - update
      - deletecollection
      - patch
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies/status
      - ciliumclusterwidenetworkpolicies/status
    verbs:
      - patch
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
      - ciliumidentities
    verbs:
      - delete
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes/status
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumbgppeerconfigs
      - ciliumbgpadvertisements
      - ciliumbgpnodeconfigs
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
      - patch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumbgpclusterconfigs/status
      - ciliumbgppeerconfigs/status
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - create
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resourceNames:
      - ciliumloadbalancerippools.cilium.io
      - ciliumbgppeeringpolicies.cilium.io
      - ciliumbgpclusterconfigs.cilium.io
      - ciliumbgppeerconfigs.cilium.io
      - ciliumbgpadvertisements.cilium.io
      - ciliumbgpnodeconfigs.cilium.io
      - ciliumbgpnodeconfigoverrides.cilium.io
      - ciliumclusterwideenvoyconfigs.cilium.io
      - ciliumclusterwidenetworkpolicies.cilium.io
      - ciliumegressgatewaypolicies.cilium.io
      - ciliumendpoints.cilium.io
      - ciliumendpointslices.cilium.io
      - ciliumenvoyconfigs.cilium.io
      - ciliumidentities.cilium.io
      - ciliumlocalredirectpolicies.cilium.io
      - ciliumnetworkpolicies.cilium.io
      - ciliumnodes.cilium.io
      - ciliumnodeconfigs.cilium.io
      - ciliumcidrgroups.cilium.io
      - ciliuml2announcementpolicies.cilium.io
      - ciliumpodippools.cilium.io
      - ciliumgatewayclassconfigs.cilium.io
    resources:
      - customresourcedefinitions
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumpodippools
      - ciliumbgppeeringpolicies
      - ciliumbgpclusterconfigs
      - ciliumbgpnodeconfigoverrides
      - ciliumbgppeerconfigs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumpodippools
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools/status
    verbs:
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-cloudnative-pg
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - mutatingwebhookconfigurations
      - validatingwebhookconfigurations
    verbs:
      - get
      - patch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusterimagecatalogs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
      - secrets
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps/status
      - secrets/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
      - pods
      - pods/exec
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - watch
  - apiGroups:
      - ""
    resources:
      - pods/status
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - create
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups
      - clusters
      - databases
      - poolers
      - publications
      - scheduledbackups
      - subscriptions
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - failoverquorums
    verbs:
      - create
      - delete
      - get
      - list
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups/status
      - databases/status
      - publications/status
      - scheduledbackups/status
      - subscriptions/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - imagecatalogs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusters/finalizers
      - poolers/finalizers
    verbs:
      - update
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusters/status
      - poolers/status
      - failoverquorums/status
    verbs:
      - get
      - patch
      - update
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - rolebindings
      - roles
    verbs:
      - create
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshots
    verbs:
      - create
      - get
      - list
      - patch
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-cloudnative-pg-edit
rules:
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups
      - clusters
      - databases
      - failoverquorums
      - poolers
      - publications
      - scheduledbackups
      - imagecatalogs
      - clusterimagecatalogs
      - subscriptions
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-cloudnative-pg-view
rules:
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups
      - clusters
      - databases
      - failoverquorums
      - poolers
      - publications
      - scheduledbackups
      - imagecatalogs
      - clusterimagecatalogs
      - subscriptions
    verbs:
      - get
      - list
      - watch
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-crossplane: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-admin
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-browse: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-browse
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-edit: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-edit
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-rbac-manager
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions/finalizers
    verbs:
      - update
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - escalate
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
    verbs:
      - bind
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
    verbs:
      - '*'
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-view: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-view
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    rbac.crossplane.io/aggregate-to-admin: "true"
  name: crossplane:aggregate-to-admin
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
      - namespaces
    verbs:
      - '*'
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - rolebindings
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    rbac.crossplane.io/aggregate-to-browse: "true"
  name: crossplane:aggregate-to-browse
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    rbac.crossplane.io/aggregate-to-edit: "true"
  name: crossplane:aggregate-to-edit
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    rbac.crossplane.io/aggregate-to-view: "true"
  name: crossplane:aggregate-to-view
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-allowed-provider-permissions: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane:allowed-provider-permissions
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    crossplane.io/scope: system
    helm.sh/chart: crossplane-2.0.2
    rbac.crossplane.io/aggregate-to-crossplane: "true"
  name: crossplane:system:aggregate-to-crossplane
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
      - customresourcedefinitions/status
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
      - services
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
      - ops.crossplane.io
      - pkg.crossplane.io
      - protection.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - extensions
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - delete
      - watch
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: external-dns
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: external-dns
    app.kubernetes.io/version: 0.19.0
    helm.sh/chart: external-dns-1.19.0
  name: external-dns
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - externaldns.k8s.io
    resources:
      - dnsendpoints
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - externaldns.k8s.io
    resources:
      - dnsendpoints/status
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
rules:
  - apiGroups:
      - ""
      - extensions
      - networking.k8s.io
      - discovery.k8s.io
    resources:
      - ingresses
      - namespaces
      - endpointslices
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-ui
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - componentstatuses
      - endpoints
      - namespaces
      - nodes
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubevirt.io: ""
  name: kubevirt-operator
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - list
      - watch
      - patch
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
      - services
      - endpoints
      - pods/exec
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - patch
      - delete
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - patch
  - apiGroups:
      - apps
    resources:
      - controllerrevisions
    verbs:
      - watch
      - list
      - create
      - delete
      - patch
  - apiGroups:
      - apps
    resources:
      - deployments
      - daemonsets
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - patch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - clusterrolebindings
      - roles
      - rolebindings
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - patch
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - patch
  - apiGroups:
      - security.openshift.io
    resources:
      - securitycontextconstraints
    verbs:
      - create
      - get
      - list
      - watch
  - apiGroups:
      - security.openshift.io
    resourceNames:
      - privileged
    resources:
      - securitycontextconstraints
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - security.openshift.io
    resourceNames:
      - kubevirt-handler
      - kubevirt-controller
    resources:
      - securitycontextconstraints
    verbs:
      - get
      - list
      - watch
      - update
      - delete
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
      - validatingadmissionpolicybindings
      - validatingadmissionpolicies
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
      - patch
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
      - patch
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - servicemonitors
      - prometheusrules
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - delete
      - patch
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachineinstances
    verbs:
      - get
      - list
      - watch
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines/status
    verbs:
      - patch
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachineinstancemigrations
    verbs:
      - create
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachineinstancepresets
    verbs:
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - limitranges
    verbs:
      - watch
      - list
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - snapshot.kubevirt.io
    resources:
      - virtualmachinesnapshots
      - virtualmachinerestores
      - virtualmachinesnapshotcontents
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - datasources
      - datavolumes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - instancetype.kubevirt.io
    resources:
      - virtualmachineinstancetypes
      - virtualmachineclusterinstancetypes
      - virtualmachinepreferences
      - virtualmachineclusterpreferences
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - migrations.kubevirt.io
    resources:
      - migrationpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - controllerrevisions
    verbs:
      - create
      - list
      - get
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
      - delete
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
      - endpoints
      - services
    verbs:
      - get
      - list
      - watch
      - delete
      - update
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - update
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - pods/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - pods/eviction
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - pods/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apps
    resources:
      - daemonsets
    verbs:
      - list
  - apiGroups:
      - apps
    resources:
      - controllerrevisions
    verbs:
      - watch
      - list
      - create
      - delete
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - snapshot.kubevirt.io
    resources:
      - virtualmachinesnapshots
      - virtualmachinesnapshots/status
      - virtualmachinesnapshots/finalizers
      - virtualmachinesnapshotcontents
      - virtualmachinesnapshotcontents/status
      - virtualmachinesnapshotcontents/finalizers
      - virtualmachinerestores
      - virtualmachinerestores/status
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - export.kubevirt.io
    resources:
      - virtualmachineexports
      - virtualmachineexports/status
      - virtualmachineexports/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - pool.kubevirt.io
    resources:
      - virtualmachinepools
      - virtualmachinepools/finalizers
      - virtualmachinepools/status
      - virtualmachinepools/scale
    verbs:
      - watch
      - list
      - create
      - delete
      - update
      - patch
      - get
  - apiGroups:
      - kubevirt.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines/finalizers
      - virtualmachineinstances/finalizers
    verbs:
      - update
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/stop
      - virtualmachineinstances/addvolume
      - virtualmachineinstances/removevolume
      - virtualmachineinstances/freeze
      - virtualmachineinstances/unfreeze
      - virtualmachineinstances/reset
      - virtualmachineinstances/softreboot
      - virtualmachineinstances/sev/setupsession
      - virtualmachineinstances/sev/injectlaunchsecret
    verbs:
      - update
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - k8s.cni.cncf.io
    resources:
      - network-attachment-definitions
    verbs:
      - get
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshotclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshots
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - instancetype.kubevirt.io
    resources:
      - virtualmachineinstancetypes
      - virtualmachineclusterinstancetypes
      - virtualmachinepreferences
      - virtualmachineclusterpreferences
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - migrations.kubevirt.io
    resources:
      - migrationpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - clone.kubevirt.io
    resources:
      - virtualmachineclones
      - virtualmachineclones/status
      - virtualmachineclones/finalizers
    verbs:
      - get
      - list
      - watch
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - resourcequotas
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
      - get
      - delete
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachineinstances
    verbs:
      - update
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - patch
      - list
      - watch
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - migrations.kubevirt.io
    resources:
      - migrationpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - export.kubevirt.io
    resources:
      - virtualmachineexports
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - list
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - version
      - guestfs
    verbs:
      - get
      - list
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachineinstances/console
      - virtualmachineinstances/vnc
      - virtualmachineinstances/vnc/screenshot
      - virtualmachineinstances/portforward
      - virtualmachineinstances/guestosinfo
      - virtualmachineinstances/filesystemlist
      - virtualmachineinstances/userlist
      - virtualmachineinstances/sev/fetchcertchain
      - virtualmachineinstances/sev/querylaunchmeasurement
      - virtualmachineinstances/usbredir
    verbs:
      - get
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachineinstances/pause
      - virtualmachineinstances/unpause
      - virtualmachineinstances/addvolume
      - virtualmachineinstances/removevolume
      - virtualmachineinstances/freeze
      - virtualmachineinstances/unfreeze
      - virtualmachineinstances/softreboot
      - virtualmachineinstances/reset
      - virtualmachineinstances/sev/setupsession
      - virtualmachineinstances/sev/injectlaunchsecret
    verbs:
      - update
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/expand-spec
      - virtualmachines/portforward
    verbs:
      - get
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/start
      - virtualmachines/stop
      - virtualmachines/restart
      - virtualmachines/addvolume
      - virtualmachines/removevolume
      - virtualmachines/memorydump
    verbs:
      - update
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - expand-vm-spec
    verbs:
      - update
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachineinstances
      - virtualmachineinstancepresets
      - virtualmachineinstancereplicasets
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachineinstancemigrations
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - snapshot.kubevirt.io
    resources:
      - virtualmachinesnapshots
      - virtualmachinesnapshotcontents
      - virtualmachinerestores
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - export.kubevirt.io
    resources:
      - virtualmachineexports
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - clone.kubevirt.io
    resources:
      - virtualmachineclones
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - instancetype.kubevirt.io
    resources:
      - virtualmachineinstancetypes
      - virtualmachineclusterinstancetypes
      - virtualmachinepreferences
      - virtualmachineclusterpreferences
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - pool.kubevirt.io
    resources:
      - virtualmachinepools
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - migrations.kubevirt.io
    resources:
      - migrationpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachineinstances/console
      - virtualmachineinstances/vnc
      - virtualmachineinstances/vnc/screenshot
      - virtualmachineinstances/portforward
      - virtualmachineinstances/guestosinfo
      - virtualmachineinstances/filesystemlist
      - virtualmachineinstances/userlist
      - virtualmachineinstances/sev/fetchcertchain
      - virtualmachineinstances/sev/querylaunchmeasurement
      - virtualmachineinstances/usbredir
    verbs:
      - get
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachineinstances/pause
      - virtualmachineinstances/unpause
      - virtualmachineinstances/addvolume
      - virtualmachineinstances/removevolume
      - virtualmachineinstances/freeze
      - virtualmachineinstances/unfreeze
      - virtualmachineinstances/softreboot
      - virtualmachineinstances/reset
      - virtualmachineinstances/sev/setupsession
      - virtualmachineinstances/sev/injectlaunchsecret
    verbs:
      - update
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/expand-spec
      - virtualmachines/portforward
    verbs:
      - get
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/start
      - virtualmachines/stop
      - virtualmachines/restart
      - virtualmachines/addvolume
      - virtualmachines/removevolume
      - virtualmachines/memorydump
    verbs:
      - update
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - expand-vm-spec
    verbs:
      - update
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachineinstances
      - virtualmachineinstancepresets
      - virtualmachineinstancereplicasets
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachineinstancemigrations
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - snapshot.kubevirt.io
    resources:
      - virtualmachinesnapshots
      - virtualmachinesnapshotcontents
      - virtualmachinerestores
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
  - apiGroups:
      - export.kubevirt.io
    resources:
      - virtualmachineexports
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
  - apiGroups:
      - clone.kubevirt.io
    resources:
      - virtualmachineclones
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
  - apiGroups:
      - instancetype.kubevirt.io
    resources:
      - virtualmachineinstancetypes
      - virtualmachineclusterinstancetypes
      - virtualmachinepreferences
      - virtualmachineclusterpreferences
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
  - apiGroups:
      - pool.kubevirt.io
    resources:
      - virtualmachinepools
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - list
  - apiGroups:
      - migrations.kubevirt.io
    resources:
      - migrationpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - list
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/expand-spec
      - virtualmachineinstances/guestosinfo
      - virtualmachineinstances/filesystemlist
      - virtualmachineinstances/userlist
      - virtualmachineinstances/sev/fetchcertchain
      - virtualmachineinstances/sev/querylaunchmeasurement
    verbs:
      - get
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - expand-vm-spec
    verbs:
      - update
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachineinstances
      - virtualmachineinstancepresets
      - virtualmachineinstancereplicasets
      - virtualmachineinstancemigrations
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - snapshot.kubevirt.io
    resources:
      - virtualmachinesnapshots
      - virtualmachinesnapshotcontents
      - virtualmachinerestores
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - export.kubevirt.io
    resources:
      - virtualmachineexports
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - clone.kubevirt.io
    resources:
      - virtualmachineclones
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - instancetype.kubevirt.io
    resources:
      - virtualmachineinstancetypes
      - virtualmachineclusterinstancetypes
      - virtualmachinepreferences
      - virtualmachineclusterpreferences
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pool.kubevirt.io
    resources:
      - virtualmachinepools
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - migrations.kubevirt.io
    resources:
      - migrationpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - instancetype.kubevirt.io
    resources:
      - virtualmachineclusterinstancetypes
      - virtualmachineclusterpreferences
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/migrate
    verbs:
      - update
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachineinstancemigrations
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    operator.kubevirt.io: ""
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
  name: kubevirt.io:operator
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - kubevirts
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
      - deletecollection
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-role
rules:
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
      - delete
      - deletecollection
  - apiGroups:
      - ""
    resources:
      - secrets
      - services
      - endpoints
      - configmaps
      - serviceaccounts
      - pods/log
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
      - persistentvolumes
      - persistentvolumeclaims
      - persistentvolumeclaims/status
      - nodes
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
  - apiGroups:
      - apps
    resources:
      - daemonsets
      - statefulsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - scheduling.k8s.io
    resources:
      - priorityclasses
    verbs:
      - watch
      - list
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
      - volumeattachments
      - volumeattachments/status
      - csinodes
      - csidrivers
      - csistoragecapacities
    verbs:
      - '*'
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshotclasses
      - volumesnapshots
      - volumesnapshotcontents
      - volumesnapshotcontents/status
    verbs:
      - '*'
  - apiGroups:
      - longhorn.io
    resources:
      - volumes
      - volumes/status
      - engines
      - engines/status
      - replicas
      - replicas/status
      - settings
      - settings/status
      - engineimages
      - engineimages/status
      - nodes
      - nodes/status
      - instancemanagers
      - instancemanagers/status
      - sharemanagers
      - sharemanagers/status
      - backingimages
      - backingimages/status
      - backingimagemanagers
      - backingimagemanagers/status
      - backingimagedatasources
      - backingimagedatasources/status
      - backuptargets
      - backuptargets/status
      - backupvolumes
      - backupvolumes/status
      - backups
      - backups/status
      - recurringjobs
      - recurringjobs/status
      - orphans
      - orphans/status
      - snapshots
      - snapshots/status
      - supportbundles
      - supportbundles/status
      - systembackups
      - systembackups/status
      - systemrestores
      - systemrestores/status
      - volumeattachments
      - volumeattachments/status
      - backupbackingimages
      - backupbackingimages/status
    verbs:
      - '*'
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - metrics.k8s.io
    resources:
      - pods
      - nodes
    verbs:
      - get
      - list
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - list
      - watch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - mutatingwebhookconfigurations
      - validatingwebhookconfigurations
    verbs:
      - get
      - list
      - create
      - patch
      - delete
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - roles
      - rolebindings
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - clusterroles
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: multus
rules:
  - apiGroups:
      - k8s.cni.cncf.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/status
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - ""
      - events.k8s.io
    resources:
      - events
    verbs:
      - create
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator
rules:
  - apiGroups:
      - netbird.io
    resources:
      - nbsetupkeys
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - netbird.io
    resources:
      - nbsetupkeys/finalizers
    verbs:
      - update
  - apiGroups:
      - netbird.io
    resources:
      - nbsetupkeys/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - netbird.io
    resources:
      - nbgroups
      - nbresources
      - nbroutingpeers
      - nbpolicies
    verbs:
      - get
      - patch
      - update
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - netbird.io
    resources:
      - nbgroups/status
      - nbresources/status
      - nbroutingpeers/status
      - nbpolicies/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - netbird.io
    resources:
      - nbgroups/finalizers
      - nbresources/finalizers
      - nbroutingpeers/finalizers
      - nbpolicies/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services/finalizers
    verbs:
      - update
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - get
      - patch
      - update
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - patch
      - update
      - create
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-manager-role
rules:
  - apiGroups:
      - '*'
    resources:
      - '*'
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters/finalizers
    verbs:
      - update
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters/status
    verbs:
      - get
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-metrics-auth-role
rules:
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-metrics-reader
rules:
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: publicipupdater-admin-role
rules:
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters
    verbs:
      - '*'
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters/status
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: publicipupdater-editor-role
rules:
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters/status
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: publicipupdater-viewer-role
rules:
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pubip.olav.ninja
    resources:
      - publicipupdaters/status
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: sablier-sablier
rules:
  - apiGroups:
      - apps
      - ""
    resources:
      - deployments
      - deployments/scale
      - statefulsets
      - statefulsets/scale
    verbs:
      - patch
      - get
      - update
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets-unsealer
rules:
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - update
      - delete
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-37.1.2
  name: traefik-traefik
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - nodes
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingressclasses
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.io
    resources:
      - ingressroutes
      - ingressroutetcps
      - ingressrouteudps
      - middlewares
      - middlewaretcps
      - serverstransports
      - serverstransporttcps
      - tlsoptions
      - tlsstores
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: containerized-data-importer
    app.kubernetes.io/component: storage
    app.kubernetes.io/managed-by: cdi-operator
    cdi.kubevirt.io: ""
  name: cdi-operator
  namespace: cdi
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cdi-operator
subjects:
  - kind: ServiceAccount
    name: cdi-operator
    namespace: cdi
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-cainjector:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cert-manager-tokenrequest
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-tokenrequest
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-startupapicheck:create-cert
subjects:
  - kind: ServiceAccount
    name: cert-manager-startupapicheck
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-webhook:dynamic-serving
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-config-agent
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator-tlsinterception-secrets
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-operator-tlsinterception-secrets
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-tlsinterception-secrets
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-tlsinterception-secrets
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hubble-generate-certs
subjects:
  - kind: ServiceAccount
    name: hubble-generate-certs
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    kubevirt.io: ""
  name: kubevirt-operator-rolebinding
  namespace: kubevirt
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubevirt-operator
subjects:
  - kind: ServiceAccount
    name: kubevirt-operator
    namespace: kubevirt
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: longhorn
  namespace: longhorn-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: longhorn
subjects:
  - kind: ServiceAccount
    name: longhorn-service-account
    namespace: longhorn-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator
  namespace: netbird
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: netbird-kubernetes-operator
subjects:
  - kind: ServiceAccount
    name: netbird-kubernetes-operator
    namespace: netbird
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-leader-election-rolebinding
  namespace: pubip-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pubip-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: pubip-operator-controller
    namespace: pubip-operator
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-key-admin
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-service-proxier
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:authenticated
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    operator.cdi.kubevirt.io: ""
  name: cdi-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cdi-operator-cluster
subjects:
  - kind: ServiceAccount
    name: cdi-operator
    namespace: cdi
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-cainjector
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-approve:cert-manager-io
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-approve:cert-manager-io
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-certificates
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificates
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-certificatesigningrequests
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificatesigningrequests
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-challenges
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-challenges
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-clusterissuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-clusterissuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-ingress-shim
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-ingress-shim
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-issuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-issuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-controller-orders
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-orders
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook:subjectaccessreviews
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-webhook:subjectaccessreviews
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium-operator
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-cloudnative-pg
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cnpg-cloudnative-pg
subjects:
  - kind: ServiceAccount
    name: cnpg-cloudnative-pg
    namespace: cnpg-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane
subjects:
  - kind: ServiceAccount
    name: crossplane
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: crossplane:masters
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
  name: crossplane-rbac-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-rbac-manager
subjects:
  - kind: ServiceAccount
    name: rbac-manager
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: external-dns
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: external-dns
    app.kubernetes.io/version: 0.19.0
    helm.sh/chart: external-dns-1.19.0
  name: external-dns-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
  - kind: ServiceAccount
    name: external-dns
    namespace: external-dns
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hajimari
subjects:
  - kind: ServiceAccount
    name: hajimari
    namespace: hajimari
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-ui
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hubble-ui
subjects:
  - kind: ServiceAccount
    name: hubble-ui
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    kubevirt.io: ""
  name: kubevirt-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubevirt-operator
subjects:
  - kind: ServiceAccount
    name: kubevirt-operator
    namespace: kubevirt
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-bind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: longhorn-role
subjects:
  - kind: ServiceAccount
    name: longhorn-service-account
    namespace: longhorn-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-support-bundle
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: longhorn-support-bundle
    namespace: longhorn-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: multus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: multus
subjects:
  - kind: ServiceAccount
    name: multus
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: netbird-kubernetes-operator
subjects:
  - kind: ServiceAccount
    name: netbird-kubernetes-operator
    namespace: netbird
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oidc-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:olav
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:jiyoung
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pubip-operator-manager-role
subjects:
  - kind: ServiceAccount
    name: pubip-operator-controller
    namespace: pubip-operator
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    helm.sh/chart: v0.0.1
  name: pubip-operator-metrics-auth-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pubip-operator-metrics-auth-role
subjects:
  - kind: ServiceAccount
    name: pubip-operator-controller
    namespace: pubip-operator
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: sablier-sablier
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sablier-sablier
subjects:
  - kind: ServiceAccount
    name: sablier-sablier
    namespace: sablier
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets-sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sealed-secrets-unsealer
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-37.1.2
  name: traefik-traefik
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-traefik
subjects:
  - kind: ServiceAccount
    name: traefik
    namespace: traefik
---
apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-controller-manager-config
  namespace: cnpg-system
---
apiVersion: v1
data:
  queries: |
    backends:
      query: |
       SELECT sa.datname
           , sa.usename
           , sa.application_name
           , states.state
           , COALESCE(sa.count, 0) AS total
           , COALESCE(sa.max_tx_secs, 0) AS max_tx_duration_seconds
           FROM ( VALUES ('active')
               , ('idle')
               , ('idle in transaction')
               , ('idle in transaction (aborted)')
               , ('fastpath function call')
               , ('disabled')
               ) AS states(state)
           LEFT JOIN (
               SELECT datname
                   , state
                   , usename
                   , COALESCE(application_name, '') AS application_name
                   , COUNT(*)
                   , COALESCE(EXTRACT (EPOCH FROM (max(now() - xact_start))), 0) AS max_tx_secs
               FROM pg_catalog.pg_stat_activity
               GROUP BY datname, state, usename, application_name
           ) sa ON states.state = sa.state
           WHERE sa.usename IS NOT NULL
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of the database"
        - usename:
            usage: "LABEL"
            description: "Name of the user"
        - application_name:
            usage: "LABEL"
            description: "Name of the application"
        - state:
            usage: "LABEL"
            description: "State of the backend"
        - total:
            usage: "GAUGE"
            description: "Number of backends"
        - max_tx_duration_seconds:
            usage: "GAUGE"
            description: "Maximum duration of a transaction in seconds"

    backends_waiting:
      query: |
       SELECT count(*) AS total
       FROM pg_catalog.pg_locks blocked_locks
       JOIN pg_catalog.pg_locks blocking_locks
         ON blocking_locks.locktype = blocked_locks.locktype
         AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
         AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
         AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
         AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
         AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
         AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
         AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
         AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
         AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
         AND blocking_locks.pid != blocked_locks.pid
       JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
       WHERE NOT blocked_locks.granted
      metrics:
        - total:
            usage: "GAUGE"
            description: "Total number of backends that are currently waiting on other queries"

    pg_database:
      query: |
        SELECT datname
          , pg_catalog.pg_database_size(datname) AS size_bytes
          , pg_catalog.age(datfrozenxid) AS xid_age
          , pg_catalog.mxid_age(datminmxid) AS mxid_age
        FROM pg_catalog.pg_database
        WHERE datallowconn
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of the database"
        - size_bytes:
            usage: "GAUGE"
            description: "Disk space used by the database"
        - xid_age:
            usage: "GAUGE"
            description: "Number of transactions from the frozen XID to the current one"
        - mxid_age:
            usage: "GAUGE"
            description: "Number of multiple transactions (Multixact) from the frozen XID to the current one"

    pg_postmaster:
      query: |
        SELECT EXTRACT(EPOCH FROM pg_postmaster_start_time) AS start_time
        FROM pg_catalog.pg_postmaster_start_time()
      metrics:
        - start_time:
            usage: "GAUGE"
            description: "Time at which postgres started (based on epoch)"

    pg_replication:
      query: "SELECT CASE WHEN (
                NOT pg_catalog.pg_is_in_recovery()
                OR pg_catalog.pg_last_wal_receive_lsn() = pg_catalog.pg_last_wal_replay_lsn())
              THEN 0
              ELSE GREATEST (0,
                EXTRACT(EPOCH FROM (now() - pg_catalog.pg_last_xact_replay_timestamp())))
              END AS lag,
              pg_catalog.pg_is_in_recovery() AS in_recovery,
              EXISTS (TABLE pg_stat_wal_receiver) AS is_wal_receiver_up,
              (SELECT count(*) FROM pg_catalog.pg_stat_replication) AS streaming_replicas"
      metrics:
        - lag:
            usage: "GAUGE"
            description: "Replication lag behind primary in seconds"
        - in_recovery:
            usage: "GAUGE"
            description: "Whether the instance is in recovery"
        - is_wal_receiver_up:
            usage: "GAUGE"
            description: "Whether the instance wal_receiver is up"
        - streaming_replicas:
            usage: "GAUGE"
            description: "Number of streaming replicas connected to the instance"

    pg_replication_slots:
      query: |
        SELECT slot_name,
          slot_type,
          database,
          active,
          (CASE pg_catalog.pg_is_in_recovery()
            WHEN TRUE THEN pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_last_wal_receive_lsn(), restart_lsn)
            ELSE pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), restart_lsn)
          END) as pg_wal_lsn_diff
        FROM pg_catalog.pg_replication_slots
        WHERE NOT temporary
      metrics:
        - slot_name:
            usage: "LABEL"
            description: "Name of the replication slot"
        - slot_type:
            usage: "LABEL"
            description: "Type of the replication slot"
        - database:
            usage: "LABEL"
            description: "Name of the database"
        - active:
            usage: "GAUGE"
            description: "Flag indicating whether the slot is active"
        - pg_wal_lsn_diff:
            usage: "GAUGE"
            description: "Replication lag in bytes"

    pg_stat_archiver:
      query: |
        SELECT archived_count
          , failed_count
          , COALESCE(EXTRACT(EPOCH FROM (now() - last_archived_time)), -1) AS seconds_since_last_archival
          , COALESCE(EXTRACT(EPOCH FROM (now() - last_failed_time)), -1) AS seconds_since_last_failure
          , COALESCE(EXTRACT(EPOCH FROM last_archived_time), -1) AS last_archived_time
          , COALESCE(EXTRACT(EPOCH FROM last_failed_time), -1) AS last_failed_time
          , COALESCE(CAST(CAST('x'||pg_catalog.right(pg_catalog.split_part(last_archived_wal, '.', 1), 16) AS pg_catalog.bit(64)) AS pg_catalog.int8), -1) AS last_archived_wal_start_lsn
          , COALESCE(CAST(CAST('x'||pg_catalog.right(pg_catalog.split_part(last_failed_wal, '.', 1), 16) AS pg_catalog.bit(64)) AS pg_catalog.int8), -1) AS last_failed_wal_start_lsn
          , EXTRACT(EPOCH FROM stats_reset) AS stats_reset_time
        FROM pg_catalog.pg_stat_archiver
      metrics:
        - archived_count:
            usage: "COUNTER"
            description: "Number of WAL files that have been successfully archived"
        - failed_count:
            usage: "COUNTER"
            description: "Number of failed attempts for archiving WAL files"
        - seconds_since_last_archival:
            usage: "GAUGE"
            description: "Seconds since the last successful archival operation"
        - seconds_since_last_failure:
            usage: "GAUGE"
            description: "Seconds since the last failed archival operation"
        - last_archived_time:
            usage: "GAUGE"
            description: "Epoch of the last time WAL archiving succeeded"
        - last_failed_time:
            usage: "GAUGE"
            description: "Epoch of the last time WAL archiving failed"
        - last_archived_wal_start_lsn:
            usage: "GAUGE"
            description: "Archived WAL start LSN"
        - last_failed_wal_start_lsn:
            usage: "GAUGE"
            description: "Last failed WAL LSN"
        - stats_reset_time:
            usage: "GAUGE"
            description: "Time at which these statistics were last reset"

    pg_stat_bgwriter:
      runonserver: "<17.0.0"
      query: |
        SELECT checkpoints_timed
          , checkpoints_req
          , checkpoint_write_time
          , checkpoint_sync_time
          , buffers_checkpoint
          , buffers_clean
          , maxwritten_clean
          , buffers_backend
          , buffers_backend_fsync
          , buffers_alloc
        FROM pg_catalog.pg_stat_bgwriter
      metrics:
        - checkpoints_timed:
            usage: "COUNTER"
            description: "Number of scheduled checkpoints that have been performed"
        - checkpoints_req:
            usage: "COUNTER"
            description: "Number of requested checkpoints that have been performed"
        - checkpoint_write_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in milliseconds"
        - checkpoint_sync_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in milliseconds"
        - buffers_checkpoint:
            usage: "COUNTER"
            description: "Number of buffers written during checkpoints"
        - buffers_clean:
            usage: "COUNTER"
            description: "Number of buffers written by the background writer"
        - maxwritten_clean:
            usage: "COUNTER"
            description: "Number of times the background writer stopped a cleaning scan because it had written too many buffers"
        - buffers_backend:
            usage: "COUNTER"
            description: "Number of buffers written directly by a backend"
        - buffers_backend_fsync:
            usage: "COUNTER"
            description: "Number of times a backend had to execute its own fsync call (normally the background writer handles those even when the backend does its own write)"
        - buffers_alloc:
            usage: "COUNTER"
            description: "Number of buffers allocated"

    pg_stat_bgwriter_17:
      runonserver: ">=17.0.0"
      name: pg_stat_bgwriter
      query: |
        SELECT buffers_clean
          , maxwritten_clean
          , buffers_alloc
          , EXTRACT(EPOCH FROM stats_reset) AS stats_reset_time
        FROM pg_catalog.pg_stat_bgwriter
      metrics:
        - buffers_clean:
            usage: "COUNTER"
            description: "Number of buffers written by the background writer"
        - maxwritten_clean:
            usage: "COUNTER"
            description: "Number of times the background writer stopped a cleaning scan because it had written too many buffers"
        - buffers_alloc:
            usage: "COUNTER"
            description: "Number of buffers allocated"
        - stats_reset_time:
            usage: "GAUGE"
            description: "Time at which these statistics were last reset"

    pg_stat_checkpointer:
      runonserver: ">=17.0.0"
      query: |
        SELECT num_timed AS checkpoints_timed
          , num_requested AS checkpoints_req
          , restartpoints_timed
          , restartpoints_req
          , restartpoints_done
          , write_time
          , sync_time
          , buffers_written
          , EXTRACT(EPOCH FROM stats_reset) AS stats_reset_time
        FROM pg_catalog.pg_stat_checkpointer
      metrics:
        - checkpoints_timed:
            usage: "COUNTER"
            description: "Number of scheduled checkpoints that have been performed"
        - checkpoints_req:
            usage: "COUNTER"
            description: "Number of requested checkpoints that have been performed"
        - restartpoints_timed:
            usage: "COUNTER"
            description: "Number of scheduled restartpoints due to timeout or after a failed attempt to perform it"
        - restartpoints_req:
            usage: "COUNTER"
            description: "Number of requested restartpoints that have been performed"
        - restartpoints_done:
            usage: "COUNTER"
            description: "Number of restartpoints that have been performed"
        - write_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of processing checkpoints and restartpoints where files are written to disk, in milliseconds"
        - sync_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of processing checkpoints and restartpoints where files are synchronized to disk, in milliseconds"
        - buffers_written:
            usage: "COUNTER"
            description: "Number of buffers written during checkpoints and restartpoints"
        - stats_reset_time:
            usage: "GAUGE"
            description: "Time at which these statistics were last reset"

    pg_stat_database:
      query: |
        SELECT datname
          , xact_commit
          , xact_rollback
          , blks_read
          , blks_hit
          , tup_returned
          , tup_fetched
          , tup_inserted
          , tup_updated
          , tup_deleted
          , conflicts
          , temp_files
          , temp_bytes
          , deadlocks
          , blk_read_time
          , blk_write_time
        FROM pg_catalog.pg_stat_database
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of this database"
        - xact_commit:
            usage: "COUNTER"
            description: "Number of transactions in this database that have been committed"
        - xact_rollback:
            usage: "COUNTER"
            description: "Number of transactions in this database that have been rolled back"
        - blks_read:
            usage: "COUNTER"
            description: "Number of disk blocks read in this database"
        - blks_hit:
            usage: "COUNTER"
            description: "Number of times disk blocks were found already in the buffer cache, so that a read was not necessary (this only includes hits in the PostgreSQL buffer cache, not the operating system's file system cache)"
        - tup_returned:
            usage: "COUNTER"
            description: "Number of rows returned by queries in this database"
        - tup_fetched:
            usage: "COUNTER"
            description: "Number of rows fetched by queries in this database"
        - tup_inserted:
            usage: "COUNTER"
            description: "Number of rows inserted by queries in this database"
        - tup_updated:
            usage: "COUNTER"
            description: "Number of rows updated by queries in this database"
        - tup_deleted:
            usage: "COUNTER"
            description: "Number of rows deleted by queries in this database"
        - conflicts:
            usage: "COUNTER"
            description: "Number of queries canceled due to conflicts with recovery in this database"
        - temp_files:
            usage: "COUNTER"
            description: "Number of temporary files created by queries in this database"
        - temp_bytes:
            usage: "COUNTER"
            description: "Total amount of data written to temporary files by queries in this database"
        - deadlocks:
            usage: "COUNTER"
            description: "Number of deadlocks detected in this database"
        - blk_read_time:
            usage: "COUNTER"
            description: "Time spent reading data file blocks by backends in this database, in milliseconds"
        - blk_write_time:
            usage: "COUNTER"
            description: "Time spent writing data file blocks by backends in this database, in milliseconds"

    pg_stat_replication:
      primary: true
      query: |
       SELECT usename
         , COALESCE(application_name, '') AS application_name
         , COALESCE(client_addr::text, '') AS client_addr
         , COALESCE(client_port::text, '') AS client_port
         , EXTRACT(EPOCH FROM backend_start) AS backend_start
         , COALESCE(pg_catalog.age(backend_xmin), 0) AS backend_xmin_age
         , pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), sent_lsn) AS sent_diff_bytes
         , pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), write_lsn) AS write_diff_bytes
         , pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), flush_lsn) AS flush_diff_bytes
         , COALESCE(pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), replay_lsn),0) AS replay_diff_bytes
         , COALESCE((EXTRACT(EPOCH FROM write_lag)),0)::float AS write_lag_seconds
         , COALESCE((EXTRACT(EPOCH FROM flush_lag)),0)::float AS flush_lag_seconds
         , COALESCE((EXTRACT(EPOCH FROM replay_lag)),0)::float AS replay_lag_seconds
       FROM pg_catalog.pg_stat_replication
      metrics:
        - usename:
            usage: "LABEL"
            description: "Name of the replication user"
        - application_name:
            usage: "LABEL"
            description: "Name of the application"
        - client_addr:
            usage: "LABEL"
            description: "Client IP address"
        - client_port:
            usage: "LABEL"
            description: "Client TCP port"
        - backend_start:
            usage: "COUNTER"
            description: "Time when this process was started"
        - backend_xmin_age:
            usage: "COUNTER"
            description: "The age of this standby's xmin horizon"
        - sent_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location sent on this connection"
        - write_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location written to disk by this standby server"
        - flush_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location flushed to disk by this standby server"
        - replay_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location replayed into the database on this standby server"
        - write_lag_seconds:
            usage: "GAUGE"
            description: "Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written it"
        - flush_lag_seconds:
            usage: "GAUGE"
            description: "Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written and flushed it"
        - replay_lag_seconds:
            usage: "GAUGE"
            description: "Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written, flushed and applied it"

    pg_settings:
      query: |
        SELECT name,
        CASE setting WHEN 'on' THEN '1' WHEN 'off' THEN '0' ELSE setting END AS setting
        FROM pg_catalog.pg_settings
        WHERE vartype IN ('integer', 'real', 'bool')
        ORDER BY 1
      metrics:
        - name:
            usage: "LABEL"
            description: "Name of the setting"
        - setting:
            usage: "GAUGE"
            description: "Setting value"

    pg_extensions:
      query: |
        SELECT
         current_database() as datname,
         name as extname,
         default_version,
         installed_version,
         CASE
           WHEN default_version = installed_version THEN 0
           ELSE 1
        END AS update_available
        FROM pg_catalog.pg_available_extensions
        WHERE installed_version IS NOT NULL
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of the database"
        - extname:
            usage: "LABEL"
            description: "Extension name"
        - default_version:
            usage: "LABEL"
            description: "Default version"
        - installed_version:
            usage: "LABEL"
            description: "Installed version"
        - update_available:
            usage: "GAUGE"
            description: "An update is available"
      target_databases:
        - '*'
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    cnpg.io/reload: ""
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-default-monitoring
  namespace: cnpg-system
---
apiVersion: v1
data:
  turnserver.conf: "# Coturn TURN SERVER configuration file\n#\n# Boolean values note: where a boolean value is supposed to be used,\n# you can use '0', 'off', 'no', 'false', or 'f' as 'false',\n# and you can use '1', 'on', 'yes', 'true', or 't' as 'true'\n# If the value is missing, then it means 'true' by default.\n#\n\n# Listener interface device (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#listening-device=eth0\n\n# TURN listener port for UDP and TCP (Default: 3478).\n# Note: actually, TLS & DTLS sessions can connect to the\n# \"plain\" TCP & UDP port(s), too - if allowed by configuration.\n#\n#listening-port=3478\n\n# TURN listener port for TLS (Default: 5349).\n# Note: actually, \"plain\" TCP & UDP sessions can connect to the TLS & DTLS\n# port(s), too - if allowed by configuration. The TURN server\n# \"automatically\" recognizes the type of traffic. Actually, two listening\n# endpoints (the \"plain\" one and the \"tls\" one) are equivalent in terms of\n# functionality; but Coturn keeps both endpoints to satisfy the RFC 5766 specs.\n# For secure TCP connections, Coturn currently supports SSL version 3 and\n# TLS version 1.0, 1.1 and 1.2.\n# For secure UDP connections, Coturn supports DTLS version 1.\n#\n#tls-listening-port=5349\n\n# Alternative listening port for UDP and TCP listeners;\n# default (or zero) value means \"listening port plus one\".\n# This is needed for RFC 5780 support\n# (STUN extension specs, NAT behavior discovery). The TURN Server\n# supports RFC 5780 only if it is started with more than one\n# listening IP address of the same family (IPv4 or IPv6).\n# RFC 5780 is supported only by UDP protocol, other protocols\n# are listening to that endpoint only for \"symmetry\".\n#\n#alt-listening-port=0\n\n# Alternative listening port for TLS and DTLS protocols.\n# Default (or zero) value means \"TLS listening port plus one\".\n#\n#alt-tls-listening-port=0\n\n# Some network setups will require using a TCP reverse proxy in front\n# of the STUN server. If the proxy port option is set a single listener\n# is started on the given port that accepts connections using the\n# haproxy proxy protocol v2.\n# (https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)\n#\n#tcp-proxy-port=5555\n\n# Listener IP address of relay server. Multiple listeners can be specified.\n# If no IP(s) specified in the config file or in the command line options,\n# then all IPv4 and IPv6 system IPs will be used for listening.\n#\n#listening-ip=172.17.19.101\n#listening-ip=10.207.21.238\n#listening-ip=2607:f0d0:1002:51::4\n\n# Auxiliary STUN/TURN server listening endpoint.\n# Aux servers have almost full TURN and STUN functionality.\n# The (minor) limitations are:\n#\n# 1) Auxiliary servers do not have alternative ports and\n# they do not support STUN RFC 5780 functionality (CHANGE REQUEST).\n#\n# 2) Auxiliary servers also are never returning ALTERNATIVE-SERVER reply.\n#\n# Valid formats are 1.2.3.4:5555 for IPv4 and [1:2::3:4]:5555 for IPv6.\n#\n# There may be multiple aux-server options, each will be used for listening\n# to client requests.\n#\n#aux-server=172.17.19.110:33478\n#aux-server=[2607:f0d0:1002:51::4]:33478\n\n# (recommended for older Linuxes only)\n# Automatically balance UDP traffic over auxiliary servers (if configured).\n# The load balancing is using the ALTERNATE-SERVER mechanism.\n# The TURN client must support 300 ALTERNATE-SERVER response for this\n# functionality.\n#\n#udp-self-balance\n\n# Relay interface device for relay sockets (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#relay-device=eth1\n\n# Relay address (the local IP address that will be used to relay the\n# packets to the peer).\n# Multiple relay addresses may be used.\n# The same IP(s) can be used as both listening IP(s) and relay IP(s).\n#\n# If no relay IP(s) specified, then the turnserver will apply the default\n# policy: it will decide itself which relay addresses to be used, and it\n# will always be using the client socket IP address as the relay IP address\n# of the TURN session (if the requested relay address family is the same\n# as the family of the client socket).\n#\n#relay-ip=172.17.19.105\n#relay-ip=2607:f0d0:1002:51::5\n\n# For Amazon EC2 users:\n#\n# TURN Server public/private address mapping, if the server is behind NAT.\n# In that situation, if a -X is used in form \"-X <ip>\" then that ip will be reported\n# as relay IP address of all allocations. This scenario works only in a simple case\n# when one single relay address is be used, and no RFC5780 functionality is required.\n# That single relay address must be mapped by NAT to the 'external' IP.\n# The \"external-ip\" value, if not empty, is returned in XOR-RELAYED-ADDRESS field.\n# For that 'external' IP, NAT must forward ports directly (relayed port 12345\n# must be always mapped to the same 'external' port 12345).\n#\n# In more complex case when more than one IP address is involved,\n# that option must be used several times, each entry must\n# have form \"-X <public-ip/private-ip>\", to map all involved addresses.\n# RFC5780 NAT discovery STUN functionality will work correctly,\n# if the addresses are mapped properly, even when the TURN server itself\n# is behind A NAT.\n#\n# By default, this value is empty, and no address mapping is used.\n#\n#external-ip=60.70.80.91\n#\n#OR:\n#\n#external-ip=60.70.80.91/172.17.19.101\n#external-ip=60.70.80.92/172.17.19.102\n\n\n# Number of the relay threads to handle the established connections\n# (in addition to authentication thread and the listener thread).\n# If explicitly set to 0 then application runs relay process in a\n# single thread, in the same thread with the listener process\n# (the authentication thread will still be a separate thread).\n#\n# If this parameter is not set, then the default OS-dependent\n# thread pattern algorithm will be employed. Usually the default\n# algorithm is optimal, so you have to change this option\n# if you want to make some fine tweaks.\n#\n# In the older systems (Linux kernel before 3.9),\n# the number of UDP threads is always one thread per network listening\n# endpoint - including the auxiliary endpoints - unless 0 (zero) or\n# 1 (one) value is set.\n#\n#relay-threads=0\n\n# Lower and upper bounds of the UDP relay endpoints:\n# (default values are 49152 and 65535)\n#\n#min-port=49152\n#max-port=65535\n\n# Uncomment to run TURN server in 'normal' 'moderate' verbose mode.\n# By default the verbose mode is off.\n#verbose\n\n# Uncomment to run TURN server in 'extra' verbose mode.\n# This mode is very annoying and produces lots of output.\n# Not recommended under normal circumstances.\n#\n#Verbose\n\n# Uncomment to use fingerprints in the TURN messages.\n# By default the fingerprints are off.\n#\n#fingerprint\n\n# Uncomment to use long-term credential mechanism.\n# By default no credentials mechanism is used (any user allowed).\n#\n#lt-cred-mech\n\n# This option is the opposite of lt-cred-mech.\n# (TURN Server with no-auth option allows anonymous access).\n# If neither option is defined, and no users are defined,\n# then no-auth is default. If at least one user is defined,\n# in this file, in command line or in usersdb file, then\n# lt-cred-mech is default.\n#\n#no-auth\n\n# Enable prometheus exporter\n# If enabled the turnserver will expose an endpoint with stats on a prometheus format\n# this endpoint is listening on a different port to not conflict with other configurations.\n#\n# You can simply run the turnserver and access the port 9641 and path /metrics\n#\n# For more info on the prometheus exporter and metrics\n# https://prometheus.io/docs/introduction/overview/\n# https://prometheus.io/docs/concepts/data_model/\n#\n#prometheus\n\n# TURN REST API flag.\n# (Time Limited Long Term Credential)\n# Flag that sets a special authorization option that is based upon authentication secret.\n#\n# This feature's purpose is to support \"TURN Server REST API\", see\n# \"TURN REST API\" link in the project's page\n# https://github.com/coturn/coturn/\n#\n# This option is used with timestamp:\n#\n# usercombo -> \"timestamp:userid\"\n# turn user -> usercombo\n# turn password -> base64(hmac(secret key, usercombo))\n#\n# This allows TURN credentials to be accounted for a specific user id.\n# If you don't have a suitable id, then the timestamp alone can be used.\n# This option is enabled by turning on secret-based authentication.\n# The actual value of the secret is defined either by the option static-auth-secret,\n# or can be found in the turn_secret table in the database (see below).\n#\n# Read more about it:\n#  - https://tools.ietf.org/html/draft-uberti-behave-turn-rest-00\n#  - https://www.ietf.org/proceedings/87/slides/slides-87-behave-10.pdf\n#\n# Be aware that use-auth-secret overrides some parts of lt-cred-mech.\n# The use-auth-secret feature depends internally on lt-cred-mech, so if you set\n# this option then it automatically enables lt-cred-mech internally\n# as if you had enabled both.\n#\n# Note that you can use only one auth mechanism at the same time! This is because,\n# both mechanisms conduct username and password validation in different ways.\n#\n# Use either lt-cred-mech or use-auth-secret in the conf\n# to avoid any confusion.\n#\n#use-auth-secret\n\n# 'Static' authentication secret value (a string) for TURN REST API only.\n# If not set, then the turn server\n# will try to use the 'dynamic' value in the turn_secret table\n# in the user database (if present). The database-stored  value can be changed on-the-fly\n# by a separate program, so this is why that mode is considered 'dynamic'.\n#\n#static-auth-secret=north\n\n# Server name used for\n# the oAuth authentication purposes.\n# The default value is the realm name.\n#\n#server-name=blackdow.carleon.gov\n\n# Flag that allows oAuth authentication.\n#\n#oauth\n\n# 'Static' user accounts for the long term credentials mechanism, only.\n# This option cannot be used with TURN REST API.\n# 'Static' user accounts are NOT dynamically checked by the turnserver process,\n# so they can NOT be changed while the turnserver is running.\n#\n#user=username1:key1\n#user=username2:key2\n# OR:\n#user=username1:password1\n#user=username2:password2\n#\n# Keys must be generated by turnadmin utility. The key value depends\n# on user name, realm, and password:\n#\n# Example:\n# $ turnadmin -k -u ninefingers -r north.gov -p youhavetoberealistic\n# Output: 0xbc807ee29df3c9ffa736523fb2c4e8ee\n# ('0x' in the beginning of the key is what differentiates the key from\n# password. If it has 0x then it is a key, otherwise it is a password).\n#\n# The corresponding user account entry in the config file will be:\n#\n#user=ninefingers:0xbc807ee29df3c9ffa736523fb2c4e8ee\n# Or, equivalently, with open clear password (less secure):\n#user=ninefingers:youhavetoberealistic\n#\n\n# SQLite database file name.\n#\n# The default file name is /var/db/turndb or /usr/local/var/db/turndb or\n# /var/lib/turn/turndb.\n#\n#userdb=/var/db/turndb\n\n# PostgreSQL database connection string in the case that you are using PostgreSQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# See http://www.postgresql.org/docs/8.4/static/libpq-connect.html for 8.x PostgreSQL\n# versions connection string format, see\n# http://www.postgresql.org/docs/9.2/static/libpq-connect.html#LIBPQ-CONNSTRING\n# for 9.x and newer connection string formats.\n#\n#psql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> connect_timeout=30\"\n\n# MySQL database connection string in the case that you are using MySQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n#\n# Optional connection string parameters for the secure communications (SSL):\n# ca, capath, cert, key, cipher\n# (see http://dev.mysql.com/doc/refman/5.1/en/ssl-options.html for the\n# command options description).\n#\n# Use the string format below (space separated parameters, all optional):\n#\n#mysql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> port=<port> connect_timeout=<seconds> read_timeout=<seconds>\"\n\n# If you want to use an encrypted password in the MySQL connection string,\n# then set the MySQL password encryption secret key file with this option.\n#\n# Warning: If this option is set, then the mysql password must be set in \"mysql-userdb\" in an encrypted format!\n# If you want to use a cleartext password then do not set this option!\n#\n# This is the file path for the aes encrypted secret key used for password encryption.\n#\n#secret-key-file=/path/\n\n# MongoDB database connection string in the case that you are using MongoDB\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format described at http://hergert.me/docs/mongo-c-driver/mongoc_uri.html\n#\n#mongo-userdb=\"mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\"\n\n# Redis database connection string in the case that you are using Redis\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-userdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Redis status and statistics database connection string, if used (default - empty, no Redis stats DB used).\n# This database keeps allocations status information, and it can be also used for publishing\n# and delivering traffic and allocation event notifications.\n# The connection string has the same parameters as redis-userdb connection string.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-statsdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# The default realm to be used for the users when no explicit\n# origin/realm relationship is found in the database, or if the TURN\n# server is not using any database (just the commands-line settings\n# and the userdb file). Must be used with long-term credentials\n# mechanism or with TURN REST API.\n#\n# Note: If the default realm is not specified, then realm falls back to the host domain name.\n#       If the domain name string is empty, or set to '(None)', then it is initialized as an empty string.\n#\n#realm=mycompany.org\n\n# This flag sets the origin consistency\n# check. Across the session, all requests must have the same\n# main ORIGIN attribute value (if the ORIGIN was\n# initially used by the session).\n#\n#check-origin-consistency\n\n# Per-user allocation quota.\n# default value is 0 (no quota, unlimited number of sessions per user).\n# This option can also be set through the database, for a particular realm.\n#\n#user-quota=0\n\n# Total allocation quota.\n# default value is 0 (no quota).\n# This option can also be set through the database, for a particular realm.\n#\n#total-quota=0\n\n# Max bytes-per-second bandwidth a TURN session is allowed to handle\n# (input and output network streams are treated separately). Anything above\n# that limit will be dropped or temporarily suppressed (within\n# the available buffer limits).\n# This option can also be set through the database, for a particular realm.\n#\n#max-bps=0\n\n#\n# Maximum server capacity.\n# Total bytes-per-second bandwidth the TURN server is allowed to allocate\n# for the sessions, combined (input and output network streams are treated separately).\n#\n#bps-capacity=0\n\n# Uncomment if no UDP client listener is desired.\n# By default UDP client listener is always started.\n#\n#no-udp\n\n# Uncomment if no TCP client listener is desired.\n# By default TCP client listener is always started.\n#\n#no-tcp\n\n# Uncomment if no TLS client listener is desired.\n# By default TLS client listener is always started.\n#\n#no-tls\n\n# Uncomment if no DTLS client listener is desired.\n# By default DTLS client listener is always started.\n#\n#no-dtls\n\n# Uncomment if no UDP relay endpoints are allowed.\n# By default UDP relay endpoints are enabled (like in RFC 5766).\n#\n#no-udp-relay\n\n# Uncomment if no TCP relay endpoints are allowed.\n# By default TCP relay endpoints are enabled (like in RFC 6062).\n#\n#no-tcp-relay\n\n# Uncomment if extra security is desired,\n# with nonce value having a limited lifetime.\n# The nonce value is unique for a session.\n# Set this option to limit the nonce lifetime.\n# Set it to 0 for unlimited lifetime.\n# It defaults to 600 secs (10 min) if no value is provided. After that delay,\n# the client will get 438 error and will have to re-authenticate itself.\n#\n#stale-nonce=600\n\n# Uncomment if you want to set the maximum allocation\n# time before it has to be refreshed.\n# Default is 3600s.\n#\n#max-allocate-lifetime=3600\n\n\n# Uncomment to set the lifetime for the channel.\n# Default value is 600 secs (10 minutes).\n# This value MUST not be changed for production purposes.\n#\n#channel-lifetime=600\n\n# Uncomment to set the permission lifetime.\n# Default to 300 secs (5 minutes).\n# In production this value MUST not be changed,\n# however it can be useful for test purposes.\n#\n#permission-lifetime=300\n\n# Certificate file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#cert=/usr/local/etc/turn_server_cert.pem\n\n# Private key file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#pkey=/usr/local/etc/turn_server_pkey.pem\n\n# Private key file password, if it is in encoded format.\n# This option has no default value.\n#\n#pkey-pwd=...\n\n# Allowed OpenSSL cipher list for TLS/DTLS connections.\n# Default value is \"DEFAULT\".\n#\n#cipher-list=\"DEFAULT\"\n\n# CA file in OpenSSL format.\n# Forces TURN server to verify the client SSL certificates.\n# By default this is not set: there is no default value and the client\n# certificate is not checked.\n#\n# Example:\n#CA-file=/etc/ssh/id_rsa.cert\n\n# Curve name for EC ciphers, if supported by OpenSSL\n# library (TLS and DTLS). The default value is prime256v1,\n# if pre-OpenSSL 1.0.2 is used. With OpenSSL 1.0.2+,\n# an optimal curve will be automatically calculated, if not defined\n# by this option.\n#\n#ec-curve-name=prime256v1\n\n# Use 566 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh566\n\n# Use 1066 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh1066\n\n# Use custom DH TLS key, stored in PEM format in the file.\n# Flags --dh566 and --dh1066 are ignored when the DH key is taken from a file.\n#\n#dh-file=<DH-PEM-file-name>\n\n# Flag to prevent stdout log messages.\n# By default, all log messages go to both stdout and to\n# the configured log file. With this option everything will\n# go to the configured log only (unless the log file itself is stdout).\n#\n#no-stdout-log\n\n# Option to set the log file name.\n# By default, the turnserver tries to open a log file in\n# /var/log, /var/tmp, /tmp and the current directory\n# (Whichever file open operation succeeds first will be used).\n# With this option you can set the definite log file name.\n# The special names are \"stdout\" and \"-\" - they will force everything\n# to the stdout. Also, the \"syslog\" name will force everything to\n# the system log (syslog).\n# In the runtime, the logfile can be reset with the SIGHUP signal\n# to the turnserver process.\n#\n#log-file=/var/tmp/turn.log\n\n# Option to redirect all log output into system log (syslog).\n#\n#syslog\n\n# Set syslog facility for syslog messages\n# Default values is ''.\n#\n#syslog-facility=\"LOG_LOCAL1\"\n\n# This flag means that no log file rollover will be used, and the log file\n# name will be constructed as-is, without PID and date appendage.\n# This option can be used, for example, together with the logrotate tool.\n#\n#simple-log\n\n# Enable full ISO-8601 timestamp in all logs.\n#new-log-timestamp\n\n# Set timestamp format (in strftime(1) format). Depends on new-log-timestamp to be enabled.\n#new-log-timestamp-format \"%FT%T%z\"\n\n# Disabled by default binding logging in verbose log mode to avoid DoS attacks.\n# Enable binding logging and UDP endpoint logs in verbose log mode.\n#log-binding\n\n# Option to set the \"redirection\" mode. The value of this option\n# will be the address of the alternate server for UDP & TCP service in the form of\n# <ip>[:<port>]. The server will send this value in the attribute\n# ALTERNATE-SERVER, with error 300, on ALLOCATE request, to the client.\n# Client will receive only values with the same address family\n# as the client network endpoint address family.\n# See RFC 5389 and RFC 5766 for the description of ALTERNATE-SERVER functionality.\n# The client must use the obtained value for subsequent TURN communications.\n# If more than one --alternate-server option is provided, then the functionality\n# can be more accurately described as \"load-balancing\" than a mere \"redirection\".\n# If the port number is omitted, then the default port\n# number 3478 for the UDP/TCP protocols will be used.\n# Colon (:) characters in IPv6 addresses may conflict with the syntax of\n# the option. To alleviate this conflict, literal IPv6 addresses are enclosed\n# in square brackets in such resource identifiers, for example:\n# [2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478 .\n# Multiple alternate servers can be set. They will be used in the\n# round-robin manner. All servers in the pool are considered of equal weight and\n# the load will be distributed equally. For example, if you have 4 alternate servers,\n# then each server will receive 25% of ALLOCATE requests. A alternate TURN server\n# address can be used more than one time with the alternate-server option, so this\n# can emulate \"weighting\" of the servers.\n#\n# Examples:\n#alternate-server=1.2.3.4:5678\n#alternate-server=11.22.33.44:56789\n#alternate-server=5.6.7.8\n#alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to set alternative server for TLS & DTLS services in form of\n# <ip>:<port>. If the port number is omitted, then the default port\n# number 5349 for the TLS/DTLS protocols will be used. See the previous\n# option for the functionality description.\n#\n# Examples:\n#tls-alternate-server=1.2.3.4:5678\n#tls-alternate-server=11.22.33.44:56789\n#tls-alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to suppress TURN functionality, only STUN requests will be processed.\n# Run as STUN server only, all TURN requests will be ignored.\n# By default, this option is NOT set.\n#\n#stun-only\n\n# Option to hide software version. Enhance security when used in production.\n# Revealing the specific software version of the agent through the\n# SOFTWARE attribute might allow them to become more vulnerable to\n# attacks against software that is known to contain security holes.\n# Implementers SHOULD make usage of the SOFTWARE attribute a\n# configurable option (https://tools.ietf.org/html/rfc5389#section-16.1.2)\n#\n#no-software-attribute\n\n# Option to suppress STUN functionality, only TURN requests will be processed.\n# Run as TURN server only, all STUN requests will be ignored.\n# By default, this option is NOT set.\n#\n#no-stun\n\n# This is the timestamp/username separator symbol (character) in TURN REST API.\n# The default value is ':'.\n#\n#rest-api-separator=:\n\n# Flag that can be used to allow peers on the loopback addresses (127.x.x.x and ::1).\n# This is an extra security measure.\n#\n# (To avoid any security issue that allowing loopback access may raise,\n# the no-loopback-peers option is replaced by allow-loopback-peers.)\n#\n# Allow it only for testing in a development environment!\n# In production it adds a possible security vulnerability, so for security reasons\n# it is not allowed using it together with empty cli-password.\n#\n#allow-loopback-peers\n\n# Flag that can be used to disallow peers on well-known broadcast addresses (224.0.0.0 and above, and FFXX:*).\n# This is an extra security measure.\n#\n#no-multicast-peers\n\n# Option to set the max time, in seconds, allowed for full allocation establishment.\n# Default is 60 seconds.\n#\n#max-allocate-timeout=60\n\n# Option to allow or ban specific ip addresses or ranges of ip addresses.\n# If an ip address is specified as both allowed and denied, then the ip address is\n# considered to be allowed. This is useful when you wish to ban a range of ip\n# addresses, except for a few specific ips within that range.\n#\n# This can be used when you do not want users of the turn server to be able to access\n# machines reachable by the turn server, but would otherwise be unreachable from the\n# internet (e.g. when the turn server is sitting behind a NAT)\n#\n# Examples:\n# denied-peer-ip=83.166.64.0-83.166.95.255\n# allowed-peer-ip=83.166.68.45\n\n# File name to store the pid of the process.\n# Default is /var/run/turnserver.pid (if superuser account is used) or\n# /var/tmp/turnserver.pid .\n#\n#pidfile=\"/var/run/turnserver.pid\"\n\n# Require authentication of the STUN Binding request.\n# By default, the clients are allowed anonymous access to the STUN Binding functionality.\n#\n#secure-stun\n\n# Mobility with ICE (MICE) specs support.\n#\n#mobility\n\n# Allocate Address Family according (DEPRECATED and will be removed in favor of allocation-default-address-family)\n# If enabled then TURN server allocates address family according  the TURN\n# Client <=> Server communication address family.\n# (By default Coturn works according RFC 6156.)\n# !!Warning: Enabling this option breaks RFC6156 section-4.2 (violates use default IPv4)!!\n#\n#keep-address-family\n\n# TURN server allocates address family according TURN client requested address family.\n# If address family not requested explicitly by the client, then it falls back to this default.\n# The standard RFC explicitly define that this default must be IPv4, \n# so use other option values with care! \n# Possible values: \"ipv4\" or \"ipv6\" or \"keep\" \n# \"keep\" sets the allocation default address family according to \n# the TURN client allocation request connection address family.\n#\n#allocation-default-address-family=\"ipv4\"\n#allocation-default-address-family=\"ipv4\"\n\n# User name to run the process. After the initialization, the turnserver process\n# will attempt to change the current user ID to that user.\n#\n#proc-user=<user-name>\n\n# Group name to run the process. After the initialization, the turnserver process\n# will attempt to change the current group ID to that group.\n#\n#proc-group=<group-name>\n\n# Turn OFF the CLI support.\n# By default it is always ON.\n# See also options cli-ip and cli-port.\n#\n#no-cli\n\n#Local system IP address to be used for CLI server endpoint. Default value\n# is 127.0.0.1.\n#\n#cli-ip=127.0.0.1\n\n# CLI server port. Default is 5766.\n#\n#cli-port=5766\n\n# CLI access password. Default is empty (no password).\n# For the security reasons, it is recommended that you use the encrypted\n# form of the password (see the -P command in the turnadmin utility).\n#\n# Secure form for password 'qwerty':\n#\n#cli-password=$5$79a316b350311570$81df9cfb9af7f5e5a76eada31e7097b663a0670f99a3c07ded3f1c8e59c5658a\n#\n# Or unsecure form for the same password:\n#\n#cli-password=qwerty\n\n# Enable Web-admin support on https. By default it is Disabled.\n# If it is enabled it also enables a http a simple static banner page\n# with a small reminder that the admin page is available only on https.\n# Not supported if no-tls option used\n#\n#web-admin\n\n# Local system IP address to be used for Web-admin server endpoint. Default value is 127.0.0.1.\n#\n#web-admin-ip=127.0.0.1\n\n# Web-admin server port. Default is 8080.\n#\n#web-admin-port=8080\n\n# Web-admin server listen on STUN/TURN worker threads\n# By default it is disabled for security reasons! (Not recommended in any production environment!)\n#\n#web-admin-listen-on-workers\n\n# Redirect ACME, i.e. HTTP GET requests matching '^/.well-known/acme-challenge/(.*)' to '<URL>$1'.\n# Default is '', i.e. no special handling for such requests.\n#\n#acme-redirect=http://redirectserver/.well-known/acme-challenge/\n\n# Server relay. NON-STANDARD AND DANGEROUS OPTION.\n# Only for those applications when you want to run\n# server applications on the relay endpoints.\n# This option eliminates the IP permissions check on\n# the packets incoming to the relay endpoints.\n#\n#server-relay\n\n# Maximum number of output sessions in ps CLI command.\n# This value can be changed on-the-fly in CLI. The default value is 256.\n#\n#cli-max-output-sessions\n\n# Set network engine type for the process (for internal purposes).\n#\n#ne=[1|2|3]\n\n# Do not allow an TLS/DTLS version of protocol\n#\n#no-tlsv1\n#no-tlsv1_1\n#no-tlsv1_2\n\n# Disable RFC5780 (NAT behavior discovery).\n#\n# Originally, if there are more than one listener address from the same\n# address family, then by default the NAT behavior discovery feature enabled.\n# This option disables the original behavior, because the NAT behavior\n# discovery adds extra attributes to response, and this increase the\n# possibility of an amplification attack.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-rfc5780\n\n# Disable handling old STUN Binding requests and disable MAPPED-ADDRESS\n# attribute in binding response (use only the XOR-MAPPED-ADDRESS).\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-stun-backward-compatibility\n\n# Only send RESPONSE-ORIGIN attribute in binding response if RFC5780 is enabled.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nresponse-origin-only-with-rfc5780"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
---
apiVersion: v1
data:
  primary.conf: |-
    dir /data
    # User-supplied primary configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of primary configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
  valkey.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://valkey.io/docs/topics/persistence.html
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-configuration
  namespace: gitea
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PASSWORD_FILE ]] && export VALKEY_PASSWORD="$(< "${VALKEY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h localhost \
        -p $VALKEY_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_primary.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_primary.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_primary.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PRIMARY_PASSWORD_FILE ]] && export VALKEY_PRIMARY_PASSWORD="$(< "${VALKEY_PRIMARY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PRIMARY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PRIMARY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h $VALKEY_PRIMARY_HOST \
        -p $VALKEY_PRIMARY_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PASSWORD_FILE ]] && export VALKEY_PASSWORD="$(< "${VALKEY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h localhost \
        -p $VALKEY_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_primary.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_primary.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_primary.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PRIMARY_PASSWORD_FILE ]] && export VALKEY_PRIMARY_PASSWORD="$(< "${VALKEY_PRIMARY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PRIMARY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PRIMARY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h $VALKEY_PRIMARY_HOST \
        -p $VALKEY_PRIMARY_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-health
  namespace: gitea
---
apiVersion: v1
data:
  start-primary.sh: |
    #!/bin/bash

    [[ -f $VALKEY_PASSWORD_FILE ]] && export VALKEY_PASSWORD="$(< "${VALKEY_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/valkey/mounted-etc/primary.conf ]];then
        cp /opt/bitnami/valkey/mounted-etc/primary.conf /opt/bitnami/valkey/etc/primary.conf
    fi
    if [[ -f /opt/bitnami/valkey/mounted-etc/valkey.conf ]];then
        cp /opt/bitnami/valkey/mounted-etc/valkey.conf /opt/bitnami/valkey/etc/valkey.conf
    fi
    ARGS=("--port" "${VALKEY_PORT}")
    ARGS+=("--requirepass" "${VALKEY_PASSWORD}")
    ARGS+=("--primaryauth" "${VALKEY_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/valkey/etc/valkey.conf")
    ARGS+=("--include" "/opt/bitnami/valkey/etc/primary.conf")
    exec valkey-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-scripts
  namespace: gitea
---
apiVersion: v1
data:
  config.yaml: |-
    customApps:
    - apps:
      - icon: https://brands.home-assistant.io/homeassistant/icon.png
        name: Home Assistant
        url: https://homeassistant.homelab.olav.ninja
      group: Applications
    darkTheme: tron
    defaultEnable: false
    globalBookmarks:
    - bookmarks:
      - name: Hubble
        url: https://hubble.homelab.olav.ninja
      - name: Keycloak
        url: https://keycloak.homelab.olav.ninja
      - name: Longhorn
        url: https://longhorn.homelab.olav.ninja
      - name: Netbird
        url: https://netbird.homelab.olav.ninja
      group: Utilities
    - bookmarks:
      - name: OpenWrt
        url: http://192.168.0.1
      group: Infrastructure
    - bookmarks:
      - name: Cloudflare Dashboard
        url: https://dash.cloudflare.com
      - name: Github Repo
        url: https://github.com/olav-st/homelab
      group: External
    instanceName: null
    lightTheme: gazette
    name: Olav
    namespaceSelector:
      any: true
      matchNames:
      - media
    title: Homelab
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari-settings
  namespace: hajimari
---
apiVersion: v1
data:
  immich-config.yaml: |
    placeholder: foo
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: immich
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-immich-config
  namespace: immich
---
apiVersion: v1
data:
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
  users.acl: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-configuration
  namespace: immich
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-health
  namespace: immich
---
apiVersion: v1
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/users.acl ]];then
        cp /opt/bitnami/redis/mounted-etc/users.acl /opt/bitnami/redis/etc/users.acl
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-scripts
  namespace: immich
---
apiVersion: v1
data:
  agent-not-ready-taint-key: node.cilium.io/agent-not-ready
  auto-direct-node-routes: "false"
  bpf-distributed-lru: "false"
  bpf-events-drop-enabled: "true"
  bpf-events-policy-verdict-enabled: "true"
  bpf-events-trace-enabled: "true"
  bpf-lb-acceleration: disabled
  bpf-lb-algorithm-annotation: "false"
  bpf-lb-external-clusterip: "false"
  bpf-lb-map-max: "65536"
  bpf-lb-mode-annotation: "false"
  bpf-lb-sock: "false"
  bpf-lb-source-range-all-types: "false"
  bpf-map-dynamic-size-ratio: "0.0025"
  bpf-policy-map-max: "16384"
  bpf-policy-stats-map-max: "65536"
  bpf-root: /sys/fs/bpf
  cgroup-root: /sys/fs/cgroup
  cilium-endpoint-gc-interval: 5m0s
  cluster-id: "0"
  cluster-name: default
  clustermesh-enable-endpoint-sync: "false"
  clustermesh-enable-mcs-api: "false"
  cni-exclusive: "false"
  cni-log-file: /var/run/cilium/cilium-cni.log
  custom-cni-conf: "false"
  datapath-mode: veth
  debug: "false"
  debug-verbose: ""
  default-lb-service-ipam: lbipam
  devices: br0
  direct-routing-skip-unreachable: "false"
  dnsproxy-enable-transparent-mode: "true"
  dnsproxy-socket-linger-timeout: "10"
  egress-gateway-reconciliation-trigger-interval: 1s
  enable-auto-protect-node-port-range: "true"
  enable-bpf-clock-probe: "false"
  enable-endpoint-health-checking: "true"
  enable-endpoint-lockdown-on-policy-overflow: "false"
  enable-health-check-loadbalancer-ip: "false"
  enable-health-check-nodeport: "true"
  enable-health-checking: "true"
  enable-hubble: "true"
  enable-internal-traffic-policy: "true"
  enable-ipv4: "true"
  enable-ipv4-big-tcp: "false"
  enable-ipv4-masquerade: "true"
  enable-ipv6: "false"
  enable-ipv6-big-tcp: "false"
  enable-ipv6-masquerade: "true"
  enable-k8s-networkpolicy: "true"
  enable-l2-announcements: "true"
  enable-l2-neigh-discovery: "false"
  enable-l7-proxy: "true"
  enable-lb-ipam: "true"
  enable-masquerade-to-route-source: "false"
  enable-metrics: "true"
  enable-node-selector-labels: "false"
  enable-non-default-deny-policies: "true"
  enable-policy: default
  enable-policy-secrets-sync: "true"
  enable-sctp: "false"
  enable-source-ip-verification: "true"
  enable-svc-source-range-check: "true"
  enable-tcx: "true"
  enable-vtep: "false"
  enable-well-known-identities: "false"
  enable-xt-socket-fallback: "true"
  envoy-access-log-buffer-size: "4096"
  envoy-base-id: "0"
  envoy-keep-cap-netbindservice: "false"
  external-envoy-proxy: "true"
  health-check-icmp-failure-threshold: "3"
  http-retry-count: "3"
  hubble-disable-tls: "false"
  hubble-listen-address: :4244
  hubble-network-policy-correlation-enabled: "true"
  hubble-socket-path: /var/run/cilium/hubble.sock
  hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt
  hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt
  hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key
  identity-allocation-mode: crd
  identity-gc-interval: 15m0s
  identity-heartbeat-timeout: 30m0s
  identity-management-mode: agent
  install-no-conntrack-iptables-rules: "false"
  ipam: kubernetes
  ipam-cilium-node-update-rate: 15s
  iptables-random-fully: "false"
  k8s-client-burst: "100"
  k8s-client-qps: "50"
  k8s-require-ipv4-pod-cidr: "false"
  k8s-require-ipv6-pod-cidr: "false"
  kube-proxy-replacement: "true"
  kube-proxy-replacement-healthz-bind-address: ""
  max-connected-clusters: "255"
  mesh-auth-enabled: "true"
  mesh-auth-gc-interval: 5m0s
  mesh-auth-queue-size: "1024"
  mesh-auth-rotated-identities-queue-size: "1024"
  metrics-sampling-interval: 5m
  monitor-aggregation: medium
  monitor-aggregation-flags: all
  monitor-aggregation-interval: 5s
  nat-map-stats-entries: "32"
  nat-map-stats-interval: 30s
  node-port-bind-protection: "true"
  nodeport-addresses: ""
  nodes-gc-interval: 5m0s
  operator-api-serve-addr: 127.0.0.1:9234
  operator-prometheus-serve-addr: :9963
  policy-cidr-match-mode: ""
  policy-default-local-cluster: "false"
  policy-secrets-namespace: cilium-secrets
  policy-secrets-only-from-secrets-namespace: "true"
  preallocate-bpf-maps: "false"
  procfs: /host/proc
  proxy-connect-timeout: "2"
  proxy-idle-timeout-seconds: "60"
  proxy-initial-fetch-timeout: "30"
  proxy-max-concurrent-retries: "128"
  proxy-max-connection-duration-seconds: "0"
  proxy-max-requests-per-connection: "0"
  proxy-xff-num-trusted-hops-egress: "0"
  proxy-xff-num-trusted-hops-ingress: "0"
  remove-cilium-node-taints: "true"
  routing-mode: tunnel
  service-no-backend-response: reject
  set-cilium-is-up-condition: "true"
  set-cilium-node-taints: "true"
  synchronize-k8s-nodes: "true"
  tofqdns-dns-reject-response-code: refused
  tofqdns-enable-dns-compression: "true"
  tofqdns-endpoint-max-ip-per-hostname: "1000"
  tofqdns-idle-connection-grace-period: 0s
  tofqdns-max-deferred-connection-deletes: "10000"
  tofqdns-preallocate-identities: "true"
  tofqdns-proxy-response-max-delay: 100ms
  tunnel-protocol: vxlan
  tunnel-source-port-range: 0-0
  unmanaged-pod-watcher-interval: "15"
  vtep-cidr: ""
  vtep-endpoint: ""
  vtep-mac: ""
  vtep-mask: ""
  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
---
apiVersion: v1
data:
  bootstrap-config.json: |
    {"admin":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/admin.sock"}}},"applicationLogConfig":{"logFormat":{"textFormat":"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v"}},"bootstrapExtensions":[{"name":"envoy.bootstrap.internal_listener","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener"}}],"dynamicResources":{"cdsConfig":{"apiConfigSource":{"apiType":"GRPC","grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}],"setNodeOnFirstMessageOnly":true,"transportApiVersion":"V3"},"initialFetchTimeout":"30s","resourceApiVersion":"V3"},"ldsConfig":{"apiConfigSource":{"apiType":"GRPC","grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}],"setNodeOnFirstMessageOnly":true,"transportApiVersion":"V3"},"initialFetchTimeout":"30s","resourceApiVersion":"V3"}},"node":{"cluster":"ingress-cluster","id":"host~127.0.0.1~no-id~localdomain"},"overloadManager":{"resourceMonitors":[{"name":"envoy.resource_monitors.global_downstream_max_connections","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig","max_active_downstream_connections":"50000"}}]},"staticResources":{"clusters":[{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"ingress-cluster","type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"egress-cluster-tls","transportSocket":{"name":"cilium.tls_wrapper","typedConfig":{"@type":"type.googleapis.com/cilium.UpstreamTlsWrapperContext"}},"type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"upstreamHttpProtocolOptions":{},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"egress-cluster","type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"ingress-cluster-tls","transportSocket":{"name":"cilium.tls_wrapper","typedConfig":{"@type":"type.googleapis.com/cilium.UpstreamTlsWrapperContext"}},"type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"upstreamHttpProtocolOptions":{},"useDownstreamProtocolConfig":{}}}},{"connectTimeout":"2s","loadAssignment":{"clusterName":"xds-grpc-cilium","endpoints":[{"lbEndpoints":[{"endpoint":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/xds.sock"}}}}]}]},"name":"xds-grpc-cilium","type":"STATIC","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","explicitHttpConfig":{"http2ProtocolOptions":{}}}}},{"connectTimeout":"2s","loadAssignment":{"clusterName":"/envoy-admin","endpoints":[{"lbEndpoints":[{"endpoint":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/admin.sock"}}}}]}]},"name":"/envoy-admin","type":"STATIC"}],"listeners":[{"address":{"socketAddress":{"address":"0.0.0.0","portValue":9964}},"filterChains":[{"filters":[{"name":"envoy.filters.network.http_connection_manager","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager","httpFilters":[{"name":"envoy.filters.http.router","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}],"internalAddressConfig":{"cidrRanges":[{"addressPrefix":"10.0.0.0","prefixLen":8},{"addressPrefix":"172.16.0.0","prefixLen":12},{"addressPrefix":"192.168.0.0","prefixLen":16},{"addressPrefix":"127.0.0.1","prefixLen":32}]},"routeConfig":{"virtualHosts":[{"domains":["*"],"name":"prometheus_metrics_route","routes":[{"match":{"prefix":"/metrics"},"name":"prometheus_metrics_route","route":{"cluster":"/envoy-admin","prefixRewrite":"/stats/prometheus"}}]}]},"statPrefix":"envoy-prometheus-metrics-listener","streamIdleTimeout":"300s"}}]}],"name":"envoy-prometheus-metrics-listener"},{"address":{"socketAddress":{"address":"127.0.0.1","portValue":9878}},"filterChains":[{"filters":[{"name":"envoy.filters.network.http_connection_manager","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager","httpFilters":[{"name":"envoy.filters.http.router","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}],"internalAddressConfig":{"cidrRanges":[{"addressPrefix":"10.0.0.0","prefixLen":8},{"addressPrefix":"172.16.0.0","prefixLen":12},{"addressPrefix":"192.168.0.0","prefixLen":16},{"addressPrefix":"127.0.0.1","prefixLen":32}]},"routeConfig":{"virtual_hosts":[{"domains":["*"],"name":"health","routes":[{"match":{"prefix":"/healthz"},"name":"health","route":{"cluster":"/envoy-admin","prefixRewrite":"/ready"}}]}]},"statPrefix":"envoy-health-listener","streamIdleTimeout":"300s"}}]}],"name":"envoy-health-listener"}]}}
kind: ConfigMap
metadata:
  name: cilium-envoy-config
  namespace: kube-system
---
apiVersion: v1
data:
  config.yaml: "cluster-name: default\npeer-service: \"hubble-peer.kube-system.svc.cluster.local.:443\"\nlisten-address: :4245\ngops: true\ngops-port: \"9893\"\nretry-timeout: \nsort-buffer-len-max: \nsort-buffer-drain-timeout: \ntls-hubble-client-cert-file: /var/lib/hubble-relay/tls/client.crt\ntls-hubble-client-key-file: /var/lib/hubble-relay/tls/client.key\ntls-hubble-server-ca-files: /var/lib/hubble-relay/tls/hubble-server-ca.crt\n\ndisable-server-tls: true\n"
kind: ConfigMap
metadata:
  name: hubble-relay-config
  namespace: kube-system
---
apiVersion: v1
data:
  nginx.conf: |-
    server {
        listen       8081;
        listen       [::]:8081;
        server_name  localhost;
        root /app;
        index index.html;
        client_max_body_size 1G;

        location / {
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;

            location /api {
                proxy_http_version 1.1;
                proxy_pass_request_headers on;
                proxy_pass http://127.0.0.1:8090;
            }
            location / {
                if ($http_user_agent ~* "kube-probe") { access_log off; }
                # double `/index.html` is required here
                try_files $uri $uri/ /index.html /index.html;
            }

            # Liveness probe
            location /healthz {
                access_log off;
                add_header Content-Type text/plain;
                return 200 'ok';
            }
        }
    }
kind: ConfigMap
metadata:
  name: hubble-ui-nginx
  namespace: kube-system
---
apiVersion: v1
data:
  daemon-config.json: |
    {
        "chrootDir": "/hostroot",
        "cniVersion": "0.3.1",
        "logLevel": "verbose",
        "logToStderr": true,
        "cniConfigDir": "/host/etc/cni/net.d",
        "multusAutoconfigDir": "/host/etc/cni/net.d",
        "multusConfigFile": "auto",
        "socketDir": "/host/run/multus/"
    }
kind: ConfigMap
metadata:
  labels:
    app: multus
    tier: node
  name: multus-daemon-config
  namespace: kube-system
---
apiVersion: v1
data:
  default-resource.yaml: |-
    backup-target: cifs://100.124.87.95/backup-longhorn
    backup-target-credential-secret: longhorn-truenas-cifs-credentials
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-default-resource
  namespace: longhorn-system
---
apiVersion: v1
data:
  default-setting.yaml: |-
    default-data-path: "/var/lib/longhorn"
    default-replica-count: "{\"v1\":\"1\",\"v2\":\"1\"}"
    default-data-locality: "best-effort"
    priority-class: "longhorn-critical"
    node-down-pod-deletion-policy: "delete-both-statefulset-and-deployment-pod"
    disable-revision-counter: "{\"v1\":\"true\"}"
    backup-compression-method: "lz4"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-default-setting
  namespace: longhorn-system
---
apiVersion: v1
data:
  storageclass.yaml: |
    kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: longhorn
      annotations:
        storageclass.kubernetes.io/is-default-class: "true"
    provisioner: driver.longhorn.io
    allowVolumeExpansion: true
    reclaimPolicy: "Delete"
    volumeBindingMode: Immediate
    parameters:
      numberOfReplicas: "1"
      staleReplicaTimeout: "30"
      fromBackup: ""
      fsType: "ext4"
      dataLocality: "best-effort"
      unmapMarkSnapChainRemoved: "ignored"
      disableRevisionCounter: "true"
      dataEngine: "v1"
      backupTargetName: "default"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-storageclass
  namespace: longhorn-system
---
apiVersion: v1
data:
  management.tmpl.json: |-
    {
        "Stuns": [
            {
                "Proto": "udp",
                "URI": "${NETBIRD_STUN_URI}",
                "Username": "",
                "Password": null
            }
        ],
        "TURNConfig": {
            "Turns": [
                {
                    "Proto": "udp",
                    "URI": "${NETBIRD_TURN_URI}",
                    "Username": "${NETBIRD_TURN_USER}",
                    "Password": "${NETBIRD_TURN_PASSWORD}"
                }
            ],
            "CredentialsTTL": "12h",
            "Secret": "secret",
            "TimeBasedCredentials": false
        },
        "Signal": {
            "Proto": "${NETBIRD_SIGNAL_PROTOCOL}",
            "URI": "${NETBIRD_SIGNAL_URI}",
            "Username": "",
            "Password": null
        },
        "Datadir": "",
        "HttpConfig": {
            "Address": "0.0.0.0:80",
            "AuthAudience": "${NETBIRD_AUTH_AUDIENCE}",
            "AuthUserIDClaim": "${NETBIRD_AUTH_USER_ID_CLAIM:-sub}",
            "CertFile": "${NETBIRD_MGMT_API_CERT_FILE}",
            "CertKey": "${NETBIRD_MGMT_API_CERT_KEY_FILE}",
            "IdpSignKeyRefreshEnabled": true,
            "OIDCConfigEndpoint": "${NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT}"
        },
        "IdpManagerConfig": {
            "ManagerType": "${NETBIRD_IDP_MANAGER_TYPE}",
            "${NETBIRD_IDP_MANAGER_TYPE^}ClientCredentials": {
                "ClientID": "${NETBIRD_IDP_CLIENT_ID}",
                "ClientSecret": "${NETBIRD_IDP_CLIENT_SECRET}",
                "GrantType": "${NETBIRD_IDP_GRANT_TYPE}",
                "Audience": "${NETBIRD_IDP_AUTH0_AUDIENCE}",
                "AuthIssuer": "${NETBIRD_IDP_AUTH0_AUTH_ISSUER}",
                "AdminEndpoint": "${NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT}",
                "TokenEndpoint": "${NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT}"
            }
        },
        "DeviceAuthorizationFlow": {
            "Provider": "${NETBIRD_AUTH_DEVICE_AUTH_PROVIDER}",
            "ProviderConfig": {
                "Audience": "${NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE}",
                "ClientID": "${NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID}",
                "DeviceAuthEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT}",
                "Domain": "${NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY}",
                "TokenEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT}",
                "Scope": "${NETBIRD_AUTH_DEVICE_AUTH_SCOPE}",
                "UseIDToken": ${NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN:-false}
            }
        },
        "Relay": {
            "Addresses": ["${NB_EXPOSED_ADDRESS}"],
            "CredentialsTTL": "24h",
            "Secret": "${NB_AUTH_SECRET}"
        }
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
data:
  .htaccess: |-
    # line below if for Apache 2.4
    <ifModule mod_authz_core.c>
    Require all denied
    </ifModule>
    # line below if for Apache 2.2
    <ifModule !mod_authz_core.c>
    deny from all
    </ifModule>
    # section for Apache 2.2 and 2.4
    <ifModule mod_autoindex.c>
    IndexIgnore *
    </ifModule>
  apache-pretty-urls.config.php: |-
    <?php
    $CONFIG = array (
      'htaccess.RewriteBase' => '/',
    );
  apcu.config.php: |-
    <?php
    $CONFIG = array (
      'memcache.local' => '\OC\Memcache\APCu',
    );
  apps.config.php: |-
    <?php
    $CONFIG = array (
      'apps_paths' => array (
          0 => array (
                  'path'     => OC::$SERVERROOT.'/apps',
                  'url'      => '/apps',
                  'writable' => false,
          ),
          1 => array (
                  'path'     => OC::$SERVERROOT.'/custom_apps',
                  'url'      => '/custom_apps',
                  'writable' => true,
          ),
      ),
    );
  autoconfig.php: |-
    <?php
    $autoconfig_enabled = false;
    if (getenv('SQLITE_DATABASE')) {
        $AUTOCONFIG["dbtype"] = "sqlite";
        $AUTOCONFIG["dbname"] = getenv('SQLITE_DATABASE');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE_FILE') && getenv('MYSQL_USER_FILE') && getenv('MYSQL_PASSWORD_FILE') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG['dbtype'] = 'mysql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('MYSQL_DATABASE_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('MYSQL_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('MYSQL_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE') && getenv('MYSQL_USER') && getenv('MYSQL_PASSWORD') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG["dbtype"] = "mysql";
        $AUTOCONFIG["dbname"] = getenv('MYSQL_DATABASE');
        $AUTOCONFIG["dbuser"] = getenv('MYSQL_USER');
        $AUTOCONFIG["dbpass"] = getenv('MYSQL_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB_FILE') && getenv('POSTGRES_USER_FILE') && getenv('POSTGRES_PASSWORD_FILE') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG['dbtype'] = 'pgsql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('POSTGRES_DB_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('POSTGRES_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('POSTGRES_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB') && getenv('POSTGRES_USER') && getenv('POSTGRES_PASSWORD') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG["dbtype"] = "pgsql";
        $AUTOCONFIG["dbname"] = getenv('POSTGRES_DB');
        $AUTOCONFIG["dbuser"] = getenv('POSTGRES_USER');
        $AUTOCONFIG["dbpass"] = getenv('POSTGRES_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    }
    if ($autoconfig_enabled) {
        $AUTOCONFIG["directory"] = getenv('NEXTCLOUD_DATA_DIR') ?: "/var/www/html/data";
    }
  mycustom.config.php: |-
    <?php
    $CONFIG = array(
      'allow_local_remote_servers' => true,
      'trusted_proxies' => array('10.0.0.0/8'),
      'default_phone_region' => 'NO',
      'maintenance_window_start' => 1,
      );
  redis.config.php: |-
    <?php
    if (getenv('REDIS_HOST')) {
      $CONFIG = array(
        'memcache.distributed' => '\OC\Memcache\Redis',
        'memcache.locking' => '\OC\Memcache\Redis',
        'redis' => array(
          'host' => getenv('REDIS_HOST'),
          'password' => getenv('REDIS_HOST_PASSWORD_FILE') ? trim(file_get_contents(getenv('REDIS_HOST_PASSWORD_FILE'))) : (string) getenv('REDIS_HOST_PASSWORD'),
        ),
      );

      if (getenv('REDIS_HOST_PORT') !== false) {
        $CONFIG['redis']['port'] = (int) getenv('REDIS_HOST_PORT');
      } elseif (getenv('REDIS_HOST')[0] != '/') {
        $CONFIG['redis']['port'] = 6379;
      }
    }
  reverse-proxy.config.php: |-
    <?php
    $overwriteHost = getenv('OVERWRITEHOST');
    if ($overwriteHost) {
      $CONFIG['overwritehost'] = $overwriteHost;
    }

    $overwriteProtocol = getenv('OVERWRITEPROTOCOL');
    if ($overwriteProtocol) {
      $CONFIG['overwriteprotocol'] = $overwriteProtocol;
    }

    $overwriteCliUrl = getenv('OVERWRITECLIURL');
    if ($overwriteCliUrl) {
      $CONFIG['overwrite.cli.url'] = $overwriteCliUrl;
    }

    $overwriteWebRoot = getenv('OVERWRITEWEBROOT');
    if ($overwriteWebRoot) {
      $CONFIG['overwritewebroot'] = $overwriteWebRoot;
    }

    $overwriteCondAddr = getenv('OVERWRITECONDADDR');
    if ($overwriteCondAddr) {
      $CONFIG['overwritecondaddr'] = $overwriteCondAddr;
    }

    $trustedProxies = getenv('TRUSTED_PROXIES');
    if ($trustedProxies) {
      $CONFIG['trusted_proxies'] = array_filter(array_map('trim', explode(' ', $trustedProxies)));
    }

    $forwardedForHeaders = getenv('FORWARDED_FOR_HEADERS');
    if ($forwardedForHeaders) {
      $CONFIG['forwarded_for_headers'] = array_filter(array_map('trim', explode(' ', $forwardedForHeaders)));
    }
  s3.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_S3_BUCKET')) {
      $use_ssl = getenv('OBJECTSTORE_S3_SSL');
      $use_path = getenv('OBJECTSTORE_S3_USEPATH_STYLE');
      $use_legacyauth = getenv('OBJECTSTORE_S3_LEGACYAUTH');
      $autocreate = getenv('OBJECTSTORE_S3_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => array(
          'class' => '\OC\Files\ObjectStore\S3',
          'arguments' => array(
            'bucket' => getenv('OBJECTSTORE_S3_BUCKET'),
            'region' => getenv('OBJECTSTORE_S3_REGION') ?: '',
            'hostname' => getenv('OBJECTSTORE_S3_HOST') ?: '',
            'port' => getenv('OBJECTSTORE_S3_PORT') ?: '',
            'storageClass' => getenv('OBJECTSTORE_S3_STORAGE_CLASS') ?: '',
            'objectPrefix' => getenv("OBJECTSTORE_S3_OBJECT_PREFIX") ? getenv("OBJECTSTORE_S3_OBJECT_PREFIX") : "urn:oid:",
            'autocreate' => strtolower($autocreate) !== 'false',
            'use_ssl' => strtolower($use_ssl) !== 'false',
            // required for some non Amazon S3 implementations
            'use_path_style' => $use_path == true && strtolower($use_path) !== 'false',
            // required for older protocol versions
            'legacy_auth' => $use_legacyauth == true && strtolower($use_legacyauth) !== 'false'
          )
        )
      );

      if (getenv('OBJECTSTORE_S3_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_KEY')) {
        $CONFIG['objectstore']['arguments']['key'] = getenv('OBJECTSTORE_S3_KEY');
      } else {
        $CONFIG['objectstore']['arguments']['key'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SECRET_FILE')) {
        $CONFIG['objectstore']['arguments']['secret'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SECRET_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SECRET')) {
        $CONFIG['objectstore']['arguments']['secret'] = getenv('OBJECTSTORE_S3_SECRET');
      } else {
        $CONFIG['objectstore']['arguments']['secret'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SSE_C_KEY')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = getenv('OBJECTSTORE_S3_SSE_C_KEY');
      }
    }
  smtp.config.php: |-
    <?php
    if (getenv('SMTP_HOST') && getenv('MAIL_FROM_ADDRESS') && getenv('MAIL_DOMAIN')) {
      $CONFIG = array (
        'mail_smtpmode' => 'smtp',
        'mail_smtphost' => getenv('SMTP_HOST'),
        'mail_smtpport' => getenv('SMTP_PORT') ?: (getenv('SMTP_SECURE') ? 465 : 25),
        'mail_smtpsecure' => getenv('SMTP_SECURE') ?: '',
        'mail_smtpauth' => getenv('SMTP_NAME') && (getenv('SMTP_PASSWORD') || getenv('SMTP_PASSWORD_FILE')),
        'mail_smtpauthtype' => getenv('SMTP_AUTHTYPE') ?: 'LOGIN',
        'mail_smtpname' => getenv('SMTP_NAME') ?: '',
        'mail_from_address' => getenv('MAIL_FROM_ADDRESS'),
        'mail_domain' => getenv('MAIL_DOMAIN'),
      );

      if (getenv('SMTP_PASSWORD_FILE')) {
          $CONFIG['mail_smtppassword'] = trim(file_get_contents(getenv('SMTP_PASSWORD_FILE')));
      } elseif (getenv('SMTP_PASSWORD')) {
          $CONFIG['mail_smtppassword'] = getenv('SMTP_PASSWORD');
      } else {
          $CONFIG['mail_smtppassword'] = '';
      }
    }
  swift.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_SWIFT_URL')) {
        $autocreate = getenv('OBJECTSTORE_SWIFT_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => [
          'class' => 'OC\\Files\\ObjectStore\\Swift',
          'arguments' => [
            'autocreate' => $autocreate == true && strtolower($autocreate) !== 'false',
            'user' => [
              'name' => getenv('OBJECTSTORE_SWIFT_USER_NAME'),
              'password' => getenv('OBJECTSTORE_SWIFT_USER_PASSWORD'),
              'domain' => [
                'name' => (getenv('OBJECTSTORE_SWIFT_USER_DOMAIN')) ?: 'Default',
              ],
            ],
            'scope' => [
              'project' => [
                'name' => getenv('OBJECTSTORE_SWIFT_PROJECT_NAME'),
                'domain' => [
                  'name' => (getenv('OBJECTSTORE_SWIFT_PROJECT_DOMAIN')) ?: 'Default',
                ],
              ],
            ],
            'serviceName' => (getenv('OBJECTSTORE_SWIFT_SERVICE_NAME')) ?: 'swift',
            'region' => getenv('OBJECTSTORE_SWIFT_REGION'),
            'url' => getenv('OBJECTSTORE_SWIFT_URL'),
            'bucket' => getenv('OBJECTSTORE_SWIFT_CONTAINER_NAME'),
          ]
        ]
      );
    }
  upgrade-disable-web.config.php: |-
    <?php
    $CONFIG = array (
      'upgrade.disable-web' => true,
    );
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 32.0.0
    helm.sh/chart: nextcloud-8.3.0
  name: nextcloud-config
  namespace: nextcloud
---
apiVersion: v1
data:
  uploadLimit.ini: |-
    upload_max_filesize = 1G
    post_max_size = 1G
    max_input_time = 5400
    max_execution_time = 5400
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 32.0.0
    helm.sh/chart: nextcloud-8.3.0
  name: nextcloud-phpconfig
  namespace: nextcloud
---
apiVersion: v1
data:
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
  users.acl: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-configuration
  namespace: nextcloud
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-health
  namespace: nextcloud
---
apiVersion: v1
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/users.acl ]];then
        cp /opt/bitnami/redis/mounted-etc/users.acl /opt/bitnami/redis/etc/users.acl
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-scripts
  namespace: nextcloud
---
apiVersion: v1
data:
  config.yaml: |
    general:
      appName: Pingvin Share
      appUrl: https://pingvin.homelab.olav.ninja
      secureCookies: "true"
      showHomePage: "false"
      sessionDuration: 1 week

    share:
      allowRegistration: "false"
      allowUnauthenticatedShares: "true"
      maxExpiration: 180 days
      shareIdLength: "8"
      maxSize: "10000000000"
      zipCompressionLevel: "9"
      chunkSize: "10000000"
      autoOpenShareModal: "false"

    cache:
      redis-enabled: "false"

    email:
      enableShareEmailRecipients: "false"

    smtp:
      enabled: "false"

    ldap:
      enabled: "false"

    oauth:
      allowRegistration: "true"
      ignoreTotp: "true"
      disablePassword: "true"
      oidc-enabled: "true"
      oidc-discoveryUri: "https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration"
      oidc-signOut: "true"
      oidc-scope: openid email profile
      oidc-usernameClaim: "preferred_username"
      oidc-clientId: "pingvinshare"
      oidc-clientSecret: "${OIDC_CLIENT_SECRET}"

    s3:
      enabled: "false"

    legal:
      enabled: "false"

    initUser:
      enabled: false
kind: ConfigMap
metadata:
  name: pingvinshare-config
  namespace: pingvinshare
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-root-ca
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-client
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-server
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea
  namespace: gitea
stringData:
  assertions: ""
  config_environment.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    function env2ini::log() {
      printf "${1}\n"
    }

    function env2ini::read_config_to_env() {
      local section="${1}"
      local line="${2}"

      if [[ -z "${line}" ]]; then
        # skip empty line
        return
      fi

      # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line
      local setting="$(awk -F '=' '{print $1}' <<< "${line}" | xargs echo -n)"

      if [[ -z "${setting}" ]]; then
        env2ini::log '  ! invalid setting'
        exit 1
      fi

      local value=''
      local regex="^${setting}(\s*)=(\s*)(.*)"
      if [[ $line =~ $regex ]]; then
        value="${BASH_REMATCH[3]}"
      else
        env2ini::log '  ! invalid setting'
        exit 1
      fi

      env2ini::log "    + '${setting}'"

      if [[ -z "${section}" ]]; then
        export "GITEA____${setting^^}=${value}"                           # '^^' makes the variable content uppercase
        return
      fi

      local masked_section="${section//./_0X2E_}"                            # '//' instructs to replace all matches
      masked_section="${masked_section//-/_0X2D_}"

      export "GITEA__${masked_section^^}__${setting^^}=${value}"        # '^^' makes the variable content uppercase
    }

    function env2ini::reload_preset_envs() {
      env2ini::log "Reloading preset envs..."

      while read -r line; do
        if [[ -z "${line}" ]]; then
          # skip empty line
          return
        fi

        # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line
        local setting="$(awk -F '=' '{print $1}' <<< "${line}" | xargs echo -n)"

        if [[ -z "${setting}" ]]; then
          env2ini::log '  ! invalid setting'
          exit 1
        fi

        local value=''
        local regex="^${setting}(\s*)=(\s*)(.*)"
        if [[ $line =~ $regex ]]; then
          value="${BASH_REMATCH[3]}"
        else
          env2ini::log '  ! invalid setting'
          exit 1
        fi

        env2ini::log "  + '${setting}'"

        export "${setting^^}=${value}"                           # '^^' makes the variable content uppercase
      done < "$TMP_EXISTING_ENVS_FILE"

      rm $TMP_EXISTING_ENVS_FILE
    }


    function env2ini::process_config_file() {
      local config_file="${1}"
      local section="$(basename "${config_file}")"

      if [[ $section == '_generals_' ]]; then
        env2ini::log "  [ini root]"
        section=''
      else
        env2ini::log "  ${section}"
      fi

      while read -r line; do
        env2ini::read_config_to_env "${section}" "${line}"
      done < <(awk 1 "${config_file}")                             # Helm .toYaml trims the trailing new line which breaks line processing; awk 1 ... adds it back while reading
    }

    function env2ini::load_config_sources() {
      local path="${1}"

      if [[ -d "${path}" ]]; then
        env2ini::log "Processing $(basename "${path}")..."

        while read -d '' configFile; do
          env2ini::process_config_file "${configFile}"
        done < <(find "${path}" -type l -not -name '..data' -print0)

        env2ini::log "\n"
      fi
    }

    function env2ini::generate_initial_secrets() {
      # These environment variables will either be
      #   - overwritten with user defined values,
      #   - initially used to set up Gitea
      # Anyway, they won't harm existing app.ini files

      export GITEA__SECURITY__INTERNAL_TOKEN=$(gitea generate secret INTERNAL_TOKEN)
      export GITEA__SECURITY__SECRET_KEY=$(gitea generate secret SECRET_KEY)
      export GITEA__OAUTH2__JWT_SECRET=$(gitea generate secret JWT_SECRET)
      export GITEA__SERVER__LFS_JWT_SECRET=$(gitea generate secret LFS_JWT_SECRET)

      env2ini::log "...Initial secrets generated\n"
    }

    # save existing envs prior to script execution. Necessary to keep order of preexisting and custom envs
    env | (grep -e '^GITEA__' || [[ $? == 1 ]]) > $TMP_EXISTING_ENVS_FILE

    # MUST BE CALLED BEFORE OTHER CONFIGURATION
    env2ini::generate_initial_secrets

    env2ini::load_config_sources "$ENV_TO_INI_MOUNT_POINT/inlines/"
    env2ini::load_config_sources "$ENV_TO_INI_MOUNT_POINT/additionals/"

    # load existing envs to override auto generated envs
    env2ini::reload_preset_envs

    env2ini::log "=== All configuration sources loaded ===\n"

    # safety to prevent rewrite of secret keys if an app.ini already exists
    if [ -f ${GITEA_APP_INI} ]; then
      env2ini::log 'An app.ini file already exists. To prevent overwriting secret keys, these settings are dropped and remain unchanged:'
      env2ini::log '  - security.INTERNAL_TOKEN'
      env2ini::log '  - security.SECRET_KEY'
      env2ini::log '  - oauth2.JWT_SECRET'
      env2ini::log '  - server.LFS_JWT_SECRET'

      unset GITEA__SECURITY__INTERNAL_TOKEN
      unset GITEA__SECURITY__SECRET_KEY
      unset GITEA__OAUTH2__JWT_SECRET
      unset GITEA__SERVER__LFS_JWT_SECRET
    fi

    environment-to-ini -o $GITEA_APP_INI
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea-init
  namespace: gitea
stringData:
  configure_gitea.sh: "#!/usr/bin/env bash\n\nset -euo pipefail\n\necho '==== BEGIN GITEA CONFIGURATION ===='\n\n{ # try\n  gitea migrate\n} || { # catch\n  echo \"Gitea migrate might fail due to database connection...This init-container will try again in a few seconds\"\n  exit 1\n}\nfunction test_valkey_connection() {\n  local RETRY=0\n  local MAX=30\n  \n  echo 'Wait for valkey to become avialable...'\n  until [ \"${RETRY}\" -ge \"${MAX}\" ]; do\n    nc -vz -w2 gitea-valkey-headless.gitea.svc.cluster.local 6379 && break\n    RETRY=$[${RETRY}+1]\n    echo \"...not ready yet (${RETRY}/${MAX})\"\n  done\n\n  if [ \"${RETRY}\" -ge \"${MAX}\" ]; then\n    echo \"Valkey not reachable after '${MAX}' attempts!\"\n    exit 1\n  fi\n}\n\ntest_valkey_connection\nfunction configure_admin_user() {\n  local full_admin_list=$(gitea admin user list --admin)\n  local actual_user_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+Username\\s+Email\\s+IsActive.*)\"\n  if [[ \"${full_admin_list}\" =~ $regex ]]; then\n    actual_user_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_admin_user' was not able to determine the current list of admin users.\"\n    echo \"       Please review the output of 'gitea admin user list --admin' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-gitea/issues.\"\n    echo \"DEBUG: Output of 'gitea admin user list --admin'\"\n    echo \"--\"\n    echo \"${full_admin_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local ACCOUNT_ID=$(echo \"${actual_user_table}\" | grep -E \"\\s+${GITEA_ADMIN_USERNAME}\\s+\" | awk -F \" \" \"{printf \\$1}\")\n  if [[ -z \"${ACCOUNT_ID}\" ]]; then\n    local -a create_args\n    create_args=(--admin --username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\" --email \"gitea@local.domain\")\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = initialOnlyRequireReset ]]; then\n      create_args+=(--must-change-password=true)\n    else\n      create_args+=(--must-change-password=false)\n    fi\n    echo \"No admin user '${GITEA_ADMIN_USERNAME}' found. Creating now...\"\n    gitea admin user create \"${create_args[@]}\"\n    echo '...created.'\n  else\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = keepUpdated ]]; then\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist. Running update to sync password...\"\n      # See https://gitea.com/gitea/helm-gitea/issues/673\n      # --must-change-password argument was added to change-password, defaulting to true, counter to the previous behavior\n      #   which acted as if it were provided with =false. If the argument is present in this version of gitea, then we\n      #   should add it to prevent requiring frequent admin password resets.\n      local -a change_args\n      change_args=(--username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\")\n      if gitea admin user change-password --help | grep -qF -- '--must-change-password'; then\n        change_args+=(--must-change-password=false)\n      fi\n      gitea admin user change-password \"${change_args[@]}\"\n      echo '...password sync done.'\n    else\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist, but update mode is set to '${GITEA_ADMIN_PASSWORD_MODE}'. Skipping.\"\n    fi\n  fi\n}\n\nconfigure_admin_user\n\nfunction configure_ldap() {\n    echo 'no ldap configuration... skipping.'\n}\n\nconfigure_ldap\n\nfunction configure_oauth() {\n  local OAUTH_NAME='keycloak'\n  local full_auth_list=$(gitea admin auth list --vertical-bars)\n  local actual_auth_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+\\|Name\\s+\\|Type\\s+\\|Enabled.*)\"\n  if [[ \"${full_auth_list}\" =~ $regex ]]; then\n    actual_auth_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_oauth' was not able to determine the current list of authentication sources.\"\n    echo \"       Please review the output of 'gitea admin auth list --vertical-bars' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-gitea/issues.\"\n    echo \"DEBUG: Output of 'gitea admin auth list --vertical-bars'\"\n    echo \"--\"\n    echo \"${full_auth_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local AUTH_ID=$(echo \"${actual_auth_table}\" | grep -E \"\\|${OAUTH_NAME}\\s+\\|\" | grep -iE '\\|OAuth2\\s+\\|' | awk -F \" \"  \"{print \\$1}\")\n\n  if [[ -z \"${AUTH_ID}\" ]]; then\n    echo \"No oauth configuration found with name '${OAUTH_NAME}'. Installing it now...\"\n    gitea admin auth add-oauth --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...installed.'\n  else\n    echo \"Existing oauth configuration with name '${OAUTH_NAME}': '${AUTH_ID}'. Running update to sync settings...\"\n    gitea admin auth update-oauth --id \"${AUTH_ID}\" --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...sync settings done.'\n  fi\n}\n\nconfigure_oauth\n\necho '==== END GITEA CONFIGURATION ===='"
  configure_gpg_environment.sh: |
    #!/usr/bin/env bash
    set -eu

    gpg --batch --import "$TMP_RAW_GPG_KEY"
  init_directory_structure.sh: |-
    #!/usr/bin/env bash

    set -euo pipefail
    mkdir -pv /data/git/.ssh
    chmod -Rv 700 /data/git/.ssh
    [ ! -d /data/gitea/conf ] && mkdir -pv /data/gitea/conf

    # prepare temp directory structure
    mkdir -pv "${GITEA_TEMP}"
    chmod -v ug+rwx "${GITEA_TEMP}"
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea-inline-config
  namespace: gitea
stringData:
  _generals_: ""
  cache: |-
    ADAPTER=redis
    HOST=redis://:changeme@gitea-valkey-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
  indexer: ISSUE_INDEXER_TYPE=db
  metrics: ENABLED=false
  queue: |-
    CONN_STR=redis://:changeme@gitea-valkey-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
    TYPE=redis
  repository: ROOT=/data/git/gitea-repositories
  security: INSTALL_LOCK=true
  server: |-
    APP_DATA_PATH=/data
    DOMAIN=gitea.homelab.olav.ninja
    ENABLE_PPROF=false
    HTTP_PORT=3000
    LFS_START_SERVER=true
    OFFLINE_MODE=false
    PROTOCOL=http
    ROOT_URL=https://gitea.homelab.olav.ninja
    SSH_DOMAIN=gitea.homelab.olav.ninja
    SSH_LISTEN_PORT=2222
    SSH_PORT=22
    START_SSH_SERVER=true
  service: DISABLE_REGISTRATION=true
  session: |-
    PROVIDER=redis
    PROVIDER_CONFIG=redis://:changeme@gitea-valkey-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
type: Opaque
---
apiVersion: v1
data:
  valkey-password: Y2hhbmdlbWU=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey
  namespace: gitea
type: Opaque
---
apiVersion: v1
kind: Endpoints
metadata:
  name: external-cluster
  namespace: traefik
subsets:
  - addresses:
      - ip: 192.168.0.80
    ports:
      - name: ingress-port
        port: 443
        protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: bambustudio
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: bambustudio
    app.kubernetes.io/service: bambustudio
    helm.sh/chart: app-template-4.3.0
  name: bambustudio
  namespace: bambustudio
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/controller: bambustudio
    app.kubernetes.io/instance: bambustudio
    app.kubernetes.io/name: bambustudio
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager
  namespace: cert-manager
spec:
  ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: http-metrics
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cert-manager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
  selector:
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cainjector
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
  selector:
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: webhook
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-webhook-service
  namespace: cnpg-system
spec:
  ports:
    - name: webhook-server
      port: 443
      targetPort: webhook-server
  selector:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/name: cloudnative-pg
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    io.cilium/lb-ipam-ips: 192.168.0.91
    lbipam.cilium.io/sharing-key: coturn
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn-tcp
  namespace: coturn
spec:
  ports:
    - name: tcp
      port: 3478
      protocol: TCP
      targetPort: tcp
    - name: tcp-tls
      port: 5349
      protocol: TCP
      targetPort: tcp-tls
  selector:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/name: coturn
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    io.cilium/lb-ipam-ips: 192.168.0.91
    lbipam.cilium.io/sharing-key: coturn
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn-udp
  namespace: coturn
spec:
  ports:
    - name: udp
      port: 3478
      protocol: UDP
      targetPort: udp
    - name: udp-tls
      port: 5349
      protocol: UDP
      targetPort: udp-tls
  selector:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/name: coturn
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    release: crossplane
  name: crossplane-webhooks
  namespace: crossplane
spec:
  ports:
    - port: 9443
      protocol: TCP
      targetPort: 9443
  selector:
    app: crossplane
    release: crossplane
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: external-dns
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: external-dns
    app.kubernetes.io/version: 0.19.0
    helm.sh/chart: external-dns-1.19.0
  name: external-dns
  namespace: external-dns
spec:
  ports:
    - name: http
      port: 7979
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: external-dns
    app.kubernetes.io/name: external-dns
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea-http
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: http
      port: 3000
      targetPort: 3000
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea-ssh
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: 2222
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-headless
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: valkey
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-primary
  namespace: gitea
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: valkey
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/service: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/name: hajimari
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: homeassistant
  namespace: homeassistant
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8123
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    - name: http-ha
      port: 8123
      protocol: TCP
      targetPort: 8123
    - name: mqtt
      port: 1883
      protocol: TCP
      targetPort: 1883
    - name: mqtts
      port: 8883
      protocol: TCP
      targetPort: 8883
  selector:
    kubevirt.io/vm: homeassistant-vm
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/service: immich-machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-machine-learning
  namespace: immich
spec:
  ports:
    - name: http
      port: 3003
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: machine-learning
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-headless
  namespace: immich
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/service: immich-server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-server
  namespace: immich
spec:
  ports:
    - name: http
      port: 2283
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: server
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: headless
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloakx
    app.kubernetes.io/version: 26.3.3
    helm.sh/chart: keycloakx-7.1.3
  name: keycloak-keycloakx-headless
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloakx
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: http
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloakx
    app.kubernetes.io/version: 26.3.3
    helm.sh/chart: keycloakx-7.1.3
  name: keycloak-keycloakx-http
  namespace: keycloak
spec:
  ports:
    - name: http-internal
      port: 9000
      protocol: TCP
      targetPort: http-internal
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloakx
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "9964"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    io.cilium/app: proxy
    k8s-app: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: envoy-metrics
      port: 9964
      protocol: TCP
      targetPort: envoy-metrics
  selector:
    k8s-app: cilium-envoy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-peer
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: hubble-peer
  namespace: kube-system
spec:
  internalTrafficPolicy: Local
  ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
  selector:
    k8s-app: cilium
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-relay
  name: hubble-relay
  namespace: kube-system
spec:
  ports:
    - port: 80
      protocol: TCP
      targetPort: grpc
  selector:
    k8s-app: hubble-relay
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    k8s-app: hubble-ui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: longhorn-admission-webhook
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-admission-webhook
  namespace: longhorn-system
spec:
  ports:
    - name: admission-webhook
      port: 9502
      targetPort: admission-wh
  selector:
    longhorn.io/admission-webhook: longhorn-admission-webhook
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: longhorn-manager
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-backend
  namespace: longhorn-system
spec:
  ports:
    - name: manager
      port: 9500
      targetPort: manager
  selector:
    app: longhorn-manager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: longhorn-ui
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-frontend
  namespace: longhorn-system
spec:
  ports:
    - name: http
      nodePort: null
      port: 80
      targetPort: http
  selector:
    app: longhorn-ui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: longhorn-recovery-backend
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-recovery-backend
  namespace: longhorn-system
spec:
  ports:
    - name: recovery-backend
      port: 9503
      targetPort: recov-backend
  selector:
    longhorn.io/recovery-backend: longhorn-recovery-backend
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-management
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-relay
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  ports:
    - name: https
      port: 80
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-signal
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.12.0
    helm.sh/chart: netbird-dashboard-1.2.0
  name: netbird-dashboard
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/name: netbird-dashboard
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-metrics
  namespace: netbird
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/name: kubernetes-operator
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-webhook-service
  namespace: netbird
spec:
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 9443
  selector:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/name: kubernetes-operator
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 32.0.0
    helm.sh/chart: nextcloud-8.3.0
  name: nextcloud
  namespace: nextcloud
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: nextcloud
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-headless
  namespace: nextcloud
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: redis
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-master
  namespace: nextcloud
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: redis
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.12.2
    helm.sh/chart: ollama-1.30.0
  name: ollama
  namespace: ollama
spec:
  ports:
    - name: http
      port: 11434
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/name: ollama
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: open-webui
    app.kubernetes.io/version: 0.6.32
    helm.sh/chart: open-webui-8.9.0
  name: open-webui
  namespace: openwebui
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pingvinshare
    app.kubernetes.io/service: pingvinshare
    helm.sh/chart: app-template-4.3.0
  name: pingvinshare
  namespace: pingvinshare
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/controller: pingvinshare
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/name: pingvinshare
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    control-plane: controller
    helm.sh/chart: v0.0.1
  name: pubip-operator-controller-metrics-service
  namespace: pubip-operator
spec:
  ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: 8443
  selector:
    control-plane: controller
---
apiVersion: v1
kind: Service
metadata:
  name: sablier-sablier
  namespace: sablier
spec:
  ports:
    - port: 10000
      protocol: TCP
      targetPort: 10000
  selector:
    app: sablier-sablier
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  ports:
    - name: http
      nodePort: null
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/name: sealed-secrets
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: external-cluster
  namespace: traefik
spec:
  clusterIP: None
  ports:
    - name: ingress-port
      port: 443
      protocol: TCP
      targetPort: 443
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    external-dns.alpha.kubernetes.io/internal-hostname: homelab.olav.ninja,*.homelab.olav.ninja
    netbird.io/expose: "true"
    netbird.io/groups: cluster
    netbird.io/policy: cluster
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-37.1.2
  name: traefik
  namespace: traefik
spec:
  clusterIP: 10.96.100.100
  externalTrafficPolicy: Local
  internalTrafficPolicy: Local
  loadBalancerIP: 192.168.0.90
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: ssh
    - name: webpublic
      port: 9443
      protocol: TCP
      targetPort: webpublic
    - name: websecure
      port: 443
      protocol: TCP
      targetPort: websecure
  selector:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/name: traefik
  type: LoadBalancer
---
apiVersion: scheduling.k8s.io/v1
description: This priority class should be used for core kubevirt components only.
globalDefault: false
kind: PriorityClass
metadata:
  name: kubevirt-cluster-critical
value: 1000000000
---
apiVersion: scheduling.k8s.io/v1
description: Ensure Longhorn pods have the highest priority to prevent any unexpected eviction by the Kubernetes scheduler under node pressure
globalDefault: false
kind: PriorityClass
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-critical
preemptionPolicy: PreemptLowerPriority
value: 1000000000
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: bambustudio-config
  namespace: bambustudio
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: longhorn-static
  volumeName: bambustudio-config
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: bambustudio-library
  namespace: bambustudio
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 25Gi
  storageClassName: longhorn-static
  volumeName: bambustudio-library
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: longhorn-static
  volumeName: gitea
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: immich-library
  namespace: immich
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 120Gi
  storageClassName: longhorn-static
  volumeName: immich-library
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: longhorn-static
  volumeName: netbird-management
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: longhorn-static
  volumeName: netbird-signal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-data
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 310Gi
  storageClassName: longhorn-static
  volumeName: nextcloud-data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.12.2
    helm.sh/chart: ollama-1.30.0
  name: ollama
  namespace: ollama
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 60Gi
  storageClassName: longhorn-static
  volumeName: ollama
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: open-webui
  namespace: openwebui
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: longhorn-static
  volumeName: open-webui
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pingvinshare
  namespace: pingvinshare
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: longhorn-static
  volumeName: pingvinshare
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/controller: bambustudio
    app.kubernetes.io/instance: bambustudio
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: bambustudio
    helm.sh/chart: app-template-4.3.0
    sablier.enable: "true"
    sablier.group: bambustudio
  name: bambustudio
  namespace: bambustudio
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/controller: bambustudio
      app.kubernetes.io/instance: bambustudio
      app.kubernetes.io/name: bambustudio
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/controller: bambustudio
        app.kubernetes.io/instance: bambustudio
        app.kubernetes.io/name: bambustudio
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DARK_MODE
              value: "true"
            - name: DRINODE
              value: /dev/dri/renderD128
            - name: GUID
              value: "1000"
            - name: PUID
              value: "1000"
            - name: TZ
              value: Europe/Oslo
          image: linuxserver/bambustudio:02.02.02
          name: app
          resources:
            limits:
              amd.com/gpu: 1
              memory: 8096M
            requests:
              memory: 128M
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /library
              name: data
      dnsPolicy: ClusterFirst
      enableServiceLinks: false
      hostIPC: false
      hostNetwork: false
      hostPID: false
      serviceAccountName: default
      volumes:
        - name: config
          persistentVolumeClaim:
            claimName: bambustudio-config
        - name: data
          persistentVolumeClaim:
            claimName: bambustudio-library
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    cdi.kubevirt.io: cdi-operator
    name: cdi-operator
    operator.cdi.kubevirt.io: ""
    prometheus.cdi.kubevirt.io: "true"
  name: cdi-operator
  namespace: cdi
spec:
  replicas: 1
  selector:
    matchLabels:
      name: cdi-operator
      operator.cdi.kubevirt.io: ""
  strategy: {}
  template:
    metadata:
      annotations:
        openshift.io/required-scc: restricted-v2
      labels:
        cdi.kubevirt.io: cdi-operator
        name: cdi-operator
        operator.cdi.kubevirt.io: ""
        prometheus.cdi.kubevirt.io: "true"
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: cdi.kubevirt.io
                      operator: In
                      values:
                        - cdi-operator
                topologyKey: kubernetes.io/hostname
              weight: 1
      containers:
        - env:
            - name: DEPLOY_CLUSTER_RESOURCES
              value: "true"
            - name: OPERATOR_VERSION
              value: v1.62.0
            - name: CONTROLLER_IMAGE
              value: quay.io/kubevirt/cdi-controller:v1.62.0
            - name: IMPORTER_IMAGE
              value: quay.io/kubevirt/cdi-importer:v1.62.0
            - name: CLONER_IMAGE
              value: quay.io/kubevirt/cdi-cloner:v1.62.0
            - name: OVIRT_POPULATOR_IMAGE
              value: quay.io/kubevirt/cdi-importer:v1.62.0
            - name: APISERVER_IMAGE
              value: quay.io/kubevirt/cdi-apiserver:v1.62.0
            - name: UPLOAD_SERVER_IMAGE
              value: quay.io/kubevirt/cdi-uploadserver:v1.62.0
            - name: UPLOAD_PROXY_IMAGE
              value: quay.io/kubevirt/cdi-uploadproxy:v1.62.0
            - name: VERBOSITY
              value: "1"
            - name: PULL_POLICY
              value: IfNotPresent
            - name: MONITORING_NAMESPACE
          image: quay.io/kubevirt/cdi-operator:v1.62.0
          imagePullPolicy: IfNotPresent
          name: cdi-operator
          ports:
            - containerPort: 8080
              name: metrics
              protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 150Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
      serviceAccountName: cdi-operator
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cert-manager
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cert-manager
        app.kubernetes.io/version: v1.18.2
        helm.sh/chart: cert-manager-v1.18.2
    spec:
      containers:
        - args:
            - --v=2
            - --cluster-resource-namespace=$(POD_NAMESPACE)
            - --leader-election-namespace=cert-manager
            - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.18.2
            - --max-concurrent-challenges=60
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.18.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
            - containerPort: 9403
              name: http-healthz
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cainjector
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cainjector
        app.kubernetes.io/version: v1.18.2
        helm.sh/chart: cert-manager-v1.18.2
    spec:
      containers:
        - args:
            - --v=2
            - --leader-election-namespace=cert-manager
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.18.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-cainjector
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: webhook
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: webhook
        app.kubernetes.io/version: v1.18.2
        helm.sh/chart: cert-manager-v1.18.2
    spec:
      containers:
        - args:
            - --v=2
            - --secure-port=10250
            - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
            - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
            - --dynamic-serving-dns-names=cert-manager-webhook
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.18.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
            - containerPort: 10250
              name: https
              protocol: TCP
            - containerPort: 6080
              name: healthcheck
              protocol: TCP
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-cloudnative-pg
  namespace: cnpg-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: cnpg
      app.kubernetes.io/name: cloudnative-pg
  template:
    metadata:
      annotations:
        checksum/config: c0361e36cbad50677066d4c096e50c3debed68e7a743ebd671c0a428b5565580
        checksum/monitoring-config: 6cce6ad11601c246e0531eb45d4b8c6c327647be0a57e42375c600cd5d329739
        checksum/rbac: 61a046ed01892794802487ddb709ba74073547b7ebbf55903efa7205703ba4af
      labels:
        app.kubernetes.io/instance: cnpg
        app.kubernetes.io/name: cloudnative-pg
    spec:
      containers:
        - args:
            - controller
            - --leader-elect
            - --max-concurrent-reconciles=10
            - --config-map-name=cnpg-controller-manager-config
            - --webhook-port=9443
          command:
            - /manager
          env:
            - name: OPERATOR_IMAGE_NAME
              value: ghcr.io/cloudnative-pg/cloudnative-pg:1.27.0
            - name: OPERATOR_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MONITORING_QUERIES_CONFIGMAP
              value: cnpg-default-monitoring
          image: ghcr.io/cloudnative-pg/cloudnative-pg:1.27.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /readyz
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 3
          name: manager
          ports:
            - containerPort: 8080
              name: metrics
              protocol: TCP
            - containerPort: 9443
              name: webhook-server
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /readyz
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 10001
            runAsUser: 10001
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 6
            httpGet:
              path: /readyz
              port: 9443
              scheme: HTTPS
            periodSeconds: 5
          volumeMounts:
            - mountPath: /controller
              name: scratch-data
            - mountPath: /run/secrets/cnpg.io/webhook
              name: webhook-certificates
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cnpg-cloudnative-pg
      terminationGracePeriodSeconds: 10
      volumes:
        - emptyDir: {}
          name: scratch-data
        - name: webhook-certificates
          secret:
            defaultMode: 420
            optional: true
            secretName: cnpg-webhook-cert
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: coturn
      app.kubernetes.io/name: coturn
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: coturn
        app.kubernetes.io/name: coturn
    spec:
      containers:
        - args:
            - --listening-port=3478
            - --tls-listening-port=5349
            - --user=netbird:$$COTURN_USER_netbird_KEY
            - --cert=/usr/local/etc/tls.crt
            - --pkey=/usr/local/etc/tls.key
            - --realm=coturn.homelab.olav.ninja
            - --log-file=stdout
            - --no-software-attribute
            - --no-cli
            - --listening-ip=0.0.0.0
          env:
            - name: COTURN_USER_netbird_KEY
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
          image: coturn/coturn:4.6.3-r3
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            tcpSocket:
              port: tcp
          name: coturn
          ports:
            - containerPort: 3478
              name: tcp
              protocol: TCP
            - containerPort: 3478
              name: udp
              protocol: UDP
            - containerPort: 5349
              name: tcp-tls
              protocol: TCP
            - containerPort: 5349
              name: udp-tls
              protocol: UDP
          readinessProbe:
            tcpSocket:
              port: tcp
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/turnserver.conf
              name: config
              subPath: turnserver.conf
            - mountPath: /usr/local/etc
              name: certs
      securityContext: {}
      serviceAccountName: coturn
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      volumes:
        - configMap:
            name: coturn
          name: config
        - name: certs
          secret:
            secretName: coturn
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    release: crossplane
  name: crossplane
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 2.0.2
        helm.sh/chart: crossplane-2.0.2
        release: crossplane
    spec:
      containers:
        - args:
            - core
            - start
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: LEADER_ELECTION
              value: "true"
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_SERVER_CERTS_DIR
              value: /tls/server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
            - name: TLS_CLIENT_CERTS_DIR
              value: /tls/client
          image: xpkg.crossplane.io/crossplane/crossplane:v2.0.2
          imagePullPolicy: IfNotPresent
          name: crossplane
          ports:
            - containerPort: 8081
              name: readyz
            - containerPort: 9443
              name: webhooks
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
          startupProbe:
            failureThreshold: 30
            periodSeconds: 2
            tcpSocket:
              port: readyz
          volumeMounts:
            - mountPath: /cache/xpkg
              name: package-cache
            - mountPath: /cache/xfn
              name: function-cache
            - mountPath: /tls/server
              name: tls-server-certs
            - mountPath: /tls/client
              name: tls-client-certs
      hostNetwork: false
      initContainers:
        - args:
            - core
            - init
            - --activation
            - '*'
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: WEBHOOK_SERVICE_NAME
              value: crossplane-webhooks
            - name: WEBHOOK_SERVICE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: WEBHOOK_SERVICE_PORT
              value: "9443"
            - name: TLS_CA_SECRET_NAME
              value: crossplane-root-ca
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
          image: xpkg.crossplane.io/crossplane/crossplane:v2.0.2
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: crossplane
      volumes:
        - emptyDir:
            medium: null
            sizeLimit: 20Mi
          name: package-cache
        - emptyDir:
            medium: null
            sizeLimit: 512Mi
          name: function-cache
        - name: tls-server-certs
          secret:
            secretName: crossplane-tls-server
        - name: tls-client-certs
          secret:
            secretName: crossplane-tls-client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane-rbac-manager
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 2.0.2
    helm.sh/chart: crossplane-2.0.2
    release: crossplane
  name: crossplane-rbac-manager
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane-rbac-manager
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane-rbac-manager
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 2.0.2
        helm.sh/chart: crossplane-2.0.2
        release: crossplane
    spec:
      containers:
        - args:
            - rbac
            - start
            - --provider-clusterrole=crossplane:allowed-provider-permissions
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: LEADER_ELECTION
              value: "true"
          image: xpkg.crossplane.io/crossplane/crossplane:v2.0.2
          imagePullPolicy: IfNotPresent
          name: crossplane
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      initContainers:
        - args:
            - rbac
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
          image: xpkg.crossplane.io/crossplane/crossplane:v2.0.2
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: rbac-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: external-dns
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: external-dns
    app.kubernetes.io/version: 0.19.0
    helm.sh/chart: external-dns-1.19.0
  name: external-dns
  namespace: external-dns
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/name: external-dns
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: external-dns
        app.kubernetes.io/name: external-dns
    spec:
      automountServiceAccountToken: true
      containers:
        - args:
            - --log-level=info
            - --log-format=text
            - --interval=1m
            - --source=crd
            - --source=service
            - --policy=upsert-only
            - --registry=txt
            - --provider=cloudflare
            - --publish-internal-services
          env:
            - name: CF_API_TOKEN
              valueFrom:
                secretKeyRef:
                  key: apiKey
                  name: cloudflare-api-key
            - name: CF_API_EMAIL
              valueFrom:
                secretKeyRef:
                  key: email
                  name: cloudflare-api-key
          image: registry.k8s.io/external-dns/external-dns:v0.19.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 2
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: external-dns
          ports:
            - containerPort: 7979
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
      securityContext:
        fsGroup: 65534
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: external-dns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    sablier.enable: "true"
    sablier.group: gitea
    version: 1.24.6
  name: gitea
  namespace: gitea
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: gitea
      sablier.enable: "true"
      sablier.group: gitea
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: 1e98933eecbde0dd0be1cad8e08a31af0eba6e6aee172a54677affc003fc127b
        checksum/oauth_0: 01764dcd9b49e15763176a4daa8f8e10f4902756616d3e834ef1341e80062ad5
      labels:
        app: gitea
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: gitea
        app.kubernetes.io/version: 1.24.6
        helm.sh/chart: gitea-12.3.0
        sablier.enable: "true"
        sablier.group: gitea
        version: 1.24.6
    spec:
      containers:
        - env:
            - name: SSH_LISTEN_PORT
              value: "2222"
            - name: SSH_PORT
              value: "22"
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMPDIR
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
          image: docker.gitea.com/gitea:1.24.6-rootless
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 200
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: gitea
          ports:
            - containerPort: 2222
              name: ssh
            - containerPort: 3000
              name: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      initContainers:
        - command:
            - /usr/sbinx/init_directory_structure.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
          image: docker.gitea.com/gitea:1.24.6-rootless
          imagePullPolicy: IfNotPresent
          name: init-directories
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbinx
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
        - command:
            - /usr/sbinx/config_environment.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMP_EXISTING_ENVS_FILE
              value: /tmp/existing-envs
            - name: ENV_TO_INI_MOUNT_POINT
              value: /env-to-ini-mounts
            - name: GITEA__database__DB_TYPE
              value: postgres
            - name: GITEA__database__HOST
              valueFrom:
                secretKeyRef:
                  key: host
                  name: gitea-postgresql-app
            - name: GITEA__database__NAME
              valueFrom:
                secretKeyRef:
                  key: dbname
                  name: gitea-postgresql-app
            - name: GITEA__database__USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: gitea-postgresql-app
            - name: GITEA__database__PASSWD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-postgresql-app
          image: docker.gitea.com/gitea:1.24.6-rootless
          imagePullPolicy: IfNotPresent
          name: init-app-ini
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbinx
              name: config
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
            - mountPath: /env-to-ini-mounts/inlines/
              name: inline-config-sources
        - command:
            - /usr/sbinx/configure_gitea.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
            - name: GITEA_OAUTH_KEY_0
              valueFrom:
                secretKeyRef:
                  key: key
                  name: gitea-oidc-credentials
            - name: GITEA_OAUTH_SECRET_0
              valueFrom:
                secretKeyRef:
                  key: secret
                  name: gitea-oidc-credentials
            - name: GITEA_ADMIN_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD_MODE
              value: keepUpdated
          image: docker.gitea.com/gitea:1.24.6-rootless
          imagePullPolicy: IfNotPresent
          name: configure-gitea
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/sbinx
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      securityContext:
        fsGroup: 1000
      terminationGracePeriodSeconds: 60
      volumes:
        - name: init
          secret:
            defaultMode: 110
            secretName: gitea-init
        - name: config
          secret:
            defaultMode: 110
            secretName: gitea
        - name: inline-config-sources
          secret:
            secretName: gitea-inline-config
        - emptyDir: {}
          name: temp
        - name: data
          persistentVolumeClaim:
            claimName: gitea
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: hajimari
      app.kubernetes.io/name: hajimari
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: hajimari
        app.kubernetes.io/name: hajimari
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: TZ
              value: UTC
          image: ghcr.io/toboshii/hajimari:v0.3.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          name: hajimari
          ports:
            - containerPort: 3000
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 0
            periodSeconds: 5
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config/config.yaml
              name: hajimari-settings
              subPath: config.yaml
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: hajimari
      volumes:
        - configMap:
            name: hajimari-settings
          name: hajimari-settings
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-machine-learning
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: machine-learning
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: machine-learning
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql-rw
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: immich-postgresql-user
            - name: DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: immich-postgresql-user
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
            - name: TRANSFORMERS_CACHE
              value: /cache
          envFrom:
            - secretRef:
                name: immich-postgresql-user
          image: ghcr.io/immich-app/immich-machine-learning:v1.144.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-machine-learning
          ports:
            - containerPort: 3003
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 1
              ephemeral-storage: 10Gi
            requests:
              cpu: 10m
              memory: 2Gi
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /cache
              name: cache
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - emptyDir: {}
          name: cache
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-server
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: server
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 88c813a390a4e10d83e7da3a7df99e9aa98f2f6ce28cc391662822301a54b78e
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: server
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql-rw
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: immich-postgresql-user
            - name: DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: immich-postgresql-user
            - name: IMMICH_CONFIG_FILE
              value: /config/immich-config.yaml
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
          envFrom:
            - secretRef:
                name: immich-postgresql-user
          image: ghcr.io/immich-app/immich-server:v1.144.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-server
          ports:
            - containerPort: 2283
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /usr/src/app/upload
              name: library
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - name: config
          secret:
            secretName: immich-config
        - name: library
          persistentVolumeClaim:
            claimName: immich-library
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: cilium-operator
    app.kubernetes.io/part-of: cilium
    io.cilium/app: operator
    name: cilium-operator
  name: cilium-operator
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      io.cilium/app: operator
      name: cilium-operator
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 100%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/port: "9963"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: cilium-operator
        app.kubernetes.io/part-of: cilium
        io.cilium/app: operator
        name: cilium-operator
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  io.cilium/app: operator
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
            - --debug=$(CILIUM_DEBUG)
          command:
            - cilium-operator-generic
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_DEBUG
              valueFrom:
                configMapKeyRef:
                  key: debug
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/operator-generic:v1.18.2@sha256:cb4e4ffc5789fd5ff6a534e3b1460623df61cba00f5ea1c7b40153b5efb81805
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
          name: cilium-operator
          ports:
            - containerPort: 9963
              hostPort: 9963
              name: prometheus
              protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp/cilium/config-map
              name: cilium-config-path
              readOnly: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cilium-operator
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: node-role.kubernetes.io/master
          operator: Exists
        - key: node.kubernetes.io/not-ready
          operator: Exists
        - key: node.cloudprovider.kubernetes.io/uninitialized
          operator: Exists
        - key: node.cilium.io/agent-not-ready
          operator: Exists
      volumes:
        - configMap:
            name: cilium-config
          name: cilium-config-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-relay
  name: hubble-relay
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-relay
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cilium.io/hubble-relay-configmap-checksum: 0aebee6bdee393dd840ea0e068f2efeae387cc07114bb26becb030f0ab1e2397
      labels:
        app.kubernetes.io/name: hubble-relay
        app.kubernetes.io/part-of: cilium
        k8s-app: hubble-relay
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: false
      containers:
        - args:
            - serve
          command:
            - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.18.2@sha256:6079308ee15e44dff476fb522612732f7c5c4407a1017bc3470916242b0405ac
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 12
            grpc:
              port: 4222
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 10
          name: hubble-relay
          ports:
            - containerPort: 4245
              name: grpc
          readinessProbe:
            grpc:
              port: 4222
            timeoutSeconds: 3
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 4222
            initialDelaySeconds: 10
            periodSeconds: 3
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/hubble-relay
              name: config
              readOnly: true
            - mountPath: /var/lib/hubble-relay/tls
              name: tls
              readOnly: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: null
      restartPolicy: Always
      securityContext:
        fsGroup: 65532
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: hubble-relay
      terminationGracePeriodSeconds: 1
      volumes:
        - configMap:
            items:
              - key: config.yaml
                path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: client.crt
                    - key: tls.key
                      path: client.key
                    - key: ca.crt
                      path: hubble-server-ca.crt
                  name: hubble-relay-client-certs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-ui
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cilium.io/hubble-ui-nginx-configmap-checksum: 76283720d1bb70050debf51116121fa9a67ebc9d1cd9167c3dd9bdbfb613df37
      labels:
        app.kubernetes.io/name: hubble-ui
        app.kubernetes.io/part-of: cilium
        k8s-app: hubble-ui
    spec:
      automountServiceAccountToken: true
      containers:
        - image: quay.io/cilium/hubble-ui:v0.13.3@sha256:661d5de7050182d495c6497ff0b007a7a1e379648e60830dd68c4d78ae21761d
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
          name: frontend
          ports:
            - containerPort: 8081
              name: http
          readinessProbe:
            httpGet:
              path: /
              port: 8081
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/nginx/conf.d/default.conf
              name: hubble-ui-nginx-conf
              subPath: nginx.conf
            - mountPath: /tmp
              name: tmp-dir
        - env:
            - name: EVENTS_SERVER_PORT
              value: "8090"
            - name: FLOWS_API_ADDR
              value: hubble-relay:80
          image: quay.io/cilium/hubble-ui-backend:v0.13.3@sha256:db1454e45dc39ca41fbf7cad31eec95d99e5b9949c39daaad0fa81ef29d56953
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
            - containerPort: 8090
              name: grpc
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts: null
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: null
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      serviceAccountName: hubble-ui
      volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    kubevirt.io: virt-operator
  name: virt-operator
  namespace: kubevirt
spec:
  replicas: 2
  selector:
    matchLabels:
      kubevirt.io: virt-operator
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        openshift.io/required-scc: restricted-v2
      labels:
        kubevirt.io: virt-operator
        name: virt-operator
        prometheus.kubevirt.io: "true"
      name: virt-operator
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: kubevirt.io
                      operator: In
                      values:
                        - virt-operator
                topologyKey: kubernetes.io/hostname
              weight: 1
      containers:
        - args:
            - --port
            - "8443"
            - -v
            - "2"
          command:
            - virt-operator
          env:
            - name: VIRT_OPERATOR_IMAGE
              value: quay.io/kubevirt/virt-operator:v1.5.2
            - name: WATCH_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.annotations['olm.targetNamespaces']
            - name: KUBEVIRT_VERSION
              value: v1.5.2
          image: quay.io/kubevirt/virt-operator:v1.5.2
          imagePullPolicy: IfNotPresent
          name: virt-operator
          ports:
            - containerPort: 8443
              name: metrics
              protocol: TCP
            - containerPort: 8444
              name: webhooks
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /metrics
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 5
            timeoutSeconds: 10
          resources:
            requests:
              cpu: 10m
              memory: 450Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /etc/virt-operator/certificates
              name: kubevirt-operator-certs
              readOnly: true
            - mountPath: /profile-data
              name: profile-data
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: kubevirt-cluster-critical
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: kubevirt-operator
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
      volumes:
        - name: kubevirt-operator-certs
          secret:
            optional: true
            secretName: kubevirt-operator-certs
        - emptyDir: {}
          name: profile-data
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-driver-deployer
  namespace: longhorn-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: longhorn-driver-deployer
  template:
    metadata:
      labels:
        app: longhorn-driver-deployer
        app.kubernetes.io/instance: longhorn
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: longhorn
        app.kubernetes.io/version: v1.10.0
        helm.sh/chart: longhorn-1.10.0
    spec:
      containers:
        - command:
            - longhorn-manager
            - -d
            - deploy-driver
            - --manager-image
            - longhornio/longhorn-manager:v1.10.0
            - --manager-url
            - http://longhorn-backend:9500/v1
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: CSI_ATTACHER_IMAGE
              value: longhornio/csi-attacher:v4.9.0-20250826
            - name: CSI_PROVISIONER_IMAGE
              value: longhornio/csi-provisioner:v5.3.0-20250826
            - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE
              value: longhornio/csi-node-driver-registrar:v2.14.0-20250826
            - name: CSI_RESIZER_IMAGE
              value: longhornio/csi-resizer:v1.14.0-20250826
            - name: CSI_SNAPSHOTTER_IMAGE
              value: longhornio/csi-snapshotter:v8.3.0-20250826
            - name: CSI_LIVENESS_PROBE_IMAGE
              value: longhornio/livenessprobe:v2.16.0-20250826
          image: longhornio/longhorn-manager:v1.10.0
          imagePullPolicy: IfNotPresent
          name: longhorn-driver-deployer
      initContainers:
        - command:
            - sh
            - -c
            - while [ $(curl -m 1 -s -o /dev/null -w "%{http_code}" http://longhorn-backend:9500/v1) != "200" ]; do echo waiting; sleep 2; done
          image: longhornio/longhorn-manager:v1.10.0
          name: wait-longhorn-manager
      priorityClassName: longhorn-critical
      securityContext:
        runAsUser: 0
      serviceAccountName: longhorn-service-account
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: longhorn-ui
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-ui
  namespace: longhorn-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: longhorn-ui
  template:
    metadata:
      labels:
        app: longhorn-ui
        app.kubernetes.io/instance: longhorn
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: longhorn
        app.kubernetes.io/version: v1.10.0
        helm.sh/chart: longhorn-1.10.0
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - longhorn-ui
                topologyKey: kubernetes.io/hostname
              weight: 1
      containers:
        - env:
            - name: LONGHORN_MANAGER_IP
              value: http://longhorn-backend:9500
            - name: LONGHORN_UI_PORT
              value: "8000"
          image: longhornio/longhorn-ui:v1.10.0
          imagePullPolicy: IfNotPresent
          name: longhorn-ui
          ports:
            - containerPort: 8000
              name: http
          volumeMounts:
            - mountPath: /var/cache/nginx/
              name: nginx-cache
            - mountPath: /var/config/nginx/
              name: nginx-config
            - mountPath: /var/run/
              name: var-run
      priorityClassName: longhorn-critical
      serviceAccountName: longhorn-ui-service-account
      volumes:
        - emptyDir: {}
          name: nginx-cache
        - emptyDir: {}
          name: nginx-config
        - emptyDir: {}
          name: var-run
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-management
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: a7d495890f56d73e445f354af57bb002ed4c977911c8422a1650a4a531d69800
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-management
    spec:
      containers:
        - args:
            - --log-level
            - info
            - --log-file
            - console
            - --dns-domain
            - netbird
          image: netbirdio/management:0.59.2
          imagePullPolicy: IfNotPresent
          name: netbird-management
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /var/lib/netbird
              name: management
      initContainers:
        - args:
            - |
              go install github.com/drone/envsubst/cmd/envsubst@latest && envsubst < /tmp/netbird/management.tmpl.json > /etc/netbird/management.json && cat /etc/netbird/management.json
          command:
            - /bin/sh
            - -c
          env:
            - name: NETBIRD_SIGNAL_URI
              value: netbird.homelab.olav.ninja:443
            - name: NETBIRD_SIGNAL_PROTOCOL
              value: https
            - name: NETBIRD_STUN_URI
              value: stun:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_URI
              value: turn:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_USER
              value: netbird
            - name: NETBIRD_TURN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
            - name: NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: NETBIRD_MGMT_API_CERT_FILE
              value: ""
            - name: NETBIRD_MGMT_API_CERT_KEY_FILE
              value: ""
            - name: NETBIRD_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_USER_ID_CLAIM
            - name: NETBIRD_AUTH_DEVICE_AUTH_PROVIDER
              value: hosted
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/auth
            - name: NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
            - name: NETBIRD_AUTH_DEVICE_AUTH_SCOPE
              value: openid
            - name: NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN
              value: "false"
            - name: NETBIRD_IDP_MANAGER_TYPE
              value: none
          envFrom:
            - secretRef:
                name: netbird-relay-secret
          image: golang:latest
          imagePullPolicy: IfNotPresent
          name: configure
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /tmp/netbird
              name: config-template
      securityContext: {}
      serviceAccountName: netbird-backend-management
      volumes:
        - emptyDir:
            medium: Memory
          name: config
        - configMap:
            name: netbird-backend-management
          name: config-template
        - name: management
          persistentVolumeClaim:
            claimName: netbird-backend-management
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-relay
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-relay
    spec:
      containers:
        - args:
            - --listen-address
            - :80
            - --log-level
            - info
            - --log-file
            - console
          envFrom:
            - secretRef:
                name: netbird-relay-secret
          image: netbirdio/relay:0.59.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: 80
          name: netbird-relay
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: 80
          resources: {}
          securityContext: null
      securityContext: null
      serviceAccountName: netbird-backend-relay
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-signal
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-signal
    spec:
      containers:
        - args:
            - --port
            - "80"
            - --log-level
            - info
            - --log-file
            - console
          image: netbirdio/signal:0.59.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: https
          name: netbird-signal
          ports:
            - containerPort: 80
              name: https
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: https
          resources: {}
          securityContext: null
          volumeMounts:
            - mountPath: /var/lib/netbird
              name: signal
      securityContext: null
      serviceAccountName: netbird-backend-signal
      volumes:
        - name: signal
          persistentVolumeClaim:
            claimName: netbird-backend-signal
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.12.0
    helm.sh/chart: netbird-dashboard-1.2.0
  name: netbird-dashboard
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-dashboard
      app.kubernetes.io/name: netbird-dashboard
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-dashboard
        app.kubernetes.io/name: netbird-dashboard
    spec:
      containers:
        - args:
            - |
              sed -i 's/listen \[\:\:\]\:80 default_server\;//g' /etc/nginx/http.d/default.conf && /usr/bin/supervisord -c /etc/supervisord.conf
          command:
            - /bin/sh
            - -c
          env:
            - name: AUTH_AUDIENCE
              value: netbird
            - name: AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: AUTH_CLIENT_ID
              value: netbird
            - name: AUTH_SUPPORTED_SCOPES
              value: openid profile email offline_access netbird-api
            - name: USE_AUTH0
              value: "false"
            - name: NETBIRD_MGMT_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
            - name: NETBIRD_MGMT_GRPC_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
            - name: NGINX_SSL_PORT
              value: "443"
          image: netbirdio/dashboard:v2.19.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /
              port: http
          name: netbird-dashboard
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources: {}
          securityContext: {}
      securityContext: {}
      serviceAccountName: netbird-dashboard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: operator
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird
      app.kubernetes.io/name: kubernetes-operator
  template:
    metadata:
      labels:
        app.kubernetes.io/component: operator
        app.kubernetes.io/instance: netbird
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubernetes-operator
        app.kubernetes.io/version: 0.1.4
        helm.sh/chart: kubernetes-operator-0.1.13
    spec:
      containers:
        - args:
            - --metrics-bind-address=:8080
            - --leader-elect
            - --health-probe-bind-address=:8081
            - --webhook-cert-path=/tmp/k8s-webhook-server/serving-certs
            - --netbird-management-url=https://netbird.homelab.olav.ninja
            - --cluster-name=cluster
            - --cluster-dns=svc.cluster.local
            - --netbird-api-key=$(NB_API_KEY)
            - --netbird-client-image=netbirdio/netbird:0.59.2-rootless
          command:
            - /manager
          env:
            - name: NB_API_KEY
              valueFrom:
                secretKeyRef:
                  key: NB_API_KEY
                  name: netbird-operator-api-key
          image: docker.io/netbirdio/kubernetes-operator:0.1.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: kubernetes-operator
          ports:
            - containerPort: 443
              name: webhook-server
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          volumeMounts:
            - mountPath: /tmp/k8s-webhook-server/serving-certs
              name: webhook-certs
              readOnly: true
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: netbird-kubernetes-operator
      volumes:
        - name: webhook-certs
          secret:
            defaultMode: 420
            secretName: netbird-kubernetes-operator-tls
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 32.0.0
    helm.sh/chart: nextcloud-8.3.0
  name: nextcloud
  namespace: nextcloud
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: app
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: nextcloud
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204
        nextcloud-config-hash: a1d16d19d9e6a7542bf8cde75cecdc3357d062366aef5d2551e3b984a16915cf
        php-config-hash: ef6725a34482d427b584b8ddff54804f14e7c98827ae18ff7642f7cf0dd254af
      labels:
        app.kubernetes.io/component: app
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/name: nextcloud
        nextcloud-redis-client: "true"
    spec:
      containers:
        - env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  key: host
                  name: nextcloud-postgresql-app
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  key: dbname
                  name: nextcloud-postgresql-app
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-postgresql-app
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-postgresql-app
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
            - name: REDIS_HOST
              value: nextcloud-redis-master
            - name: REDIS_HOST_PORT
              value: "6379"
            - name: REDIS_HOST_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-redis-credentials
          image: nextcloud:32.0.0-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nextcloud
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
        - command:
            - /cron.sh
          env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  key: host
                  name: nextcloud-postgresql-app
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  key: dbname
                  name: nextcloud-postgresql-app
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-postgresql-app
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-postgresql-app
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
            - name: REDIS_HOST
              value: nextcloud-redis-master
            - name: REDIS_HOST_PORT
              value: "6379"
            - name: REDIS_HOST_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-redis-credentials
          image: nextcloud:32.0.0-apache
          imagePullPolicy: IfNotPresent
          name: nextcloud-cron
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
      securityContext:
        fsGroup: 33
      volumes:
        - name: nextcloud-main
          persistentVolumeClaim:
            claimName: nextcloud-data
        - configMap:
            name: nextcloud-config
          name: nextcloud-config
        - configMap:
            name: nextcloud-phpconfig
          name: nextcloud-phpconfig
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.12.2
    helm.sh/chart: ollama-1.30.0
    sablier.enable: "true"
    sablier.group: ollama-openwebui
  name: ollama
  namespace: ollama
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: ollama
      app.kubernetes.io/name: ollama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: ollama
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: ollama
        app.kubernetes.io/version: 0.12.2
        helm.sh/chart: ollama-1.30.0
    spec:
      containers:
        - args: null
          env:
            - name: OLLAMA_HOST
              value: 0.0.0.0:11434
          envFrom: null
          image: ollama/ollama:0.12.2-rocm
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: ollama
          ports:
            - containerPort: 11434
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              amd.com/gpu: 1
            requests: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /root/.ollama
              name: ollama-data
      securityContext: {}
      serviceAccountName: ollama
      terminationGracePeriodSeconds: 120
      tolerations: null
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/controller: pingvinshare
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pingvinshare
    helm.sh/chart: app-template-4.3.0
    sablier.enable: "true"
    sablier.group: pingvinshare
  name: pingvinshare
  namespace: pingvinshare
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/controller: pingvinshare
      app.kubernetes.io/instance: pingvinshare
      app.kubernetes.io/name: pingvinshare
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/controller: pingvinshare
        app.kubernetes.io/instance: pingvinshare
        app.kubernetes.io/name: pingvinshare
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: TRUST_PROXY
              value: "true"
          image: ghcr.io/stonith404/pingvin-share:v1.13.0
          livenessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 3
            periodSeconds: 10
          name: app
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 3
            periodSeconds: 10
          resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 5m
              memory: 256Mi
          volumeMounts:
            - mountPath: /opt/app/config.yaml
              name: config
              subPath: config.yaml
            - mountPath: /data
              name: data
      dnsPolicy: ClusterFirst
      enableServiceLinks: false
      hostIPC: false
      hostNetwork: false
      hostPID: false
      initContainers:
        - command:
            - sh
            - -c
            - |
              apk add --no-cache gettext
              echo "Processing config file..."
              envsubst < /config-template/config.yaml > /config/config.yaml
              echo "Config file processed successfully"
          env:
            - name: OIDC_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: pingvinshare-oidc-credentials
          image: alpine:3.22
          name: config-processor
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /config-template
              name: config-template
              readOnly: true
            - mountPath: /data
              name: data
      serviceAccountName: default
      volumes:
        - emptyDir: {}
          name: config
        - configMap:
            name: pingvinshare-config
          name: config-template
        - name: data
          persistentVolumeClaim:
            claimName: pingvinshare
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: pubip-operator
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pubip-operator
    app.kubernetes.io/version: v0.0.1
    control-plane: controller
    helm.sh/chart: v0.0.1
  name: pubip-operator-controller
  namespace: pubip-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: pubip-operator
      app.kubernetes.io/name: pubip-operator
      control-plane: controller
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        app.kubernetes.io/instance: pubip-operator
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: pubip-operator
        app.kubernetes.io/version: v0.0.1
        control-plane: controller
        helm.sh/chart: v0.0.1
    spec:
      containers:
        - args:
            - --leader-elect
            - --metrics-bind-address=:8443
            - --health-probe-bind-address=:8081
          command:
            - /manager
          image: ghcr.io/olav-st/pubip-operator-controller:v0.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          name: manager
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits:
              cpu: 500m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: pubip-operator-controller
      terminationGracePeriodSeconds: 10
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: sablier-sablier
  name: sablier-sablier
  namespace: sablier
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sablier-sablier
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: sablier-sablier
    spec:
      containers:
        - args:
            - start
            - --provider.name=kubernetes
            - --logging.level=trace
          image: sablierapp/sablier:1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 10000
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: sablier
          ports:
            - containerPort: 10000
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 10000
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
      serviceAccountName: sablier-sablier
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: sealed-secrets
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: sealed-secrets
        app.kubernetes.io/version: 0.31.0
        helm.sh/chart: sealed-secrets-2.5.19
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: sealed-secrets
                    app.kubernetes.io/name: sealed-secrets
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - args:
            - --key-prefix
            - sealed-secrets-key
            - --update-status
            - --key-renew-period
            - "0"
          command:
            - controller
          image: docker.io/bitnami/sealed-secrets-controller:0.31.0-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: sealed-secrets
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: sealed-secrets
      volumes:
        - emptyDir: {}
          name: empty-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-37.1.2
  name: traefik
  namespace: traefik
spec:
  minReadySeconds: 0
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: traefik-traefik
      app.kubernetes.io/name: traefik
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9100"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: traefik-traefik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: traefik
        helm.sh/chart: traefik-37.1.2
    spec:
      automountServiceAccountToken: true
      containers:
        - args:
            - --entryPoints.metrics.address=:9100/tcp
            - --entryPoints.ssh.address=:2222/tcp
            - --entryPoints.traefik.address=:8080/tcp
            - --entryPoints.web.address=:8000/tcp
            - --entryPoints.webpublic.address=:9443/tcp
            - --entryPoints.websecure.address=:8443/tcp
            - --entryPoints.websecure.asDefault=true
            - --api.dashboard=true
            - --ping=true
            - --metrics.prometheus=true
            - --metrics.prometheus.entrypoint=metrics
            - --experimental.plugins.sablier.moduleName=github.com/sablierapp/sablier
            - --experimental.plugins.sablier.version=v1.10.1
            - --providers.kubernetescrd
            - --providers.kubernetescrd.allowEmptyServices=true
            - --providers.kubernetesingress
            - --providers.kubernetesingress.allowEmptyServices=true
            - --providers.kubernetesingress.ingressendpoint.publishedservice=traefik/traefik
            - --entryPoints.webpublic.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.webpublic.http.tls=true
            - --entryPoints.webpublic.transport.respondingTimeouts.readTimeout=0
            - --entryPoints.websecure.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.websecure.http.tls=true
            - --entryPoints.websecure.transport.respondingTimeouts.readTimeout=0
            - --log.level=INFO
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: USER
              value: traefik
          image: docker.io/traefik:v3.5.3
          imagePullPolicy: IfNotPresent
          lifecycle: null
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
            - containerPort: 9100
              name: metrics
              protocol: TCP
            - containerPort: 2222
              name: ssh
              protocol: TCP
            - containerPort: 8080
              name: traefik
              protocol: TCP
            - containerPort: 8000
              name: web
              protocol: TCP
            - containerPort: 9443
              name: webpublic
              protocol: TCP
            - containerPort: 8443
              name: websecure
              protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: null
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /tmp
              name: tmp
            - mountPath: /plugins-storage
              name: plugins
      hostNetwork: false
      securityContext:
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: traefik
      terminationGracePeriodSeconds: 60
      volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: plugins
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-primary
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey
  serviceName: gitea-valkey-headless
  template:
    metadata:
      annotations:
        checksum/configmap: c8cdc0c4c772ac3192446a08bf401c29f1e39f33614657d36e4bb1692e34b39f
        checksum/health: e3a0f06458110f02986bb8df4391c43567355d3582dd02f25447184391196fbc
        checksum/scripts: 791de9ea7b477455268cda3e85c8e58a1505e8d19ff8e851d78a246cb185a0f7
        checksum/secret: b6a1ec38f338ddc2549686770cd644719aa49f186495fe8a75c2d5a761d66bf9
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: valkey
        app.kubernetes.io/version: 8.1.3
        helm.sh/chart: valkey-3.0.31
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: valkey
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-primary.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: VALKEY_REPLICATION_MODE
              value: primary
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: VALKEY_PASSWORD_FILE
              value: /opt/bitnami/valkey/secrets/valkey-password
            - name: VALKEY_TLS_ENABLED
              value: "no"
            - name: VALKEY_PORT
              value: "6379"
          image: docker.io/bitnami/valkey:8.1.3-debian-12-r3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: valkey
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /opt/bitnami/valkey/secrets/
              name: valkey-password
            - mountPath: /data
              name: valkey-data
            - mountPath: /opt/bitnami/valkey/mounted-etc
              name: config
            - mountPath: /opt/bitnami/valkey/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: gitea-valkey-primary
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: gitea-valkey-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: gitea-valkey-health
          name: health
        - name: valkey-password
          secret:
            items:
              - key: valkey-password
                path: valkey-password
            secretName: gitea-valkey
        - configMap:
            name: gitea-valkey-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: valkey-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  serviceName: immich-redis-headless
  template:
    metadata:
      annotations:
        checksum/configmap: 2a9ab4a5432825504d910f022638674ce88eaefe9f9f595ad8bc107377d104fb
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: bdec350b84a1ace7cc118db113a21c9f160bde3425b07714e1c0c1da722621cf
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: immich
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.4.3
        helm.sh/chart: redis-20.13.2
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          image: docker.io/bitnami/redis:7.4.3-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /data
              name: redis-data
            - mountPath: /opt/bitnami/redis/mounted-etc
              name: config
            - mountPath: /opt/bitnami/redis/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: immich-redis-master
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: immich-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: immich-redis-health
          name: health
        - configMap:
            name: immich-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: redis-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloakx
    app.kubernetes.io/version: 26.3.3
    helm.sh/chart: keycloakx-7.1.3
  name: keycloak-keycloakx
  namespace: keycloak
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloakx
  serviceName: keycloak-keycloakx-headless
  template:
    metadata:
      annotations:
        checksum/config-startup: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/secrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloakx
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/component
                      operator: NotIn
                      values:
                        - test
                  matchLabels:
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: keycloakx
                topologyKey: topology.kubernetes.io/zone
              weight: 100
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: NotIn
                    values:
                      - test
                matchLabels:
                  app.kubernetes.io/instance: keycloak
                  app.kubernetes.io/name: keycloakx
              topologyKey: kubernetes.io/hostname
      containers:
        - command:
            - /opt/keycloak/bin/kc.sh
            - start
            - --http-port=8080
            - --hostname-strict=false
          env:
            - name: KC_HTTP_RELATIVE_PATH
              value: /
            - name: KC_CACHE
              value: ispn
            - name: KC_CACHE_STACK
              value: kubernetes
            - name: KC_PROXY_HEADERS
              value: xforwarded
            - name: KC_HTTP_ENABLED
              value: "true"
            - name: KC_DB
              value: postgres
            - name: KC_DB_URL_HOST
              value: keycloak-postgresql-rw
            - name: KC_DB_URL_PORT
              value: "5432"
            - name: KC_DB_URL_DATABASE
              value: keycloak
            - name: KC_DB_USERNAME
              value: keycloak
            - name: KC_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: keycloak-postgresql-app
            - name: KC_METRICS_ENABLED
              value: "true"
            - name: KC_HEALTH_ENABLED
              value: "true"
            - name: KC_BOOTSTRAP_ADMIN_USERNAME
              value: admin
            - name: KC_BOOTSTRAP_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: pazzword
                  name: keycloak-admin-password
            - name: KEYCLOAK_FRONTEND_URL
              value: https://keycloak.homelab.olav.ninja
            - name: KC_HOSTNAME
              value: keycloak.homelab.olav.ninja
            - name: JAVA_OPTS_APPEND
              value: -Djgroups.dns.query=keycloak-keycloakx-headless
          envFrom: null
          image: quay.io/keycloak/keycloak:26.3.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /health/live
              port: http-internal
              scheme: HTTP
            initialDelaySeconds: 0
            timeoutSeconds: 5
          name: keycloak
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 9000
              name: http-internal
              protocol: TCP
            - containerPort: 8443
              name: https
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /health/ready
              port: http-internal
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          resources:
            limits:
              memory: 4096Mi
            requests:
              cpu: 500m
              memory: 1024Mi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: http-internal
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 1
          volumeMounts: null
      enableServiceLinks: true
      initContainers:
        - command:
            - sh
            - -c
            - |
              echo 'Waiting for Database to become ready...'

              until printf "." && nc -z -w 2 keycloak-postgresql-rw 5432; do
                  sleep 2;
              done;

              echo 'Database OK ✓'
          image: docker.io/busybox:1.32
          imagePullPolicy: IfNotPresent
          name: dbchecker
          resources:
            limits:
              cpu: 20m
              memory: 32Mi
            requests:
              cpu: 20m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
      restartPolicy: Always
      securityContext:
        fsGroup: 1000
      serviceAccountName: keycloak-keycloakx
      terminationGracePeriodSeconds: 60
      volumes: null
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-master
  namespace: nextcloud
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: redis
  serviceName: nextcloud-redis-headless
  template:
    metadata:
      annotations:
        checksum/configmap: 2a9ab4a5432825504d910f022638674ce88eaefe9f9f595ad8bc107377d104fb
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 0717e77fd3bb941f602860e9be4f2ed87b481cddeadf37be463f8512ecde0c3e
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 8.0.1
        helm.sh/chart: redis-21.1.3
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -ec
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD_FILE
              value: /opt/bitnami/redis/secrets/redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          image: docker.io/bitnamilegacy/redis:8.0.1-debian-12-r1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /opt/bitnami/redis/secrets/
              name: redis-password
            - mountPath: /data
              name: redis-data
            - mountPath: /opt/bitnami/redis/mounted-etc
              name: config
            - mountPath: /opt/bitnami/redis/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: nextcloud-redis-master
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: nextcloud-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: nextcloud-redis-health
          name: health
        - name: redis-password
          secret:
            items:
              - key: password
                path: redis-password
            secretName: nextcloud-redis-credentials
        - configMap:
            name: nextcloud-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: redis-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: open-webui
    app.kubernetes.io/version: 0.6.32
    helm.sh/chart: open-webui-8.9.0
    sablier.enable: "true"
    sablier.group: ollama-openwebui
  name: open-webui
  namespace: openwebui
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: open-webui
      app.kubernetes.io/instance: openwebui
  serviceName: open-webui
  template:
    metadata:
      labels:
        app.kubernetes.io/component: open-webui
        app.kubernetes.io/instance: openwebui
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: open-webui
        app.kubernetes.io/version: 0.6.32
        helm.sh/chart: open-webui-8.9.0
    spec:
      automountServiceAccountToken: false
      containers:
        - env:
            - name: WEBUI_URL
              value: https://openwebui.homelab.olav.ninja
            - name: OLLAMA_BASE_URLS
              value: http://ollama.ollama.svc.cluster.local:11434
            - name: OPENAI_API_BASE_URL
              value: https://api.openai.com/v1
            - name: ENABLE_OPENAI_API
              value: "False"
            - name: ENABLE_OAUTH_SIGNUP
              value: "True"
            - name: OAUTH_MERGE_ACCOUNTS_BY_EMAIL
              value: "True"
            - name: OAUTH_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  key: clientId
                  name: openwebui-oidc-credentials
            - name: OAUTH_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: openwebui-oidc-credentials
            - name: OPENID_PROVIDER_URL
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: OAUTH_PROVIDER_NAME
              value: Keycloak
            - name: OAUTH_SCOPES
              value: openid email profile
          image: ghcr.io/open-webui/open-webui:0.6.32
          imagePullPolicy: IfNotPresent
          name: open-webui
          ports:
            - containerPort: 8080
              name: http
          tty: true
          volumeMounts:
            - mountPath: /app/backend/data
              name: data
      enableServiceLinks: false
      initContainers:
        - command:
            - sh
            - -c
            - cp -R -n /app/backend/data/* /tmp/app-data/
          image: ghcr.io/open-webui/open-webui:0.6.32
          imagePullPolicy: IfNotPresent
          name: copy-app-data
          volumeMounts:
            - mountPath: /tmp/app-data
              name: data
      serviceAccountName: open-webui
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: open-webui
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: energisk-cache-prime
  namespace: default
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - args:
                - |
                  URLS=" https://api.energisk.app/price?area=BE&currency=EUR https://api.energisk.app/price?area=BG&currency=BGN https://api.energisk.app/price?area=CH&currency=CHF https://api.energisk.app/price?area=CZ&currency=CZK https://api.energisk.app/price?area=DE&currency=EUR https://api.energisk.app/price?area=DK1&currency=DKK https://api.energisk.app/price?area=DK2&currency=DKK https://api.energisk.app/price?area=EE&currency=EUR https://api.energisk.app/price?area=ES&currency=EUR https://api.energisk.app/price?area=FI&currency=EUR https://api.energisk.app/price?area=FR&currency=EUR https://api.energisk.app/price?area=GR&currency=EUR https://api.energisk.app/price?area=HR&currency=EUR https://api.energisk.app/price?area=HU&currency=HUF https://api.energisk.app/price?area=IE&currency=EUR https://api.energisk.app/price?area=LT&currency=EUR https://api.energisk.app/price?area=LU&currency=EUR https://api.energisk.app/price?area=LV&currency=EUR https://api.energisk.app/price?area=NL&currency=EUR https://api.energisk.app/price?area=NO1&currency=NOK https://api.energisk.app/price?area=NO2&currency=NOK https://api.energisk.app/price?area=NO3&currency=NOK https://api.energisk.app/price?area=NO4&currency=NOK https://api.energisk.app/price?area=NO5&currency=NOK https://api.energisk.app/price?area=NORD&currency=EUR https://api.energisk.app/price?area=PL&currency=PLN https://api.energisk.app/price?area=PT&currency=EUR https://api.energisk.app/price?area=RO&currency=RON https://api.energisk.app/price?area=RS&currency=RSD https://api.energisk.app/price?area=SARD&currency=EUR https://api.energisk.app/price?area=SE1&currency=SEK https://api.energisk.app/price?area=SE2&currency=SEK https://api.energisk.app/price?area=SE3&currency=SEK https://api.energisk.app/price?area=SE4&currency=SEK https://api.energisk.app/price?area=SI&currency=EUR https://api.energisk.app/price?area=SK&currency=EUR https://api.energisk.app/price?area=SICI&currency=EUR https://api.energisk.app/price?area=SUD&currency=EUR "
                  for URL in $URLS; do

                    curl -s -o /dev/null -w '%{url_effective}\t%{http_code}\t%{time_total}s\n' $URL\&date=$(date -d tomorrow +'%Y-%m-%d');
                  done
              command:
                - /bin/sh
                - -c
              image: rockylinux:9-minimal
              name: curl-container
          restartPolicy: Never
  schedule: 0 13 * * *
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: wakeup-deployments
  namespace: default
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - args:
                - |
                  urls="https://gitea.homelab.olav.ninja/ https://pingvin.homelab.olav.ninja/ https://openwebui.homelab.olav.ninja/"
                  for url in $urls; do
                    echo "Sending GET to $url"
                    curl -X GET "$url"
                  done
              command:
                - /bin/sh
                - -c
              image: curlimages/curl:latest
              name: curl-get
          restartPolicy: OnFailure
  schedule: 50 2 * * 6
---
apiVersion: batch/v1
kind: CronJob
metadata:
  labels:
    app.kubernetes.io/name: hubble-generate-certs
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-generate-certs
  name: hubble-generate-certs
  namespace: kube-system
spec:
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            k8s-app: hubble-generate-certs
        spec:
          affinity: null
          automountServiceAccountToken: true
          containers:
            - args:
                - --ca-generate=true
                - --ca-reuse-secret
                - --ca-secret-namespace=kube-system
                - --ca-secret-name=cilium-ca
                - --ca-common-name=Cilium CA
              command:
                - /usr/bin/cilium-certgen
              env:
                - name: CILIUM_CERTGEN_CONFIG
                  value: |
                    certs:
                    - name: hubble-server-certs
                      namespace: kube-system
                      commonName: "*.default.hubble-grpc.cilium.io"
                      hosts:
                      - "*.default.hubble-grpc.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - server auth
                      - client auth
                      validity: 8760h
                    - name: hubble-relay-client-certs
                      namespace: kube-system
                      commonName: "*.hubble-relay.cilium.io"
                      hosts:
                      - "*.hubble-relay.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - client auth
                      validity: 8760h
              image: quay.io/cilium/certgen:v0.2.4@sha256:de7b97b1d19a34b674d0c4bc1da4db999f04ae355923a9a994ac3a81e1a1b5ff
              imagePullPolicy: IfNotPresent
              name: certgen
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
          hostNetwork: false
          restartPolicy: OnFailure
          securityContext:
            seccompProfile:
              type: RuntimeDefault
          serviceAccount: hubble-generate-certs
          serviceAccountName: hubble-generate-certs
      ttlSecondsAfterFinished: 1800
  schedule: 0 0 1 */4 *
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey-primary
  namespace: gitea
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis-master
  namespace: nextcloud
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: redis
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.31.0
    helm.sh/chart: sealed-secrets-2.5.19
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
---
apiVersion: apiextensions.crossplane.io/v1
kind: CompositeResourceDefinition
metadata:
  name: xoidcclients.oidc.homelab.olav.ninja
  namespace: crossplane
spec:
  group: oidc.homelab.olav.ninja
  names:
    kind: XOidcClient
    plural: xoidcclients
  versions:
    - name: v1alpha1
      referenceable: true
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                baseUrl:
                  description: Default URL to use when the auth server needs to redirect or link back to the client.
                  type: string
                clientId:
                  description: The Client ID for this client, referenced in the URI during authentication and in issued tokens
                  type: string
                clientSecretSecretRef:
                  description: |-
                    The client or client secret registered within the identity provider. This field is able to obtain its value from vault, use $${vault.ID} format.
                    Client Secret.
                  properties:
                    key:
                      description: The key to select.
                      type: string
                    name:
                      description: Name of the secret.
                      type: string
                    namespace:
                      description: Namespace of the secret.
                      type: string
                  required:
                    - key
                    - name
                    - namespace
                  type: object
                defaultScopes:
                  description: The default scopes to be requested when asking for authorization
                  items:
                    type: string
                  type: array
                description:
                  description: The description of this client in the GUI
                  type: string
                displayName:
                  description: The display name of this client in the GUI
                  type: string
                grantTypes:
                  description: A list of grant types that should be enabled for the client
                  items:
                    type: string
                  type: array
                groupsClaim:
                  description: If set, this claim will be populated with the Keycloak groups that the user is a member of
                  type: string
                postLogoutRedirectUris:
                  description: A list of valid URIs a browser is permitted to redirect to after a successful logout.
                  items:
                    type: string
                  type: array
                realm:
                  description: The realm this client is attached to
                  type: string
                redirectUris:
                  description: |-
                    A list of valid URIs a browser is permitted to redirect to after a successful login or logout. Simple
                    wildcards in the form of an asterisk can be used here. This attribute must be set if either standard_flow_enabled or implicit_flow_enabled
                    is set to true.
                  items:
                    type: string
                  type: array
                serviceAccountRoles:
                  description: A list of roles to assign to the clients service account
                  items:
                    properties:
                      client:
                        type: string
                      realm:
                        type: string
                      role:
                        type: string
                    type: object
                  type: array
                type:
                  description: Specifies the type of client
                  type: string
                webOrigins:
                  description: |-
                    A list of allowed CORS origins. To permit all valid
                    redirect URIs, add +. Note that this will not include the *
                    wildcard. To permit all origins, explicitly add *.
                  items:
                    type: string
                  type: array
              required:
                - clientId
                - realm
              type: object
          required:
            - spec
          type: object
      served: true
---
apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: keycloak-oidc-client
  namespace: crossplane
spec:
  compositeTypeRef:
    apiVersion: oidc.homelab.olav.ninja/v1alpha1
    kind: XOidcClient
  mode: Pipeline
  pipeline:
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: Client
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}
            spec:
              forProvider:
                name: {{ .observed.composite.resource.spec.displayName }}
                accessType: {{ .observed.composite.resource.spec.type }}
                clientId: {{ .observed.composite.resource.spec.clientId }}
                {{ with .observed.composite.resource.spec.clientSecretSecretRef }}
                clientSecretSecretRef: {{ toYaml . | nindent 6 }}
                {{ end }}
                description: {{ .observed.composite.resource.spec.description }}
                {{ with .observed.composite.resource.spec.baseUrl }}
                baseUrl: {{ . }}
                {{ end }}
                {{ with .observed.composite.resource.spec.redirectUris }}
                validRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.postLogoutRedirectUris }}
                validPostLogoutRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.webOrigins }}
                webOrigins: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ if has "client_credentials" .observed.composite.resource.spec.grantTypes }}
                serviceAccountsEnabled: true
                {{ end }}
                {{ if has "code" .observed.composite.resource.spec.grantTypes }}
                standardFlowEnabled: true
                {{ end }}
                {{ if has "device_code" .observed.composite.resource.spec.grantTypes }}
                oauth2DeviceAuthorizationGrantEnabled: true
                {{ end }}
                {{ if has "password" .observed.composite.resource.spec.grantTypes }}
                directAccessGrantsEnabled: true
                {{- end }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
        kind: GoTemplate
        source: Inline
      step: create-client
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-audience-mapper
            spec:
              forProvider:
                name: Set token audience
                protocol: openid-connect
                protocolMapper: oidc-audience-mapper
                config:
                  included.client.audience: "{{ .observed.composite.resource.spec.clientId }}"
                  id.token.claim: "false"
                  access.token.claim: "true"
                  introspection.token.claim: "true"
                  userinfo.token.claim: "false"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-audience-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-sub-mapper
            spec:
              forProvider:
                name: Username as 'sub' claim
                protocol: openid-connect
                protocolMapper: oidc-usermodel-property-mapper
                config:
                  user.attribute: username
                  id.token.claim: "true"
                  access.token.claim: "true"
                  claim.name: sub
                  userinfo.token.claim: "true"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-usermodel-property-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            {{ if .observed.composite.resource.spec.groupsClaim }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-groups-mapper
            spec:
              forProvider:
                name: Groups as '{{ .observed.composite.resource.spec.groupsClaim }}' claim
                protocol: openid-connect
                protocolMapper: oidc-group-membership-mapper
                config:
                  claim.name: {{ .observed.composite.resource.spec.groupsClaim }}
                  full.path: 'false'
                  id.token.claim: 'true'
                  access.token.claim: 'true'
                  lightweight.claim: 'false'
                  userinfo.token.claim: 'true'
                  introspection.token.claim: 'true'
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-group-membership-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            {{ if .observed.composite.resource.spec.defaultScopes }}
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientDefaultScopes
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-default-scopes
            spec:
              forProvider:
                {{ with .observed.composite.resource.spec.defaultScopes }}
                defaultScopes: {{ toYaml . | nindent 6 }}
                {{ end }}
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-client-default-scopes
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ range .observed.composite.resource.spec.serviceAccountRoles }}
            ---
            apiVersion: meta.gotemplating.fn.crossplane.io/v1alpha1
            kind: ExtraResources
            requirements:
              client:
                apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
                kind: Client
                matchName: {{ .client }}
              realm:
                apiVersion: realm.keycloak.crossplane.io/v1alpha1
                kind: Realm
                matchName: {{ .realm }}
            {{ end }}
            {{ if and (ne .observed.resources nil) (ne .extraResources nil) }}
            {{ range $i, $serviceAccountRole := .observed.composite.resource.spec.serviceAccountRoles }}
            {{ $client := (index (index $.extraResources "client").items $i).resource }}
            {{ $realm := (index (index $.extraResources "realm").items $i).resource }}
            ---
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientServiceAccountRole
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ $.observed.composite.resource.metadata.name }}-{{ .role }}
            spec:
              forProvider:
                clientId: {{ $client.status.atProvider.id }}
                realmId: {{ $realm.status.atProvider.id }}
                role: {{ $serviceAccountRole.role }}
                serviceAccountUserId: {{ ( index $.observed.resources $.observed.composite.resource.metadata.name ).resource.status.atProvider.serviceAccountUserId | default "null" }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-service-account-role
    - functionRef:
        name: function-auto-ready
      step: automatically-detect-ready-composed-resources
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: amdgpu-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: amdgpu-dp-ds
  template:
    metadata:
      labels:
        name: amdgpu-dp-ds
    spec:
      containers:
        - image: rocm/k8s-device-plugin
          name: amdgpu-dp-cntr
          securityContext:
            capabilities:
              drop:
                - ALL
            privileged: true
          volumeMounts:
            - mountPath: /var/lib/kubelet/device-plugins
              name: dp
            - mountPath: /sys
              name: sys
      nodeSelector:
        kubernetes.io/arch: amd64
      priorityClassName: system-node-critical
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
      volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
          name: dp
        - hostPath:
            path: /sys
          name: sys
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-agent
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  template:
    metadata:
      annotations:
        cilium.io/cilium-configmap-checksum: 9d6ad9c79ac02258b8aecfad1f188a904d3888ce200450eb5f03fc28c3cca0e9
        kubectl.kubernetes.io/default-container: cilium-agent
      labels:
        app.kubernetes.io/name: cilium-agent
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
          command:
            - cilium-agent
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_CLUSTERMESH_CONFIG
              value: /var/lib/cilium/clustermesh/
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: KUBE_CLIENT_BACKOFF_BASE
              value: "1"
            - name: KUBE_CLIENT_BACKOFF_DURATION
              value: "120"
          image: quay.io/cilium/cilium:v1.18.2@sha256:858f807ea4e20e85e3ea3240a762e1f4b29f1cb5bbd0463b8aa77e7b097c0667
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                  - bash
                  - -c
                  - |
                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    # When running in AWS ENI mode, it's likely that 'aws-node' has
                    # had a chance to install SNAT iptables rules. These can result
                    # in dropped traffic, so we should attempt to remove them.
                    # We do it using a 'postStart' hook since this may need to run
                    # for nodes which might have already been init'ed but may still
                    # have dangling rules. This is safe because there are no
                    # dependencies on anything that is part of the startup script
                    # itself, and can be safely run multiple times per node (e.g. in
                    # case of a restart).
                    if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
                    then
                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
                    fi
                    echo 'Done!'
            preStop:
              exec:
                command:
                  - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
                - name: require-k8s-connectivity
                  value: "false"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - CHOWN
                - KILL
                - NET_ADMIN
                - NET_RAW
                - IPC_LOCK
                - SYS_ADMIN
                - SYS_RESOURCE
                - DAC_OVERRIDE
                - FOWNER
                - SETGID
                - SETUID
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 300
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /host/proc/sys/net
              name: host-proc-sys-net
            - mountPath: /host/proc/sys/kernel
              name: host-proc-sys-kernel
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
            - mountPath: /var/run/cilium/netns
              mountPropagation: HostToContainer
              name: cilium-netns
            - mountPath: /host/etc/cni/net.d
              name: etc-cni-netd
            - mountPath: /var/lib/cilium/clustermesh
              name: clustermesh-secrets
              readOnly: true
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /var/lib/cilium/tls/hubble
              name: hubble-tls
              readOnly: true
            - mountPath: /tmp
              name: tmp
      hostNetwork: true
      initContainers:
        - command:
            - cilium-dbg
            - build-config
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium:v1.18.2@sha256:858f807ea4e20e85e3ea3240a762e1f4b29f1cb5bbd0463b8aa77e7b097c0667
          imagePullPolicy: IfNotPresent
          name: config
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp
              name: tmp
        - command:
            - sh
            - -ec
            - |
              cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
              nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
              rm /hostbin/cilium-sysctlfix
          env:
            - name: BIN_PATH
              value: /opt/cni/bin
          image: quay.io/cilium/cilium:v1.18.2@sha256:858f807ea4e20e85e3ea3240a762e1f4b29f1cb5bbd0463b8aa77e7b097c0667
          imagePullPolicy: IfNotPresent
          name: apply-sysctl-overwrites
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - SYS_CHROOT
                - SYS_PTRACE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /hostproc
              name: hostproc
            - mountPath: /hostbin
              name: cni-path
        - args:
            - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
          command:
            - /bin/bash
            - -c
            - --
          image: quay.io/cilium/cilium:v1.18.2@sha256:858f807ea4e20e85e3ea3240a762e1f4b29f1cb5bbd0463b8aa77e7b097c0667
          imagePullPolicy: IfNotPresent
          name: mount-bpf-fs
          securityContext:
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              mountPropagation: Bidirectional
              name: bpf-maps
        - command:
            - /init-container.sh
          env:
            - name: CILIUM_ALL_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-state
                  name: cilium-config
                  optional: true
            - name: CILIUM_BPF_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-bpf-state
                  name: cilium-config
                  optional: true
            - name: WRITE_CNI_CONF_WHEN_READY
              valueFrom:
                configMapKeyRef:
                  key: write-cni-conf-when-ready
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/cilium:v1.18.2@sha256:858f807ea4e20e85e3ea3240a762e1f4b29f1cb5bbd0463b8aa77e7b097c0667
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_RESOURCE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              mountPropagation: HostToContainer
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
        - command:
            - /install-plugin.sh
          image: quay.io/cilium/cilium:v1.18.2@sha256:858f807ea4e20e85e3ea3240a762e1f4b29f1cb5bbd0463b8aa77e7b097c0667
          imagePullPolicy: IfNotPresent
          name: install-cni-binaries
          resources:
            requests:
              cpu: 100m
              memory: 10Mi
          securityContext:
            capabilities:
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-path
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      securityContext:
        appArmorProfile:
          type: Unconfined
        seccompProfile:
          type: Unconfined
      serviceAccountName: cilium
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - emptyDir: {}
          name: tmp
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /var/run/netns
            type: DirectoryOrCreate
          name: cilium-netns
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /sys/fs/cgroup
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - name: clustermesh-secrets
          projected:
            defaultMode: 256
            sources:
              - secret:
                  name: cilium-clustermesh
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: common-etcd-client.key
                    - key: tls.crt
                      path: common-etcd-client.crt
                    - key: ca.crt
                      path: common-etcd-client-ca.crt
                  name: clustermesh-apiserver-remote-cert
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: local-etcd-client.key
                    - key: tls.crt
                      path: local-etcd-client.crt
                    - key: ca.crt
                      path: local-etcd-client-ca.crt
                  name: clustermesh-apiserver-local-cert
                  optional: true
        - hostPath:
            path: /proc/sys/net
            type: Directory
          name: host-proc-sys-net
        - hostPath:
            path: /proc/sys/kernel
            type: Directory
          name: host-proc-sys-kernel
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: server.crt
                    - key: tls.key
                      path: server.key
                    - key: ca.crt
                      path: client-ca.crt
                  name: hubble-server-certs
                  optional: true
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium-envoy
    name: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium-envoy
  template:
    metadata:
      annotations: null
      labels:
        app.kubernetes.io/name: cilium-envoy
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium-envoy
        name: cilium-envoy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: cilium.io/no-schedule
                    operator: NotIn
                    values:
                      - "true"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium-envoy
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --
            - -c /var/run/cilium/envoy/bootstrap-config.json
            - --base-id 0
            - --log-level info
          command:
            - /usr/bin/cilium-envoy-starter
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium-envoy:v1.34.7-1757592137-1a52bb680a956879722f48c591a2ca90f7791324@sha256:7932d656b63f6f866b6732099d33355184322123cfe1182e6f05175a3bc2e0e0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-envoy
          ports:
            - containerPort: 9964
              hostPort: 9964
              name: envoy-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /var/run/cilium/envoy/artifacts
              name: envoy-artifacts
              readOnly: true
            - mountPath: /var/run/cilium/envoy/
              name: envoy-config
              readOnly: true
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      securityContext:
        appArmorProfile:
          type: Unconfined
      serviceAccountName: cilium-envoy
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - hostPath:
            path: /var/run/cilium/envoy/artifacts
            type: DirectoryOrCreate
          name: envoy-artifacts
        - configMap:
            defaultMode: 256
            items:
              - key: bootstrap-config.json
                path: bootstrap-config.json
            name: cilium-envoy-config
          name: envoy-config
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: multus
    name: multus
    tier: node
  name: kube-multus-ds
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: multus
  template:
    metadata:
      labels:
        app: multus
        name: multus
        tier: node
    spec:
      containers:
        - command:
            - /usr/src/multus-cni/bin/multus-daemon
          env:
            - name: MULTUS_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          image: ghcr.io/k8snetworkplumbingwg/multus-cni:snapshot-thick
          name: kube-multus
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /host/etc/cni/net.d
              name: cni
            - mountPath: /opt/cni/bin
              name: cnibin
            - mountPath: /host/run
              name: host-run
            - mountPath: /var/lib/cni/multus
              name: host-var-lib-cni-multus
            - mountPath: /var/lib/kubelet
              mountPropagation: HostToContainer
              name: host-var-lib-kubelet
            - mountPath: /run/k8s.cni.cncf.io
              name: host-run-k8s-cni-cncf-io
            - mountPath: /run/netns
              mountPropagation: HostToContainer
              name: host-run-netns
            - mountPath: /etc/cni/net.d/multus.d
              name: multus-daemon-config
              readOnly: true
            - mountPath: /hostroot
              mountPropagation: HostToContainer
              name: hostroot
            - mountPath: /etc/cni/multus/net.d
              name: multus-conf-dir
      hostNetwork: true
      hostPID: true
      initContainers:
        - command:
            - /usr/src/multus-cni/bin/install_multus
            - -d
            - /host/opt/cni/bin
            - -t
            - thick
          image: ghcr.io/k8snetworkplumbingwg/multus-cni:snapshot-thick
          name: install-multus-binary
          resources:
            requests:
              cpu: 10m
              memory: 64Mi
          securityContext:
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              mountPropagation: Bidirectional
              name: cnibin
      serviceAccountName: multus
      terminationGracePeriodSeconds: 10
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
      volumes:
        - hostPath:
            path: /var/run/netns/
          name: host-run-netns
        - hostPath:
            path: /etc/cni/net.d
          name: cni
        - hostPath:
            path: /opt/cni/bin
          name: cnibin
        - hostPath:
            path: /
          name: hostroot
        - configMap:
            items:
              - key: daemon-config.json
                path: daemon-config.json
            name: multus-daemon-config
          name: multus-daemon-config
        - hostPath:
            path: /run
          name: host-run
        - hostPath:
            path: /var/lib/cni/multus
          name: host-var-lib-cni-multus
        - hostPath:
            path: /var/lib/kubelet
          name: host-var-lib-kubelet
        - hostPath:
            path: /run/k8s.cni.cncf.io
          name: host-run-k8s-cni-cncf-io
        - hostPath:
            path: /etc/cni/multus/net.d
          name: multus-conf-dir
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: longhorn-manager
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-manager
  namespace: longhorn-system
spec:
  selector:
    matchLabels:
      app: longhorn-manager
  template:
    metadata:
      labels:
        app: longhorn-manager
        app.kubernetes.io/instance: longhorn
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: longhorn
        app.kubernetes.io/version: v1.10.0
        helm.sh/chart: longhorn-1.10.0
    spec:
      containers:
        - command:
            - longhorn-manager
            - -d
            - daemon
            - --engine-image
            - longhornio/longhorn-engine:v1.10.0
            - --instance-manager-image
            - longhornio/longhorn-instance-manager:v1.10.0
            - --share-manager-image
            - longhornio/longhorn-share-manager:v1.10.0
            - --backing-image-manager-image
            - longhornio/backing-image-manager:v1.10.0
            - --support-bundle-manager-image
            - longhornio/support-bundle-kit:v0.0.69
            - --manager-image
            - longhornio/longhorn-manager:v1.10.0
            - --service-account
            - longhorn-service-account
            - --upgrade-version-check
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          image: longhornio/longhorn-manager:v1.10.0
          imagePullPolicy: IfNotPresent
          name: longhorn-manager
          ports:
            - containerPort: 9500
              name: manager
            - containerPort: 9502
              name: admission-wh
            - containerPort: 9503
              name: recov-backend
          readinessProbe:
            httpGet:
              path: /v1/healthz
              port: 9502
              scheme: HTTPS
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /host/boot/
              name: boot
              readOnly: true
            - mountPath: /host/dev/
              name: dev
            - mountPath: /host/proc/
              name: proc
              readOnly: true
            - mountPath: /host/etc/
              name: etc
              readOnly: true
            - mountPath: /var/lib/longhorn/
              mountPropagation: Bidirectional
              name: longhorn
            - mountPath: /tls-files/
              name: longhorn-grpc-tls
        - command:
            - sh
            - -c
            - echo share-manager image pulled && sleep infinity
          image: longhornio/longhorn-share-manager:v1.10.0
          imagePullPolicy: IfNotPresent
          name: pre-pull-share-manager-image
      priorityClassName: longhorn-critical
      serviceAccountName: longhorn-service-account
      volumes:
        - hostPath:
            path: /boot/
          name: boot
        - hostPath:
            path: /dev/
          name: dev
        - hostPath:
            path: /proc/
          name: proc
        - hostPath:
            path: /etc/
          name: etc
        - hostPath:
            path: /var/lib/longhorn/
          name: longhorn
        - name: longhorn-grpc-tls
          secret:
            optional: true
            secretName: longhorn-grpc-tls
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 100%
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "1"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-startupapicheck
  namespace: cert-manager
spec:
  backoffLimit: 4
  template:
    metadata:
      labels:
        app: startupapicheck
        app.kubernetes.io/component: startupapicheck
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: startupapicheck
        app.kubernetes.io/version: v1.18.2
        helm.sh/chart: cert-manager-v1.18.2
    spec:
      containers:
        - args:
            - check
            - api
            - --wait=5m
            - -v
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-startupapicheck:v1.18.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-startupapicheck
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-startupapicheck
  ttlSecondsAfterFinished: 60
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-upgrade,post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  name: netbird-clusterip-expose
  namespace: netbird
spec:
  backoffLimit: 3
  template:
    metadata:
      name: netbird-clusterip-expose
    spec:
      containers:
        - args:
            - "# Get the network ID\nexport NETWORK_ID=$(kubectl get NBRoutingPeer -n netbird router -o 'jsonpath={.status.networkID}')\n\n# Find all services with the netbird.io/expose annotation set to true\nservices=$(kubectl get services --all-namespaces -o json | \\\n  jq -r '.items[] | select(.metadata.annotations[\"netbird.io/expose\"] == \"true\") | \n  \"\\(.metadata.namespace) \\(.metadata.name) \\(.spec.clusterIP) \\([.spec.ports[].port] | join(\" \"))\"')\n\nif [ -z \"$services\" ]; then\n  echo \"No services found with netbird.io/expose annotation set to true\"\n  exit 0\nfi\n\necho \"Found services to expose:\"\necho \"$services\"\n\n# Process each service\necho \"$services\" | while read -r namespace service_name cluster_ip ports; do\n  # Skip if any required field is empty or null\n  if [ -z \"$namespace\" ] || [ -z \"$service_name\" ] || [ -z \"$cluster_ip\" ] || [ \"$cluster_ip\" = \"null\" ]; then\n    echo \"Skipping service $service_name in namespace $namespace: missing required fields\"\n    continue\n  fi\n  \n  # Skip headless services (ClusterIP: None)\n  if [ \"$cluster_ip\" = \"None\" ]; then\n    echo \"Skipping headless service $service_name in namespace $namespace\"\n    continue\n  fi\n  \n  echo \"Processing service: $service_name in namespace: $namespace\"\n  \n  # Delete existing NBResource if it exists\n  kubectl delete NBResource --ignore-not-found -n \"$namespace\" \"${service_name}-${namespace}\" 2>/dev/null || true\n  \n  # Set environment variables for substitution\n  export SERVICE_NAME=\"$service_name\"\n  export SERVICE_NAMESPACE=\"$namespace\"\n  export CLUSTER_IP=\"$cluster_ip\"\n  export SERVICE_PORTS=$(echo \"$ports\" | tr ' ' ',')\n  \n  # Create NBResource\n  echo \"Creating NBResource for $service_name with IP $cluster_ip and ports $ports\"\n  echo \"$NBRESOURCE_TEMPLATE\" | envsubst | kubectl apply -f -\n  \n  if [ $? -eq 0 ]; then\n    echo \"Successfully created NBResource for $service_name\"\n  else\n    echo \"Failed to create NBResource for $service_name\"\n  fi\ndone\n"
          command:
            - bash
            - -c
          env:
            - name: NBRESOURCE_TEMPLATE
              value: |
                apiVersion: netbird.io/v1
                kind: NBResource
                metadata:
                  finalizers:
                  - netbird.io/cleanup
                  name: ${SERVICE_NAME}-clusterip
                  namespace: ${SERVICE_NAMESPACE}
                spec:
                  address: ${CLUSTER_IP}
                  groups:
                    - cluster
                  name: ${SERVICE_NAME}-clusterip
                  networkID: ${NETWORK_ID}
                  policyName: cluster
                  tcpPorts: [${SERVICE_PORTS}]
          image: bitnami/kubectl:latest
          name: apply-nbresource
      initContainers:
        - args:
            - kubectl wait --for 'jsonpath={.status.networkID}' -n netbird nbroutingpeer router;
          command:
            - bash
            - -c
          image: bitnami/kubectl:latest
          name: wait-network-ready
      restartPolicy: Never
      serviceAccountName: netbird-kubernetes-operator
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: operator
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-delete-policies
  namespace: netbird
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app.kubernetes.io/component: operator
        app.kubernetes.io/instance: netbird
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubernetes-operator
        app.kubernetes.io/version: 0.1.4
        helm.sh/chart: kubernetes-operator-0.1.13
      name: netbird-kubernetes-operator
    spec:
      containers:
        - args:
            - delete
            - --all
            - --cascade=foreground
            - --ignore-not-found
            - NBPolicy
          image: bitnami/kubectl:latest
          name: pre-delete
      restartPolicy: Never
      serviceAccountName: netbird-kubernetes-operator
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: operator
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-delete-routers
  namespace: netbird
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app.kubernetes.io/component: operator
        app.kubernetes.io/instance: netbird
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubernetes-operator
        app.kubernetes.io/version: 0.1.4
        helm.sh/chart: kubernetes-operator-0.1.13
      name: netbird-kubernetes-operator
    spec:
      containers:
        - args:
            - delete
            - --all
            - -A
            - --cascade=foreground
            - --ignore-not-found
            - NBRoutingPeer
          image: bitnami/kubectl:latest
          name: pre-delete
      restartPolicy: Never
      serviceAccountName: netbird-kubernetes-operator
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-upgrade,post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: operator
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-kubernetes-service-expose
  namespace: netbird
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app.kubernetes.io/component: operator
        app.kubernetes.io/instance: netbird
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubernetes-operator
        app.kubernetes.io/version: 0.1.4
        helm.sh/chart: kubernetes-operator-0.1.13
      name: netbird-kubernetes-operator
    spec:
      containers:
        - args:
            - kubectl delete NBResource --ignore-not-found -n default kubernetes; export NETWORK_ID=$(kubectl get NBRoutingPeer -n netbird router -o 'jsonpath={.status.networkID}'); echo "$NBRESOURCE_VALUE" | envsubst | kubectl apply -f -
          command:
            - bash
            - -c
          env:
            - name: NBRESOURCE_VALUE
              value: |
                apiVersion: netbird.io/v1
                kind: NBResource
                metadata:
                  finalizers:
                  - netbird.io/cleanup
                  name: kubernetes
                  namespace: default
                spec:
                  address: kubernetes.default.svc.cluster.local
                  groups:
                  - cluster
                  name: default-kubernetes-api
                  networkID: ${NETWORK_ID}
                  policyName: "cluster"
                  tcpPorts:
                  - 443
          image: bitnami/kubectl:latest
          name: apply-nbresource
      initContainers:
        - args:
            - kubectl wait --for 'jsonpath={.status.networkID}' -n netbird nbroutingpeer router;
          command:
            - bash
            - -c
          image: bitnami/kubectl:latest
          name: wait-network-ready
      restartPolicy: Never
      serviceAccountName: netbird-kubernetes-operator
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: cert-manager
spec:
  encryptedData:
    cloudflare-dns-api-token: AgAJ9ChXMAezXNfPI8Qhwn0OzJcKtw95ReqgVgLGFV8x3T4BeZtUTgEJGPBlUliwFJNGMCsQcnasTXe5lyPOiCRxFZk1t1ZrUY++K6ErzT1F74wN3nqQY/Sa1slTFidyAIvbWagOMIKgjPpnvXfqo+nl5IffReTSrahI3oDC6Z3AAAVyIvXhUquYeAuGwXds+CJOUWhZCogPxYQnJf5BZCBMLzg6HGtvOgAaOpNVEP7SG0N8yhIJRGv6k1Dc2mGqGMmOQf3Ok201iPq6NkZE+uC3aO9iQGilK54WKkY/pSaUna7YPZ3FjqHKLx7BgC1w2aWFOTVXHfGRUGwnP7mTOzkSpSVSSJahqGc2O2/Bw0R6QdIZm6NSLqKRGZ6tt9rKc5zxjYSk4AMvDE1v2KZ+2VuhPk+2090sHjYGi2cq7H2u0C4OJkIZU2qhxp8L75mw/dUhX2ZEp8CtnN/PQZATY4EOJB3S9jk3NbOV7tN/uHjhYNpFVTQOx0ZLkPpYtLA8GZibC+K+wqpdiPrrUKGsARoX2t7yEHhBjz/fWkyiV206rkCz9m55eTBAtJH2p7YKt2pdVKWHedJ8Lc5yXIpTpKFn9fbgEbrvgOTzia9V7iidizD1QDcLzLYM9Jf9uwj9+vldkGB6hn5Nx8sgdtR1p8JmbR1A24yIfJmSfAeDSsLIcutjNRkDKvK8TwwxbgZnzSNLWtAioxnDO24rPP58qLgVz8lNz6EVXVd5i5dU5I7BGFuyJlB9/9xb
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: cert-manager
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: coturn
spec:
  encryptedData:
    password: AgBqb41JE43pliaqXSNHlr73oVa2YULlDYLFmhnHWsFdAMGCAOrX/3KY1PJK1nSKILblkVlo4y4zYXk1sS1J/2+4uTUStd9zeqaUbaN145joLDKqIJy9UxUMh2G19idlJmhgZ4IXWYpUQIDKgXExZUmbqOmVrecPbGmTnSAAsR2HwW4N4RUW9CzH2ecbTUXuJ5jVzv1p3HhDRyN+A0nkm9TKZOQVTJm6QWNcLF6jk0mecTZZ7m5nVrCWlN2fToBWIaFEnLwB8jZMsJO0SSbHP6CLLtl+tefHVYZC+NkOP0/98kYzE8T2UoxOPlkKZW4m9hAGII3XzzkP6cy/9QA48p7sQne44IZ2QYLMHN9vJou3cn+uDQMY4SA+nYUcBTRzbWX2j2Gd8fzDFYMY5d7j4/XIEmKFI3HKfaMfvyPxKi6zSdFwrk0zGNvU+Km1hwix6rEK4n3UqxwizzdOM2rA5QFjXnH/JLidLCcxwZKWUyh1haiXZTWxqCoKj7W9xxQohNWwG7bV2Szobw4GP3utkmet5wKGe5ZN2buasre1cyMTa/VHr4FtPT23KGbIl5iaVtXO7VxisRwpCWTBJNA1EdbTibPtYVCWqXhM5vj0XOcM6qU3IlBgSGFXWL9JCLRFRiqCvR4OPGxT9VXxOI2AN04JSoYNEGFpOuV8HglANFrBwb54N0FeHgpZ1PNIXlV3Wp6x8IEyl3U7SXnB96O2jXpe
    username: AgCNW2mqiJHi1sqEj60miMBjc6b8XhzN4CVguWfch3mQQzOAY+zd2Mh57dHqmql7bht+oEOtBenxxfxKQQllY6HkyKXK7SWPPPSL0X20oheQEslNpGEYpkpaFBjyolO1Prrx2NM94oZxcf6rPeccXo0vApZ1E3YV1iIIWsdBDpoOZEA2avLb5jc6yIt8S9sbTGlzCf0F5UeVTM2WifnbRpr7WDU1GRgDID1v2dS8jQOtdu82YYD4ux/+I9fL4It+nyBREvfyUeV9TZphxG26xtQgPcXsMs6R+EwjOPbSxivep3ClQ+QbeLvMDl6LUdOeoh9Bx+MzJG+GWKgsLcpOCfRVh+nS8od8XKQRhGrPwOdyVbzGbStVhoIK3OomFyQt7qokaRMXkkVbmsb4vOx25Y2GYcdThB6ERadpzsVA2dcyoa1yvrYDn9AREBzct6gT8sprT89JTUNC7YdbaS0FTedvzHEvF5INpdj7yVoMgp5Zz1Rh5PEYYelpHcLg6Cm49Og4XkjdDVtrxvXr73HKonftioktFi/uIRg168yo+QRedTnqNa0vlY27tnrwuDFp/nxfN5X4QBLBmwhEAEJG4DfpZUiYRrCd4sStOWP61MdoROE7TaFzs9TU70mLEP4fcJL2zXC5LWeU3C+alvsp6c55lov8NtaBFox2z76ug6zTipTutWuenqLZNvpaDtEq4B/Y8oV+Yjqc
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: coturn
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-key
  namespace: external-dns
spec:
  encryptedData:
    apiKey: AgB9xOER45zd+vqDjKi9npBwC+kgiMMcGeMFShpbFoWE9bm/6Mcyg5logppoE/HYeqPTV8EyuMCQTXTC9kuEMo9htBrSa+9e3YtjJ1TWt4bQr3sy5LtWwWh+S8aL+ZVMwhsrnGHwMQFGNq07008pwSFJMkSPhZewqB4YEY5Gvkok9jgKyOfhelihv0OoJd2XZm4VQvD8r+7IKI/GTVFJ/Ih80G6V9YLEXj2jeUspFp+qhI84SE22ODZMjRFhpBDKqiZgQGWrVx5IJh7WV/aBb76uzbl2aD6v8MjbEWaPC380XLDv5uPKi1tOmXexeQWZhrw6JYINsgVXrfvTgn9uvJ07RG2kMvhppCEkhG1fXD+LdWCyFWjQJVl/NofWyhu2LrSowQBbqSEzRbwv7ZVE/IrosD2rCGyqstPBWev7fF0rAnd8UvESm8UodttvDNLWiUQtMFiHTc/h7aI/m/rQm8mv80GsyH7O9cYU788dZ0zU1DGWjn5nlOhh9amM4OeaP5oYyFwfNJLZDMy4AnrQ2fSdUTEKKA2gm0U9sTlVYvJ5lVjH49mpEtUm/V2w+glbjK9Y+YrKAav0z1hkcke5VUNYu2C9M1Yf1X2AW0aWtwEXwRFJDp6dR+eCNcCc7Ka2VOz/DG4pY4rMj9GraaRnPxVzdDvWmIKSyPIGkvf+HDKiiAKHdtKITWwNMEpJ3b9nkoU7rYbg/eeQnhx8tqM2iynYLsvALuusEg5CxI3VCvBBn0B7W3Aj6Kej
    email: AgBV3ulEZ4JeK7Nky2N578bBkCfowM1TG4tCVlkuaDV/TeDLGH/868LvR2gF9Lo71X5wZ9OLuAYssLWGGgJ2/qAery0pXVA8ay9DBNwkWz1jjntOEMDru2ewc3VY2ZLgqvr5y9cRz/svMNoJPxZHCe9SavstQ3hWnJqhPCN5pzWTaD3WYGA7DeWGZN4j9M9SwVRyfE657dnKJiY2vkBeI5wa56pTEihLxHX12ibYp8fS4MTUz8repKF4tZ9LcM++X3N5uUEeJcUIrrFL7Bt6KiLr4mUqBOuPheOabloEG1MaiO7TK/PtCeddR1VAMikwp9ZmReu8vS1C+9MULeHxqJ11UUY/aW9GjsiUTf4PlXg1+W7Yq3BA5BG7mmhSFDZQxSm3QmoCdpA43cCgF5JLcUql02gn4XlFObmHn1pSPzeTQCzOsqmbwpLeAxy5RbmczJtasA/EtE17IgibwigYKXssCSMI1nEDqT9ueU9ktmyEy1rkhxX5oNpyhxxB1hcegPNStGN3P9KH6rcVdw+9mTc/KoIT8Kc8McW8MkF8G2esKR6p4MQqVNbNSK194RVw1tnGFjz0/HQHniXHmtXT1dTCxRlxf78X4dVc7qW7HfFVCCyjWEBgwo8/6QpU4PpuewFnSmsxfYS1v1RPBi0+2/IZCqopp8ffakeUWNG9tZeVgQE1p8uLD82KbMCHV4LCg9D6VLrPiFded6si7K5hK+GfC1Uz
  template:
    metadata:
      name: cloudflare-api-key
      namespace: external-dns
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-admin-user
  namespace: gitea
spec:
  encryptedData:
    password: AgCGBjrd5MQeC0yx5epmouyLnJ6+5cZkaCiyXlyS4r+KSgvh1bSrtL7yyTI3LXh5ix7scx1j9cpzyRaGHVexM9LNM9XWqC7y7GRNJb3opCoDnSGcZ8PwBuLMTg4sxwM+fIQDq5oci4YyR+Jz8RUnslRCSBVr8qIE6xz1tMlpl16o0ni+vsb9q1AjuO9TTIhnctU/IJ54P4NQI7UYHBV29S7dLiY6R5JwYm992EIqFls6GPEiDGKKzHbphZnznTmlPENmM3fnC2UQzvoUlJ/sP1rbN9HM1gT08GCVnj/3M/W3fUb7dDAE7KAFnFAZSNhhQW2+8+5ipwndQy+CkrV1d3s+TE4rFvsuce87Dm9UTRXMB4brYQb480adZWIJJLxKR5fXy/cWeWOw+mO78FiY3szxgu8pSLUWndCBfRAFoDpuVSoQJJfNyL/9KTJI+IcubLHl+xboF4HpWnfKNRLix2JdYJCs4AmsJNAOcYBGa/fdnOdg6YQ7YhDRuQqlNflgA6UM4lta1mUMFBPdP55qTsdCnpYba0Z1uAWLS4EERVNTgPl0aGLlmmvYqsi8vWpu8JpoNiMFKtk8if0+ldSAQ0Z8ozgSGK7jwz0XWNmbhQLeyf1297qIaenWiNo8GACZbvPHvVOZ9At+xq4jLpMoQA4obMv0WBXpiohaHYq7lLTkFMyN1xFa8ABuguvDR8NjisuuB9tgxTY0zMDAdeQ1qkLKcioao5nxeA4Q
    username: AgA4Du7ge2bBzpz+hHKV3E74rVdFAVoGNs7Qts6Jmr0r3E4dNobiqyroocqsfHK65fOWCfnzBdYN1vKeVml66XG7GalcF21JvnYll9KC0qDRGO5qUW67rtZjG6F6BBV5CJTP4+1+VpNOcQxXdBNnj3WV6M78MhL1oYX1IeB/1tRJjDRgKcRJUc5vArSQAUHoykXq9zNwsib0G+J4EL7MaTfNEZgjpF5pgdb63f2TXiELEufCNzYisF5KyYsEa96NexnCKRowBWQ6TtiQunq4RoYZnxNBNvL03mYhEDpGH5DU/SNWdDRKQf9byaaKaGSgxKKks7a+vUBwYJDoWUd01tQOVN/uaEQwSJ/BazLoQjo244wxhu6yDWuZLfwc2VqlFsG53urT2uPwTv7sNhW5L2PUIWKgOaRhMAlaA9MWgwTfQYvvAOBmlV2GsgwY2IxRGiO0uSNeeAWS5cIhXuc+aZ8vNwj5CZR98WTtZ0E6mmyDmUpXZcakDE8W5enuYR6iejN555jlxK3U4/HvMKe9ywNXzG9LxqzLF40Tm55hH0nJHAFsin9aElfAmpeRY9UcmwIoNWfIHTM2RwFCm//S5GxYLt9uNwYPlfRg/DsREYy2qPNeXth05TbM1IND5ho0Qb6WLB3f0MM39N9YdPsv+nCDPr+Xs8uF/E0tHgowj0LdDgfaPlX+eLOQReiChorjhs3lybBN
  template:
    metadata:
      name: gitea-admin-user
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-oidc-credentials
  namespace: gitea
spec:
  encryptedData:
    key: AgA+nsX7ujf+uzZ2N653HXfGosxZT7zuvHTW/8aZAZ6Ll/wtswLCHwx6G3Sr2u6m8L6AaxhSoUE/RO9NEhQs+xCcTc9J5ODI1lsvd0Qiupw7+EZMn+4EBsT/LjypXfpDvhM9DTbyX7hbJxR04ncJW9q518CHqoT5j4pkm5PLglbmocb85j3QUpBNOaNFAe1SnhwrIoleX1q9BjdeOGP5cj8e4zbaMHTUft3NQyJyi0ghZ2CASEngTQWIVNHbElm04F6xOhnL69YY1Gh+2pO8BJLKRvCE2JA7FgBcRBwvTkGt6q11tbtEaKEoFiMqdEQKdb9AnZa8XErvBljIaZmtIRTsmoU6DZTgBXaFDaEcOKvM/NuSdLUz2j1gKmWZSJ6gMnI9blUxdaV8KHozygbBF0zj2d3eqXpmtI/atmXfbvRduuLLo1vhmaQ++P0vcHZyFlAIQTLJdCGwIt5hdK0SL6uChtYDqhYV+KebuFBmI1r1PKDaurqz+KViSIfK+kt152Mf/LVBq63foszybrDIpNyk2nqlYHJnCAeZ9HtvuZ0Zz5CtAl3KIeWFg1HlynTpNn1GJO/A8wdZRoAOr1JbX5REaoU18HS9n3X6Rrujm5SwydU26AbFT1rrJAVYAjUoufNV3e1HTulcwDjUdAAowvMwciKvpdhh48H2XlFOW3+MZOWBSo8ErVOa8/t9sSowzdqJd3Q+Mw==
    secret: AgB4vQvD4KBUOIkPCBL5+Fgb+jwbt2jvNcucCP0UFZrX3jN40bQxOqcElG1Z/6LZhoq2dh+amkpGIh3R/YKJLNeUsE1TIL9W3ThriO+Ng9AH/K6omywljIldSHsPRARjkAiuUKMlx1mEs7TRw1/YACmXKNw77KL5RO4NLtU7VgxyKcLe3JXIAexbNK6W3X8TkDqu9wx3eNduUet4Jt8jEbaMfINwexE0cH7NRVyQbLDT7i7MxJD1PisQWh6XH1ZGkLkxSxh+wYSHwIHSoMgQT3r4vxJ1rmvDCSEPMEBvB/3tLYYTGWAImffRIdnYHxWD/PDAaSqHqSMGeXtFnduv5SOYK4eSayz1p73IwcTvQc8ePUxgwS8uahN7UYRaoWem1Vs/uw63XBetm5PQX+wh/yvnK1NOdFAFIRn7Oh/iQhoy6kDIdzEndR9WZS0HW8IrMj8oFPTgJh1lRtWyXuhLiYiwsLcmMg6U2sPSJVRcnvxqdG+DqKjWL9WQjaP0EWZe3luole2CMfTkIBcPPJxMd9HJCgCcEdHl9Z5WZ3NPrOGb0DETOnTnn9hJPPlts3QJfqvyygNFqIUkSWE2zrDA4GWF2sMBE8bh8yQYGBD0COwxjvyWvfO0WlZ/63sjTwqeiV229MUGVNquD7BKvShGT71b8UFGfZG8rx5h1WiVnexCJ4Rth/u2eRaxptyD6NO0A43RAMoMJiDatL7ml/LMxfLyhDJvG368rYM=
  template:
    metadata:
      name: gitea-oidc-credentials
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  encryptedData:
    password: AgCG5I1alY74KUfK2r0gwZxaTrLD2py2Q7vBPPW+4LQg8KA+lTjYUEhN5BZLHzFRUtHW+KQVqqgxRlKOAsRZn55tgzzqxgmNxlipIuX1fGyb5Ds45azqT45nWfJlzf50r0WByMSjsdvVshkNjFjCXlPfwr2r73x+ZWqRv75Ra+6YsPHGWvkzglMQ8G04T+s2crb2PiqtOeSGsflJazLG5VAf8tddChGKsptPwate0UogTctFGHwhjMsTp/J2ivTqOh+VqPSknNuyJQfzYWHTrEHhNR4dNROy1lxFjA2/bbpUoezx4OHEmMB2iKmhVFycLPcxKG0IOykINLO11chia/sStL/3FQSJw0v99YqmgOl4g6HmkCiUSMQyULm2hdAsfbkgt6mGbSTqFkXS+uKClW5WKU0tzowXWQXZ4tCSC4ODZGDRuWbAsipClQLSknd9plh5FEzsrXu5Gujg+ALFDFef6lv6ukd6e9tOlAB68mMklXUVuas3a7JKSeZgYf4+JucmDyn1pDNpyT7KnOzP2GrQ77vTkfgay3QOZ4zh1CMw+5oxrdRbML0JcDvOTBraOKAu+nVwZCWK5nLmS81qj4fWRvqdiklw457cu9m9DZYJIgcp2OGKddyvyH/O8p5JqkmsRowd6LcmUt9F+/9ibnO3z/L9X1+pQkIfDnyBWdoGgIO1MSQzzsoqDdd+MDOxdZZTV9yYkQ==
    postgres-password: AgCfEmOIW7DTsRFmEUsqOAd1+ASfVPxQfZyIcrq1q9LVXX/FWY6dBTmLBpVRPLF1h3acLoUqqIPLGOK0wPgCbr9waEvM9HKsfhFqzTGvPIVid2r2tuga4u0pi0U6JqidcHROpyQCwE2NJrFiY2r3aGo5wstybYzHGDmTIAU8FfCTf38GzqgdneR3Gike9L9IoUmvEXzGu9edcQYinb5+yJ/wWp2+XA9936HmY8vTsjKlv2VWBiu5ZbWO3YRNYOvKesONDgvGh/3I9yagEqajjlpSEIAffDsXRnf16ihqGaxWk6KUD89yOtuiLo5BR9Z2KQmJ2jHB5T/666dZLMUGRgZBxf8G5wR64hSOaAoXI4HZ6xYYsmamGRrVExphzUNrkNu2wHmUwTW3LT8pXG1IyUAd0MZS8ZUlS3RdB6cyczYvhCuln2ER1iLXSPLjKy8HVhvDBwp8deGdvtgXK6USziDbYAaKvosYe5jZynLuEddCFJ55i+Kfk6hPotnnlltlIA5XA9ncK20L6r+tj6Y6CLYR+eShMAi4TS3d3vmmwcWdaqUevh6v+klpCCSJ8JyIP+YTBBl+0fWVNqQDqtAXHzFeU3IRl2MYCZ1pbaLQQgsORabT3tbYbZ1Ab0re56BnIDykw8zwylr2sOOq68laEYqmiXBKEc/9OZZNUFZx2BHE8cx4ewiTkXiCeH917XZRnC60Nlo2zn85v72N
  template:
    metadata:
      name: gitea-postgresql
      namespace: gitea
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-config
  namespace: immich
spec:
  encryptedData:
    immich-config.yaml: AgAA1uORMAGoH8uX44bzeylc/bxC39WkaKTzVBAsHzEem2iW+FzmGv5amYEBlgWnX8jjSGK4SS22g/n8DcL7GFsrgnrZNlJPUXzJQUD9w2DjjpVaIFrQHvjrnLgFLu9+od78RWXAWds85oiRX6LYCiqVc54Nyc5zbnQ8DHlvw+8VwfpitqWZA38cclJ+sDbB9hCLBPSEDsBlC37p8bQqjPoP2m68QhEpg4Kxrv40k7WxCvrNdNUCAiOh9hRdHvbCqEtleL0dk3aWJ3kGSf6p0rsVPl+cyxlfsFrFg74lN6gJKfgTV4j3mzv296wb1btDVN36JaOTpcdS4aVK10Kg53idBG4vX73eQZFRaPw7ZGi6u2A/c9I13etOMxrCX0y2Vxu2ZnAtnFdkAdlcsjhiX2LdjVYGzmT+ZqOlGp7lJd+gKX9CXhfrJlw3/2lQtRpqqWihIrrdkthm/4mAybayREV+Q/ojEXMmk20XkI6OLsP/ljZjMNA7vCB3tyLnbRfqHacpKbnVp/b1Jth6TOYZnvLhne5i0JmM6RCUiZXitJnxDzlQskFAUKoyBHorS8vYD8lYHqMv4iO2E4Qn38vHnSAaeQOH13Fe5tMJ+U/VPQ4ybawWCapUil9iuGTKwMQ4xbkpyN8z1KyP3y0RZf2w9NXoFeBZR/iSgGFUataEbpXWipGb66ieV0CAqqYGvLHFfk2kTbN9HMRKTcsClCaLOCFQKiMQLXNtxLsIsxPxrb1l1kI1xnW1H87upx1Y3SEzIHZdQgIeVmkVUFaVrFw5/WjqrdVZsxBxGkqvD6lKFY3X1srzcxh1A7LasSuoOJNVxVTFBr4LsQ7tNnT/rHaMZ4YSLwRJUQc6hnNK+4B4GaTX0190B3rion3vM3XsEqU8U3hpVNZupFnJe1Lwsp3Dom9nQga30WJjcD5SDGZQUtuPEjL5//A7uTMGrx4/B3ub2VAc+8LulG7dIAGSO7Ez0drVYdqcGoNQ+EPmY0WxO/CbYoD06QWbqQDgBD/f1ftERTwBGW0IQe6MEWqeTcgFdg==
  template:
    metadata:
      name: immich-config
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-oidc-credentials
  namespace: immich
spec:
  encryptedData:
    clientId: AgAvvCAj4sOLQgLcYpkSUKYld6dWCwEsTxtJ0BRjSOdKU29lh7l7XMkLcqiQJKV36PL/he0YKD1MW/jWcD5oys7ZHtaoDKAfCmTMR8eWGU3UcY+0w0ELn+G6TL0EkgKY6Gmc0JOyguwJmsilD8iyW021NknF+hj/pIak5iLxHOidPbRmNCWqS0b+iM0WLfLs+jrstLOYfs/VA/dftqZMogN5JLQ59edb9DU7Z5DRpZbaqmE7kgLuRR3t8Iiz300S2Tvhx8E2/2gKLQbyVMmZ60KjdtNln6HDhIjwxHsas3iWUXlgh1V2sIh9hDOBS4yWPoGDRYEAUdBJfGLFUYi3K2cQOC2oAAYaU0WTTDRGzQMugPDbKONO+fDVjKT0UJpiBwJ5aoNW244f7qUNDbeGJBM4iyQYit24fsHyL+4jbXjaoN/mjfZuN57h+QV6K5sH+bJOSl0OyH8gT+UNYsIZ/Er1w+AsNMdxB6Laf7rPnxlVsyy6bDLp8mRE8lPjz8f9EPGZIHTkcLTgNdsbDowG2RfvEft+UkRVuk0g8++VxQs9T9jzUgU1CsIESW0mDwl/K64rJm4y6G6wAoqCB4RyS0XHQ0+e0dqpponRKpokqzxZPgfnlay1Hc+L2DnAlD3kYsvAnQdECQMaQ93xh2/18Ym5BSQCBDB5RnElbgJBj0uE3eOJ4CMMftts3b+rFtVi+SOvpkXciAg=
    clientSecret: AgBj7B514brIE0r4g9UmqIo8gnBAHA1G/e/reQH3ATOUz2gAMGmC6VVXhfLam4Nwq3U3s+SwcU+jvVYAu8mmf/2ltUIxvBIfFnk5olM8QlE7eTVQAOdKLbhBmIWN5WhTgP4kIXawny9i1h8ft7anYVpI+PJoAQ0X8OiRbybsb+O8wWDFaJQ/lEwFAww3yNtiltCf3b7wJHI8pjvjJrtHeJRb13+T/sisSSh85QdQdbeSXBFGAo3waPHcbtaTVAmMs74ZTpDw56o9rP8hPtybSN67IcBa4lt/p3Z5OsVQgHa8Q1mHdzb/8wLDY+39UbHIZtxhH46MYDs7BL7G6ddTZUViXYBZRFKBdcFSs5tT2jeWR021stpKosRSgrEv3P5d3QDMYIdsK6FZw+lMAzaT7glqxiYLa6Ot0lYLU/fF1iDJ+76J4u1Py+BqHHGQV4qE+q4WTCXnAHlKzLtoV2MiaZN7NaPSfVl751BJFl1lgXbAievsgGPU0rYDn2q+ZS89QYi3yPqhN/3ICTbOE/iMaHhrEKkBBRmz6rllLqrLcTwJo5AWJAUqSyPXY6Jbm3akT/MJJ06Rj3IulZPN2At2ps3tz7+DVSES/MgP6GmpY9ZpIbiqR28rnNjzqRK3gZ0QNvJrScoeAy+uQTFXjRyRuW/8yNPnkYsMpqDe9cmB88lbV1XJzjtdtKKX4e26nWlJZqaT/S7AeoFdDDwLuRSHs2oZYVvFnosLRG8=
  template:
    metadata:
      name: immich-oidc-credentials
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-postgresql
  namespace: immich
spec:
  encryptedData:
    password: AgBg5CBTb0W5tXw8ibklTQP/1D/NEsliDeV/pHL1Pk39RFkUg3OB0pQbSe7ZVqcGHxyLaaTDr7o8NpDsAZnsJOH5vzpGBrddtkPJNwrSFxd2GAzGTI2SQH/hoQQfp27UpJzEqXiLIHw1rFTOTrJJayeWgwDHf26zc0oIBpbbQlu0o+26Vca7sArnGixX4iU5SS0HF4S2Hhk0CkL/dW18k/xrey+GlFdAtYcww2i5Cd6lxRTC5CIX3dQSqBWWcRy3Q4wOWH3zrQpkt96bSIlW9YrDD57dX7NAwCwrCx8pI6MdNAJEZpdjF7T6WAX+t5L1YzF7ecbtGvw4l3IavFKnvJZn1CCS95cI8xFrTw+igozYwMJB+oxre7E+q4vk20LTYuv+hSCeTIzX8t/5u0XqHW3/6XrqMNVOt8ZA0LIpABqqIgR/vEDsQPWwuehPwNvOhTJNKSBQRQi/SGtHUTkYCQwfLSPRpUV/MK++MaQin1nVy/Ch9dexVuWsR/cdffTwagzs4MPO1DDgfYNZe8sY9hPp3XUDjx03ayCT1rUA8Ymqajl2rRseCMuzdQnM33pizqOKXJys2DHWthr2xi0DCYd96dky/L92eHOC/tSQlyiYYjZGOUIMmHMdJpxkHUIXHJ/F1iTIyY4UMJMP3GkavggEk8WIImOebijYSuK1U1QJ/dlPuE4Q30YbhA7Ns5vj4Mu3IsAmAfg=
    postgres-password: AgAJAAmqRjNDaheSbzBtB39VjcDeSGEFyFnSmw7a3BlBBVqSuh2PXs01AVo483Qhy5VJQCHt/pkbnyw3bNn25OPmYZVdeoiwl6gZFwquY7fxpNANhvado6xeC0PFNhHAqX0M/U9UL5xPIfMRNc6JztvQaFosJD56EPiC3Mx76LrKc7hIDN8JxXDoba8NaUfQmvPeqX0bXucbDgDFv5yJmHWv4xtuTO6a/xwnt5uYn0nmC0ut07ntFeQSA4RqT2ptauwMy5SwM9JaR6fL8RUVQKX24Imk8s70XeVXE2mwdXu3w0fVx3PKlHcL2OwLumUsxhgW68ypeoHASR+B59mnJgMajYEQY+OwXwaaQNe1VAZyvfgwL4KUv7DvnwPgcUJNFzOowqmtfYzoydwKgtRboWlRvTR2sfWjtxgOXPxFLd/fKMtv1+CVbrRCivOGsBk7bG5k2QTraTuGCq3MlwMRm0vdhFVnOqZ79/Y9H6GZiFwxN+7W8w9wRKqRb7FmfLGgUbsf5UVqUpAqbHy82Uq3SvUwQ+f+rxBFN8RJVP2zDFiM8WBsywMU4Rvcl9SFcnsLTunaHalPMJw+AiflsT5X6hvrTo1S8xYujq2U2TltFTVK5SKRBrkmNVm+MWef1TmClN/2tFTqm0qy3qNrPZXdNzUVBpIyo7gB0EPuOJ6YXKKWI42LLavb12DdmDEsrK5VRTWcK0/RiMCH5e8G
  template:
    metadata:
      name: immich-postgresql
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-postgresql-user
  namespace: immich
spec:
  encryptedData:
    DB_DATABASE_NAME: AgAQgjOnZOKDyXwbFsgeZg1okBja90dNsjewAwfUY9XsdKTOPsDlzBjVOz5UmtyZ8kTzkCY6SDsBVmOxsxfcnvXlBjjBW2n/XmOtm7ftKGaEH9g48Lh9BCOfKey9dWrk04JhVwXwdM3lQtwQxlTwHQ1Ie5J72Ht4bK/ti78FbriynvjNwBFNwv6zyBuJY2Zb9EYyGMkIOxawPLSSkYIvzjVSfy8aRHZ8tZdqrL9PjBSn7q2T+EGC+AfhRGkntTySVmgSjfiQfdfH9goum08m41tNfM2JShBPfzMaBI1AQEHdN3EcPkLIE5frQSJ8Pak7am/w4LN7kB9IJQzMF1UrCe5CR5waOBBEczuh5Hlw5krPgCe7u4epV9h3BOk9h1mT6kyMBBNKFnRPvbgMdEG+/Il2oYv7q+Ql5tPIVIYrwdSlPDVAYSQOasQ+oL9/Y+ESRePr3CzlAHy+dCkBflWKW6nAq16VH0XLzElfqLuBZN8ag2kI5waXhO+12gPv3MbRxwL3N51LS4xuKQ7RsRsaxh3if/wSAaLzJEekagTBYSPPUv7CtbZR6ixLFwSRmpaC2IeXd2hXAlanjof76fO1vEnebQ8k8UVBdgEjwTC2STLEAYvICLcwIgTuPWxU/Rpra6tAGAa/PKF70UIKkP+lGrcc0/LsVcWLakCkL6MV6kAeNTYKMRBwkNsn0ra8T6MKS/wMYn4Mcgg=
    DB_PASSWORD: AgBXzZBsinnsjx/rMLcfGlthCWVqQT8QejjZewRPGyeX6/ksQhuNTt+YDKDQc/0wvJuOwQ35/dWJDIFTZRVyVlKFYQKtCzQuOWdDmwCarbgj4d+OH33d/J7iyJ68/jqRdT389mEnOVh2cCclym13yHD2N3ifd7MWt6TGOHEC6DE3GyNm9bUMyOcOtyBCYn95xBEeUwCPtLzZB+3POrlCaZZxKBVOtR1zkA9dt4naVE5xwtLXK51SkbhRAnurNCzVziCCUwgqdMikXWd7T5GQN8hGXqAlgnENPUqNrxeTd9K2yhLgeEdVcg36m3kG/S3+HzGMxBeNpXOd8XkOCOP1Jm0Yr8X6qfF0MTAmnmru/kaxAhvLMIJkHP8DwxE4q2taal54PgOuAX6uk1Va2zaFk95s/1wbvLsa5b+GZHdagaZSADT1uBJ57N59PZ3kfv4s7Pe3EOSfsuzSTsEsF6I+QPhrKV0uBVviewRcNdpGRHjAe+DAACxJvr2se/Qyzby5mJnMgawe6LHXyqVd+c4qcZ48/6hucQiXhOAhBSHG6pTLnDc/WLmatB3w+Pz9GEb+Rr7zQZRHG5ZxeFm7o9Z3eHPu3ywk2L0ziFXg1NJ7uNrYQXTmIK3AdXThgI0nYN2pn88iPYrMHATcpv9vqF1+HWb5NquaeD8V4YCcD4l0Rj11GLG4Lq0ZUWDFSrRo5/79v7XrC37UKEiT/gP6b/Mqn2ENAEfS6v8K
    DB_USERNAME: AgB1+8+LCEYsybi0bHdZEA0cSqzuuj0dqDLzxbVoNSZarVmrpb5IseK+4iUCLuB6K1dHI7qXMUjpKiZgM2oNQnJySNh/Epyz658orYaxM6EZz2GNIqQTwo6DIiIM8V7l2++N4fz2M/j2arBm4oHXeTJ0NJmh2KtEQ27y+kL+m9qvXoh2gGzY/LCYaqE7EvTtp284BiKC7TAJEfMO9qgQbvjXRouZXNW0p9XMksyZp7M5feZ/LXxXjkd10QIWFXrnj+nWtw713kZD6FBuvmPDITQZ70J35AeXp7Sdfz07rXHOmMWY0S6IHOxQ56k60+umP5ng+kiLciRKZoef43yTtm3wsG+X3n9ZWadZxBimwKgQkq7xLJAwDWgASg0J5bwkICmcGCU2cxv1J9RqT5y+6ItkQiZgk1BZIcBvhwt1whzUI3qGBetA+wDqTgSJMSpJgQZ7wV49SSG+Dv0gjDzLsDq6ntmkpYWjNloUNM2pc8toQW0H8yRY5ursvJnzfRlhHDlvx8t9YYEyazHDmI3cYSKmPcoJuL3OczhhO4qiPXgomgYzPrsbTdpEgsrQEArG9vaJDqH0LnKK73YxlKvlyg3kZUV1XUF0hivynQPs4MEYee5JWgsGiaVIAH12Gz1cDVJ3nH2YxhsqvlcAcXv2HBrsZU81Fgl29rwxSWjFBmITwcd+DC38fvbu9Fp/D6s4FAO+7Ig85/8=
    password: AgAGkMOzFeqF/aRBpZUDoq/6BNWr4tkf+MuFPQHCmsn9hvcOkCTksEK7p02C0Ru+7iLrtHF2KK+49WHjo4z3/QCAAnS2BmG2iZXkugPfp/8znMOgTQTg3VT5pw/YanCNcr50BB+a+3MFNXjMSpyNjitcWd+y9SzhCWsIFDYxRhMw5xrK6G+6cQUtdYzPXBlnxghPZBWBg5wiD57aA4SaSjz9LssiPbp1QtH6fW9JPkoNKpQkK+/3i5Ko+xMWigyDpcY1Qd74o67vRVGCQoHO8vxZ8A2lRHWYjW80xAHXUUzXNZUEefUfGQ7XJ8aTqZ21OBtSbzgn4gJFDFGRXpcqLcl9cTg7P1C0pcfIgclGT1kkDX+d+kPaqhxW9Ee1u0aiA69ynof/rGu0YnbzVLXNZlp0S5iVZb2mUPyLALwXmAPXuh5lsfHJ45t9TTiTKxb5ZAggXObHdl48bcM5qn3Ypyah2uU1/K4zdUd5rFdZkqyVTevEOYnGMD3hjasX++zCs5mT+SWY3/6x8bHePf0HUkfMdJKbFFkN97iIRAqP1PkOzJiZMcclssyiQkTnS2BMbUf/owMYzzezGeF5ZIT6X1n5i5QAtNvUQSaPqxVVDNgAsCccJKpUjC+Hx3t9M2kolHhkmDJCkV+2Plq3IYkkBzdo5ubpv5TCVpxXF9S5hqmvSqb0LzNAfTzgar/RShjsjctycfSBid5kvQEWRnQq5NhIlY7wkR3o
    username: AgASpZ4/NI10U0e3pa2zB2vfxXyA/aa8pSYyNwCSLcM9Kp8Ag+UxQXC8m6bcm9sWh4nEb0oM7IX8GPUOxSyf5bf31RNMAq4LagOY5jf9WBEuoYUpdoe2m9lKRBXkFXyd1Ip54VLjkWXGLjGV/buDPAm3ebUE6rBHQr5aTGbqe1eQ8WLkFc44rk7jjxLnejAuGiV2SjLXHCocvE9jr2KbQDJbILw0tVlpB9jabP/UVJRSPGrV+yTdvdaKD+PJXnly5uHFk+PPtwn1Z1xm6am4jujWoAgb+FBjcaZdmFECtGdYoLVFPQW33EZQJnM06E+QVU4ssHULvX4s8muNamWcBGnuWxhCflWGvtQZQy6ZF9sWVsn81majrUGfzWaUCQJy8IFvetzhmQsFy8zJFoWHS/bcjo7OrJ/IYxie7Wsy4eq4ph5T9Mvy19TTdyGQUAKAGSI7AbpGK+hKb7TGozklR5KpCViKa9NYm16JJGaLdGjtet6kPF5c99+yy0JXUX1F4ufsuaTOYpO1PYMlyqMKdpZyamNuWJa3Bsr32qguJNtsCU+OVyoR74NgUHteEiIvFuWMdFPXKCsrUwcr89jwW+md/UfjzJXpGwie25nReVMwEeAzmn5obBPYyc2VSCUeXnidJEh902ajeXXwpHb36KjPI9Ak1F1b84GHePUKo75juoZG1PdTYFYidkm4v6ROgvqz7jMB6Dk=
  template:
    metadata:
      name: immich-postgresql-user
      namespace: immich
    type: kubernetes.io/basic-auth
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: crossplane-keycloak-credentials
  namespace: keycloak
spec:
  encryptedData:
    credentials: AgBTjhI8iY1NIToAtBAbRe9qVikwtCVKzH52DYazQICVjjHOBOmG+XHQ0XSnopit6le+ayC9ls2pp3zRMXacMyd6a9x5qmFcO9N0hTxPa9F9O2tuwAjUBaWWegM+WgMu2DXa/9nYdKi0Kz1HSDy0gnB9Wtlmd4Xjbr/tYpDRVTRKizCgs1LLNr7/o6G/ZV0zrJH0Hxt85cZNPjSIoODu4v69ea/Xq8ioqSC6GvfThFQdwx1CZi/TS1OAHarpSWl+i3RfPEls46iLNhHi3t8tD1YumSCENqpFtQ3B1jMc5kAE4kGtsvA9RdrGp8oDLIurmWOzL7HeYSJILrB6cVo7L+Y9wnI+ovCeyqbHeGIjBnYUUYTW7kvubbhBzzdhwDDVsUk5/3TYV8eQoccvYWPfmOhzIGEmBH7jOdD7TfCvbx1TMv2cuMjTGoE2A0WBE+hHY/OgtlWN+Mgvld83KvMLW8cNp2Q4ZhY+syo5okgkAUUqUbsw17bGpBnGObsipdB2j/PqqGTHlNcqskK46TVsu8MPV2MwWcZ/UfDbiEyA9GVYFB8aZXtoNwhXsRLNVS874K3HbtiMoNNWYaZ3ddairQl8uomhEm/H/33IiRJwElFgKfQvQOr5ak6yoBC75NoFetz4fZLr0XgWgB2RvIckb3+WxjrCmc5IdrlrL221otu70T1bZcvZN1vaDA9V2AseY7/WZJuGDqHpfeARkdXzDJd99LPW1Ww6eGBru1BQQWi0obilE+dVEueJ9neebkmZX07OTU1dOQRVvHFQb7k0U39s646mseV5g62d5J+2dnxE9bwoYdy9XY2Hu/oCqPW3ezgyENy0rjwhJo3ua36P2BSkDGI694N7EEb2GjuZSZKlDCWaAmfEGhj4Fw9z
  template:
    metadata:
      labels:
        type: provider-credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-admin-password
  namespace: keycloak
spec:
  encryptedData:
    pazzword: AgBgtRvU9pbVKgL7wZQnogcTXA+I8XO3BqAzialWSeeX3YpYCBbhNELkTe4BcugoYfL+keC/0D2VqiECisYL4arvUkOmHXKBVUsl97rzSKcS8zSomy4OMx3lQWI6xBuB4vwLzDN6qcl+LfnoIJlzcycpgQhdWPPcEJ3utzp/dhO9LzocCW2xHREgaySYSkuaTBdq/Tj2HZKuaJ7hVNgbO00zjHZd7qAs0OVFoZpJlZyMYdZU0S1r0gL+XaP3v2ARZ3XQEDmmxufLOPWcxYOeJ712q8wFZn5sCJdns31sFCFYI5mN/d6iBvcEfuWLnCz1ET0DC6HgedLID1jB3sCjYnQdz92V2fkDyj6uzqb7X9CTKwn6JvY8Egd40KVbamsLuoiT2aNpFw23gkK3Ajus3H5S9aL3k8zCSgBrHBKkO2TZDER6Fzw39cLBSnRBWzPS7wy10JIfRNPKV9rmGlHWY6y+H4KWkPiIlUOqDGjlfaLPXyD9o0DcTs3rTPF/lScqPtqSlKJCEDwWqudC8UKLn5YSPfECdOnqNCDyj586h+FZk0IxU4MSG7HbLHi+B3/Aev3qibBeMIYSsUYGn+B61YY0INm+AEV+M2Co98WU46S7+zIHpxSK6LxV+ov6MIdyo1tGC4PnFd1bIZ2ZxjlUnwMfE8tFSyWO+963ECVcPx0rXbvEDnzKvPsW4f25ttk0zzjX2WLxOfs6fcOKnoeGnG+U
  template:
    metadata:
      name: keycloak-admin-password
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-initial-passwords
  namespace: keycloak
spec:
  encryptedData:
    jiyoung: AgBcUBmxePlb1zMfM3AuCjb5l94cq0Y/wo4aKaM8Zf64oGxJfsdV62hYasXdgAhXrzOpOwV+gS1pJxlgGZXBhRecfT8pGJ565BpN6BUW3S3tlBzKKz9KQeywdTWSNwro/H+y/ySGxvmBQcK5MrFitbkaWegyh8XiNZi6zwKshIfYYrS67h19bfjmlKREPbMtO4ML0ABm1aK1PvNIk6ootKpJrap+XcBtnKPJXyF80qrOBcQvrN8z+MXzBcP4hvYB9bnaqGdsR70DLH5H4HfBo6HkWdYYO61ymcz1x/rQ3xFATR9+cZYLEQrgvHGOzGbvLXbbNGmTEQf6oouN8p0y2zIFb7s3UtxfhbSvYbx7ynXuZXGzemB87uSH/+hdLcg2zwxM9BpAJvnyml6n5EkXYINIKiOUPc39Xx59DKy/sdhAxNat9XkUY3T+Hjy1POGrHQ7CYvwVKqF57ffpH7T4ZVmbU3PCdqJg0NSfUO8/K4gILareKM3/a5ljHyBBSnajpJDMc2ZI2OtCGPWyhWLkzB5DuiB7NOEkgKnv8fsoAmNuT85VoHrYUAafIUn06rQ+TB3Zbeyc682UQm4A8AuZbjsnbA5lEBkznhXltceUPkjsM9vC4o9NGsdgw2y0IALRDzCiY9QPoiNYDSihSwmdR2qWR+HmH5tRvd+W6y88wwT1uNSChYq5ZhV1WelDKTdmgqd5d20yw0UJgm67JipXEH0=
    olav: AgA3vrrT4IljgWy68cCPOStbZdOqo8dpAVtXvSYTyhogPOJQgW/tC1LOPEkrrUbEqMbeahYJ4NihjmKROaeVyuY7Ll3cbLtocxbGKT46xsaJmwRqj6D/cEeOUyK2F1pCBdUHsIrElknZoLTX78JqtuxTvcA4vEz0RHHcuTTqZfSeXxNteYrWbUfBaY7ZPW4RX5CU+s5cInJLYAS/N7asj9ZB5v84vy/WHZI/AQmO7IBcn2KV3WC84HAxtFNxXQee9MAQiutyLjXoC1tdTWIxZ/mI64qU4cq3Nq0AARcLmEh+Q+7eGLzC3xDxIJ3s0DzgJ11qpgcd491VG78PcyQXvAHDHUTasQoqMqgvgSsTDO3dPP2lPxaEgRjzCXIwxSLUOmQtqHQ5qEXxTtSnSuvL/JQ3jM6Q1Xcc+kzD+4ail2ROu09/pNJFUZwLMS8y140+vH6FZn3CZFLVrKkSxsL04xWCE+DCblHIg1MCW5bt0QvaCqHm0BhiJwInG113XrqNxIvJMzf596G6Qebl6AyhLOBCeoC6iehN0xc2lO6oAl8S8Q4hbmJsUzOam1s3eNlnqROuqjOqj8BY8GlwfyL1fmVpSUg6GrHwwH9Ie+3ZWy9NvNmSrBiCm9v00CGL/6RBkMkHVmDsiIBWrMyNDEmDOAMMmm0AiSWz9TKiyy63PPpE8Cfo6fu9k5+4mGjd6vR5pRF4xu8lOt5Yycg59hzOb9zS
  template:
    metadata:
      name: keycloak-initial-passwords
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: longhorn-truenas-cifs-credentials
  namespace: longhorn-system
spec:
  encryptedData:
    CIFS_PASSWORD: AgAK0Zx71bgM/afd7fl+ynP/g78kh+mBf7gQtCjrwXQZCpBhNWMhmLHs4P6Ca07c/IRT/K7UCgEcq4Eqjh0+0N9QyLJAaPhDP46oEtb+hZgbW8WYvRYTqHKxeqGcKThUH0ktIcQNXIBwj00JNHsPvAwm693S6NaLV1DmUZsMr7aEkUoK7dfwPTF0V6DMGuPsHSrJL7SkIccViU/ThBliUdLttO8R1eTM/MKAPVTokQtxilRBVuM9Jmn017K6uXqX/14+I37W3o22qNng7ouxVggP5xH9+R4rXOqIyHi+jUfQFo0MI1U/lWt9kg1/IoTPHJ/PXoNYQcqpQOuTbO+hESeQyKisx8e+ApwRemC1LkGMSzd7ymgQQOROdD92spWFZKBbIgtywIbGQbjLQKmSMEhioMDPi0D9bEvyjDUth/sMvplqWHu170w1A6yQS2W3OPM31zikYt5XyDnOEfzZ87pqvdZsTWW8FJjukancrixSuQLKFPNgrUku9vw22DyRqbqoI+NbQdKt4IYckpPkKmkVXbgRfD5hLQV2B0WBgSTEt2JMKXNAZMq7gRmwrI8aQLsSFumKIZD1xGpQhUr5e137RbuX//GPUl5mBHdiVfkSW7Ojz1RCn2uh4P55HJZk/dQPZNdHqz08vRIqAmRa+CeSbImzR+1h2Bi4YN0AvCgm4+vXrrs2cAfxSZ8CgAQ74srJMkC3xpGGrW85PRvxbKPdXZowaHlzZYwrO2SilzETUA==
    CIFS_USERNAME: AgAa3oFfTrhiWjqAP8j9pQugorO9184Pcx5azNfxoXE5fbRTG66cnQEr2bnQSfXX1ULo/Hu8yUkWW4hXO1ZZUlFnu1mdURd0GoqicKtId308jaqgQGC+fmK+222qstgbNGUeGupOh4h0RKMxbEPRjQ0xIQ4uiIqhpTnDzPtJeHATSFVsxEBjG1+vCzgJdpvmmf9AaX/PFjbndnFNmZnU2HSQcgLlm1Ff4ZwmolCeFja1R0bcIbCjUl3vJBkfWQ5jUewDWzmZIOvkq3bKGp4Ojm1FWleo9fhRNNjTqEgqEHcARzndVbxZUw5AOvlKGiovuan5n9Ubic4pFTHGcGBzuUXIdFJxvyZDz04bjPZcwWHd0Ll84gwRX0QatWKOVkh6MdXHSXnx4mnQFufHqRfADi/L6sIcFun5ztbbleuJCtSXh4vdRHzUSxyXst49TfAsKVpLLXlV50BXOf4Fg+LVddG/cMhkhi/ZC1nXc6fREgZAJgtqEJHSCPL2gx8RMWetVM8XK7xU4FDZDqgVacydjs1B6x7u8/rBtnD6AkR/AJZEzaGpvA8XizI8i5EZBj6QbYY+woiSW6WgU5Y6MMbPWM8LqrL4POON3RJWCh0/8eXKx8xZARrrmakncq3OlgDrDhRu37r96jxVzWaWtcWqBVE3vTLtlWpZ+U0tzBg4Au1DZbUh2fx3N7e5Y1JwLIoXPM77RFv27QhYYA==
  template:
    metadata:
      name: longhorn-truenas-cifs-credentials
      namespace: longhorn-system
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-operator-api-key
  namespace: netbird
spec:
  encryptedData:
    NB_API_KEY: AgAdOSxs5RPfXS6aBd+i+Dgc9wd4tQ+Q+3tm0+zrW0tC2K0Y4NocJPZGXvmNK33nb6gsgZshE3YGYSJ8PIPegAvYpZ5Efq47EKKQVhgonIM3A+Nx+MnBT5UYXWmGZQ6KT/Q4ksTPNE28cqecZs2HAhhu9CZlRCiDgn/Nf1BkpjLlEF3DAddZhexFQcJX/ABpbTwGmUOvBWm4xNJVbqHgGxqdXLa24SxHaTJohg6gWfsLOMHSP6J/408zyBcDeLvNBRBO8DD7YssVCKj8nPN6LB3arDh8VMsKLDM/MNNaKBJ+4puk//c0MdH+ZSo7SnugeE7Z1OfYxEQCuglrFYvTdCkCFeK76s13/Jx8jvEJLcBzwaMK85hm3M5B25HeB77MMiTQODF1HYTupub8iuoIxYAidjYim2kMemJvEdITwpiQzg+LEEtQp8KPK0AYuMQ5pqoIlMQkwIXu6ijJoeD5vjHcoEbQ7IaS46JaMK/NJmYO0XlT+0l+VUE5jjDd+gAm4Z+MLXMwsZNSxpDCYO9ykgPaZBYJXpoYyYo4ZEV+0VH88a+V7ZmdPzz0qHtOl8LYKzkCbOW/Rt8bpDjiBkH3+bB9TigjqoUbo203e0YvUffjLFMR7CyC9C20q8hEU2FU0MxfhyF4fAOahN7cZMwDDz3GZKDTeYO0ElBGmCdTga/XeneK0m4kJGoN3OqjUspdLujFJsFQtdQA22rHcBBjmW4GHJRY24RlzvfFX9osQPDWxeh7pnZcqx7b
  template:
    metadata:
      name: netbird-operator-api-key
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-relay-secret
  namespace: netbird
spec:
  encryptedData:
    NB_AUTH_SECRET: AgBwuk9UL1sF+dHVXNjTGY8/FZbHszo+c/lBR/tAcOfwJRDZMxUpDUTZCMAh8dof6Sry5RRSenZiWOM5pUYz9Vzzple+e7UjB//TJCz7LtVr6DAcZE8UHC79pxYiQoRuVPLHnzV5uy7OOrGJfAlKfYxgmYWf9YYDV4P2H86Oiflw/fm7auedQL0N/FMfe/JCYKVnSIss1m33rVYeQS66RMj36uH4mo+ctXwbeKfQFEdMmY2MpEuaLNNp4UKqOMxErHSOIR6iCDjKXNgppSwQaZ0I0Ydqr5NWYrtIpMdf4gJA/+FTCCs/nRJkPZFFUD4897XKXvueQrw3lag6dEJiZPMEl5IUaysq7vjdkBKis6INlkfPUC9K2BAAMQePqZEogYtEQUU3Ahm8JjUD57RQikYKxWv7rvcxlMlzGNGkWBpKT8idjy8gYY0/B8+RuU1XaxZ4xkNN7FrfpfNQ0qxuT1PzPYOmq8q12NmgKCNkPr0G/Tqs4Y2L6P5GFUeFqf0qrDE0XTRbPz6ly4Bic7gMq0r5Oalj8UTmk05rcDbt/1CnWqYkN3d80n23R8NMDbWb634H8gFw9E7r+1VA1qrHZz5/5VuS84yIHe5tMZ9vtok3KXXz9eE83+ucceRSu+ex5GsrnxG2w3sZB7H1ogbpDAKm8Q/BeIKWxessnfL8ZdYA96zwn22ZNgpLu1/L1bxY4NdXxzbHVSXrnQvnu7CFMU5rZ+XrBA==
    NB_EXPOSED_ADDRESS: AgAsz3POoeLHik09MEKr+e52iDi2drsmsJntQHwi0NVjkGAgrz0VnY51VDrGOS9WaXZXAD1iPU1lmIAIzH+MerB6Vcpo5F8zZjn20yRQpT5XaPBztGtMWhHE8vey/xXCssafA8OgwN/201mJBVUBOGJKdIO8NTGmuCaO4LCxbmHP0fiYE24QzkPEhTdIkHjpINQuzgAfuozrPiSWmo7br+z/Y6w5s7n/onUGHUC4scjxkMq35v+5SbLGzfOJ9nyhw6ienhUwF88KezDvxz1hfra0Il8m3PzcqluQK+Q7E2emSZBWaKBTJGI8XfjiJN/tRtp0C72V+5nPSw+AnfdJ2BUGF1lsdgHWoSj4pJhBhKnxtbmWRmWs3WYMD4hzWsAf1Su50cNoLtlyYNUqENcbkrrprLacQbPYwodA9Is92sTCbmWXaAex6GGlRU9Kn9t++IXgwl3GjTUIY59EgQyYSq2yxYhBM+ZfIEY4VDpcQLxScqz/yYiYoYVlJp7cyH1MvZ1sfNnZGkDxlyZpxH0vPuw5n4HloQSwPmbZlCMFJbNXzZTcVG4PM4VCIoUaSQIn/1em1uu1McN9bz19d0Xfx89/C0Qs/RbhDga3vlafTxXN+/efivErI9/1cXLjY3UB2xj97PVejHeRoP+L4oyh5I0zCqcS+LBUrAYhZrwaoRYwHlyBUHr9tOF1JU3+m64h6uxVVuL8BSSUeEeKrbYYt5HsipYd/5PWEMzuLxB7gHPZvWG+aaEa
  template:
    metadata:
      name: netbird-relay-secret
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: netbird
spec:
  encryptedData:
    password: AgCG/s3MyJZ4M4qNyjVMonU+TDY+ocaxyye0tMF5kh7uVSJjHoASnEfKjnPpOC/quWksWploqGTgyHsL0RCY9BOPMpAuE4Pp+53VY6jAvH/NrJZOsqQEdgtdmjchE+3d3vZOCM5tHkjZSClvTGgTRFkXdl5pIWE4+o4WuLGtcPFrB4uLYCwq6UgazoOarKTAaj3oPuqssh4HFGyds8gKNxBHaq6cO3RvcH20P7dvnw005UCDymyVoTjxiuueT5SM48aH7O8aXhRYPbFbLwHsD70Q0UaYRT+UUPWnvpHWpDheHr3dVvWM3nkXxkOwre5Ld75BjfDfIM0/CihULYIOxUi/A5e2HTdauGQCGUSJvfh0z+VwKeVsLe6mBEJ6EKCWpWPDwVtnfEYk2Yohj/Z4O5F0ie2eRhXN+QsdCu+RQAxf47gFiKe0GueN1tVn+/QxGrrv4Lp2Jb1FiMwPSmSd4kJW44cKjd36j5ESrXyifOOwvcC+7eCDFOPfTMqYmloRp4RyXTec/BFzxZZ1ONWRYq0Tni4tlw04CCNsEVy+OYDArWu48v6cfPTAiHjgZBZHyIrRpG6cPOGzcWzJLdkQXkK6kxbUOqLVjPNEW2x71kRqiyAquszYV92FBoE3n8T1Au8GsADuurnN/t1f1B8SFi0BX+dILcMkxmQgVTBk4+946/NT2jRILE4ADCWYCI2CFzK5esvn44rlCzB5TeAPZA/l
    username: AgAr8X32/nMm8HRFPn5EK8rta+TqJtiBsmNKJqRe8EANSWCj/UlVfLSv2g1NSsOYZgorVKgHsS8ROj6K0xBbN13hGIwJ0kFICi9REHTdl5j34ElrCPsbYMwr3ZHjTHqTn1h/k1LS6COpp1lzAtiE9iUHKRu2EPLDJO1E5teRFLIBXMXanWWiLzjzgbqHImmW0p8/75yqs0DK1xYNiAAge7F7KJM9wq3lpRYbJ9oh9fv/hOEAL637NXr/fKtxMYj+hYzXzKSolbVep9iBmltCI5BJJMbsGLFaSa5oSjG+pS+2UB/k/osUiO2TfRpwn04yIiiMh5hmXqsAIN5T8t41l/46hqOvMMzrWfxHHNPGHBdlMuypXMVNrjloBLxbjApD8fNYR+TlkC6QqPEdv2Thd6Fta68JUWhLPyzbVhhGD9roMvY5r1diN4QQmMEfhl1JEumNWYDA7rSi1wrMjLr1XUJe+8WfxfW3f0RzNwXJvFtMYsqnjUJJkjmuswCwpfW5jBkAE2Q5wTdaFdoGCrZZsHUbiT3Vittw2/QUWvVLNvx3DZT3T9D6T7nVuq+3I7ORNFcXYhO8fdwevMhv/1L47n/EkVo/7ZfSj2xIvaDZ4/GTQvshttkOQkGFchFmkbmH/F/NP3/Hha0hk40s8n+1GR/nOsUEnEVBlSDbL8Aolt9ZC5DSJY8ISkPwWcLv8DqdY95MpmC+qgrT
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-admin-user
  namespace: nextcloud
spec:
  encryptedData:
    password: AgA133ceVXtfCLHa+fpkKReprcm1tpKZkbpTwYf4tFydhIIO2QLXRD2CioaSDYsk0aNlQUzXZyQJyMfplLs6G7Q0t/2d8cYVgVmEBcdiXgB3CF80gY4/2newgaCQp1MbCYoc28q4scE1vZvqgRfSGWiO12sxPklMWtrAxgIoNz8myEhKEuy42GfneXDUjfN+LvHFSJugDCVBLE4rc5MkNdB40UO6R54zxL40FQf7azZEMcLDIaUfM+ki4445V00rEMA8J9k9DEdPIXwRKva+osIcbKDuWv3L43fEyP0cfj1g5lgNcm+oX/Kmo+Lp48+jvdYOMQCnWW2A19pBIHlym33bPR/ViMkl/o+ztKiH6UxxNy2FyUZkIxKjL+1h/XctiqiNjJg7zCWEn2Mtue0zN0CMSe37ApzE5DzngsHpLAkvJ0DxHq3PPd3l3TevgeVj0hc2gR0sT08TuT9TRKK/EqCcPHpmeKtnzzruV7X+Vl0nreAXS1T/x2COW4FP8RE4RkXSDxf8OPhDy+MSFX5ortwbGM29yOt/9YU1Sq9uDiCj3OeXkFq0wEHCMsaSVJB255cMTR/a8FNrQFIeHT/UkRVQuSCVKAejpeN3EQnMr6ni9BQWu0BVLTwGa3XXQ94ucnmQtye3TwhIz30Hb/4mLloU5jMYUap3NwBp9cMwOxrKyH2HU0cDARAwgsgJhRA6a+OAGPz4eBwxZpuRza5CPRXNefF3cg==
    username: AgApHO0tUVeG++vsB3O4rLN+RrgTFG4uYNvk3lI8+KMV+n3faNak6j+RwhNtwqwcDO+loVQCfvi+mpW9txEqCfTRQ/xbI7oAHGP2fg5YJw4fHFDdKyH5tAeYE27nF21cAjABFymTqkqyD00A7qlorjljc6ld+KUwS67XL0LFsssuNfZQ9bxDjl/Yk2a+4ykNXNYQnMO7hqvxcrIhq9Dbbr8rOFNYTM6ByhBwaU7R5/QgUJ/52Eyb3Og0C97SSesNQUH2UsjHFVEqdMD7dGmdiAdvlA/5wd1V8yFlaUfPkOkWnCpnkpz4frSIMKJwQcIcDF5/YIfMtb/i4AOcUAv9qW4brlS4iCB37juPfItZkUtXm/iHItEAE3E2lkl9f+t1St1f9KuLh3aNOSgfiwTSU26Iw/850ZUjQKhkSXDw4uqiZ7XJC89dT1aavQfPLugj/5U7vuGRkkC+O04x3N6lLbDOqNC7O8H+sIwqd2AnOvVFvGf9ucnyY5ziNuDgNUL8bJQPd06zGh4u15tbOaat4LJ0DbNQACp4Fln9ivipgLvDb/8+WYSmkX2gL2TPMjzoq/FLtB3jgbhGcCDb0gqDkE/UT19JVHjCoBZAHi4Zds4cSknKiqbO8X5mcK4j4dQNutsA5cAbWasjtC18tCySevkmu/6k3oZutg4OYaoiWA0lHwu8AtFlQJGVMg2m4hEp2vaZ0SRsEQ==
  template:
    metadata:
      name: nextcloud-admin-user
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-oidc-credentials
  namespace: nextcloud
spec:
  encryptedData:
    clientId: AgAflXsMtVGgtEQsniJ9UzV04P7Ewgp95GxtUTeRd6K70Hl6cJ/U49uoVwcmLxPFixBNFzItds9tAk2IUEWNuVExvDIsLYbhtrlTe8GFkw7IMkZ9goPRGkh8BsJeuRlcuMp1BoOwYEmrL+0whkuJ2od4lVpyiI6z11umOscQh2A8AYe8bVUejpD/ZHZOrLlf7DPfKKcWfnnztPIJOlzngQ3r7zTAogGfIAKRNFtIRss/EV7r7XR3FuDwoHaYGG3fcLsbbMjOaMZniW09cfWFQOy+OFDh5hKNJF4sWWeAElR9nnrYDCNt4qzPlifCjTX/qaMvDATBBv5EBrmukiWZmgMwJyMBRSsf4+2uDmSZkafq0qGwzbCaqRUUqhDiERSTGyHzFZ3z5G+nSeXH2Gsdc41qO9695Pr4gJjeHj02TKdm2klzSfsMW+qWSOjk3r/ookH77AsHZ92+lo3IelJBL7M2n8ITEjRE5wKJbfIVC2QgPqxEgTUV/A+d+YH8y5tRJWBvK/Pw9stE+lEtHY4JTFF88E7+e16ofcVtg62BfN31TpC8JI3US8LD5geAjXArd7sPCc/rL3Ly1+ga2nV0bzfvVuFVaAoG92wpNbNPw3++qwETvKdXnJQgwwtp8Ih8Ns92peVptSw34w07nIhYVJ3riW8d2t+dDFnpj+z5AoLPSWDWx7xHBEN3sms5JgkaexvW9QGbXMisY1g=
    clientSecret: AgBw8U13xVqqZW7Bco9aLogcnTBvzOmEQh6HOzGS302xWbP2Nm25HVomkStbxNI6o1ZbOnH5/bJXgPSPe72Ji0JwtS/G85iCl0bFeivJBsFsvqPqHurf49xYeW+MozaOt23bVetpzrNkGAgSl4/WQ07K+nv01nzJpVRHtfdrsxZHUx9gvPGLtydH+PXVpm2QxPd6BOpbx0Wj2A5AJ1amAtrZSOXYpidsnOFB9eg0mKg72jILzWHvZGx746SW/A9ews7vS9On5c7i/eHqq0r2bjwRsWlUhoI+X1XYL/ixJ6dkPceo9PrEF2WEpJSqvWuZZUuZ5UP3EoqUzHMHsBvMYwa8I5pnYsUgQMhM+qLQ2ZhpBAFbyOHC2eOq+zy5BFeMZZ1gZcInr27+dOm7fHjABi5AXLJxVXUDzWnjC+C79dx9Uphi4ynWgTqeHsmcnAVHF5U/8caqYOB/lJcHsGns6hqeXw6CxQ4QzaJhZB9mJYtpXMHno1hOfVpxGP+CaxmPExoUNqS47AFycJ/6ccud8hiTNJ08p/TycEdO4yKbiGiC28ZJ2/kSC5c/YapEHoPaEKARqFbgrCmR6BdfeW6Qn/5Z829/cQ87JbB3V7aHIxGVElcpYo4/OABfv4+jCLkVpXexQOZe+A3AdPhOCjGhTO+C/sHVb7qF2ylbdgD0sdqoeIAHmwcotLwgKBD1LKvGYSv2bxdyD7l2YFRkEVHqPm0/qzrIBw==
  template:
    metadata:
      name: nextcloud-oidc-credentials
      namespace: nextcloud
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-redis-credentials
  namespace: nextcloud
spec:
  encryptedData:
    password: AgBy7SJXZ7YDR4IfFnkBjBI7JCUKXTJYu3LXkyLz/ops6Y6yXwQPyRVaQgWz3SWPZwjIrABEHHc1iDByXbZ5xlwdKccG7qu93eY7g4eeyAWVHDYdJiJKVZwK/vwIpJ87K9ImacGZMZrptLxdqmPJXNi13wVM/Ybbb06wnBfvknagKdCUH3EYiSvssGof9d+ELsiPTPYF+zbKMFgqFKheZTK2DgJEvgQc4upMu5+0BfhXTezt5pxL1j44n3VidCDuWUYg/Qwv5Mu5Yd0frrIa6wAeVi8hjJoiHcFA7k+xOY1y8ginZ/MuGrQSDJMEMJ4y1M8zckPZ3QXwljcmDRZ2q49L8xoAcsqQpFGC8k4cl50PtuSAcidcLLMX13cgZ7rQ+0Vqfsm4GYazEJTFm8inY8EliiuCDTuNouRiP3Ur6WnlZoKgIMZDfZOBeNj3sggAVj+anAivMPvoz+tCtJiZ6wohwnpAtz7Pn/wFOfFwukMbpzZwsKOpQo31pYrI1uJONP8r+gN+2wHXCJ6TBM7/nd1qZ/+ImGjbr/1JKwbL4t782z/gcEM0B1IGZSThgdxiXxjKEnYQWobII/Wiv4j7rgz8J+cTaiC3CZOaMXO2vRdHQ3+3grEBkwDk7/9UcdlOCimt78MP+3D3bfzxHpFPO0Xc39//aJec3Xj3Xoa4r9CfUfrKDb4B8zR4QWVZ3YhNT8RGnMuuUVMaX6rrcSj5V99y+Nstr+qVDZyQKzPV31nAhg==
  template:
    metadata:
      name: nextcloud-redis-credentials
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: openwebui-oidc-credentials
  namespace: openwebui
spec:
  encryptedData:
    clientId: AgATZxYet2gz1Of6VtDGIDaStL66v1Z4hJKja87uAnc730z78ePf5irRcv//h5rxJkA7Ge0dB8aAdyUSKwySIRwA67yV52bTPLroyYUuqe3bIRFaUWZJKWWf4gSpwHIXDDKMxhzq67k6fuwvVKT/Y74FgkUGRqmyDF0syvXiZ5dtS/V8wqxlWdS0cng03+JQf468N5LUM2CBW4kYwhfqB9EF2txNKABnAcBC5RVKRMq07VmP2BW645CpqlVCDiRQWeh25GsWDbd6YsA/BgG1uNY+OeN/Kd1TpjVdnMJJG5Kgud376Qmgl8fo7RKB51hnRETUL1yBMc5k/BUZH/deuZJTfvr9JqpWwjr2dAw9DPJvX+fTu2LT7kXQHJbYr74908Yd0JrUCO2yOz4e9DpHvCvWCOlYqN8VDc4D4ZKw7PJgNLPdkue8lNXsifCDARebtGScekXLuWnVfojTxmBPz/fUh/12VT1OJxmt0wGtbV3sPW/XT/abjdMI3/PVU4DCeD/4strplrwoPlehk6tQWtBxIIGD8KPHXiPX6WknNtWVlQ+iiIR6hHTLckCyFZ7f4x9msL8JdFquuv4ICdtJ1RJDajHjEm+GVIomsJKgUW1eN8b1pTptNM6YDPewRi87ASN7988s0semlV8YBtBoqs4p4PA3LX4PABEwBGEurJNVsTDW9HU4W786av3nQ3E8vsXMz1i0ZDNm8dY=
    clientSecret: AgAAGMRkzGpDj+45aIwMQ+DoK1FF9Nlxq7GBYgp8+9HWSdRt5rwQqmMv636bQeGqx3PGzrrCkTTUqzlr1otF7ilrwMWV+mPPjoR+Zz1q0LvlzKmhFngcTtSJJtTT5VQyd62EyxOIuVyNItdJS5SYis4mvrRUBfVnlNIVi6deC+k1s8CiF+EJPOGKeQHV8ozG11ryqK5OqzZNiidd+PT47FEfdVbnNz2dQzPKFK84F8ob9kMgbcQrxW2q1OmTCgbtweQLVq5cKmIBeIuxtomg2PMwyeQNPxwkiCSHzbZ9DA42CzqbvmqyOw4U1Ylz9KeG4AhQZN+mNVVp0pzy4yiOeJ3g9bnUFFO3GjExTxS27IXRkWmhZbR33faOASNeBEgDaiQah7dS4XNCkAPj/cs8tfGkBCfs5vFO5Buc8I0VkZiQGecX8u4sQCX258Y0vIK5fWpc2sBUjypW9w34EaztA5QqNSwcIo1KrIuKBVThqTqE0+RPkJNe73fNpGR8e5bOxjF7truTjSNKpx2VXAizvg92+LvUcd3KmL0NfMHlIeUe2sW6EzbAwhjIK6p98pOl1mo1tk1B638KT6tnHvLyr6Ej0/+XJ8rhcp1LBkQYsAol1eEOej2Hik0G1HO1YerEoiK3TIou15HwpQ2vYLFtbuzyCiuyq422UWbYiclptN3R4B7g70mXbOeY4xY1lOVOSTHYtakHSg/3Mzfhg25wM69c0ycXqMtC
  template:
    metadata:
      name: openwebui-oidc-credentials
      namespace: openwebui
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: pingvinshare-oidc-credentials
  namespace: pingvinshare
spec:
  encryptedData:
    clientId: AgB6AVC0G8DfPEKjYNVcOdHaI/Xx20vQIq+sG5jnfSjiDpHuBOAOk+dazjJ1tmIYIybyygebVOHswKvo9uCHQiIOoYPOW+9AMYUqdZY2NeCfib5x1w7wUVA0rt4O1gFUHvmFbgUcb5GeEzabe6T0pOiLFgYU0LnUztLb+dFVTdmvhU7/pqAWVtjxAfOw62T3KJpH5y7WRosDNyCxUBEFcQttkKWGL5t9tA5Z/e9dxl7cJ4WIWrQ8IciidkwsqKXD81ysGFgVIlzglPxsQjoGYo7Y8+SuTM/m9R4n7mvQYMEtq3RYRLDmIZguGHunXPD3RmBTh719h7IL+bokf2C9eQj2w0oAHeZ1WRMJoZYKP9f2LV0TcsH93wCV5jEJYf/xVwGNIgZyfoQv7kbcc3hrncf7xeUWlHVqwLfZuH9NVoeObLPnkBAp8LnMvwMTc/h86e6gsRVML9oSrJcrtmxhuCEo/Se8O4OOJFKLIbLyy0PzSEVsgO5CZoW1Ven51+EyTI6GEOJp8pw1zwBmjM0o2MkXb2al8is6kBnkD7cqIK8DaOQqPd0Pzuq34IdWIHIx60l9leoeYG6d05Rp5r4KAJmSKuNKjKhmf4VYoYCs3icR65/RQuBBDE4LXZ+BTE9OyEvr7vKdZQteS5HdYCxMINDiBfVbHofcO3zmAFxxMkxhkyJOMo85v7pCy91ESZ/3uL+YEDKa/3Se
    clientSecret: AgAxLGU9KoY7a7pO9lQNdeEF5z/IugZGcfM8C4eqolsW+1PX0ChD4hnucuhbIW3HaDvDEcV28q4Kwa7t3sHuBIF70+L3Ct0vVSnCeBXsY8kOtYtHkSxhokNGRCjwDLBOv61iWP0MyD0R/xt1uoRjua2O7MmhknkGFgfmZlIr/m4Q0zGK/ZH4r9ARNY4wdsh5FTGiaUspFJnLVTKqryUErcJFLZalKBzatfjUvuUMckKXiCpSlCCXAsFZzrniAvZ9OiXJn9gfRnFh9yhT+C5l9IyGlI0a64tMgJlu68F0YcucO8j39qDjprF3sJhkeDFxE6zrTk0rXM9/P5a490CXxdto3JrX7Jvvuc40S8S9vl6h+bl49wmLtQ6pLZZQ5Apz+HdYP4wtLWDpXAnO+ymVpsGjbg0aCb9UobruynmuZFvFjawqVIznvJLUxQLOpo1c5vNJ3DwWFh2cIR6V35kAicDC7cnyoNe1/wcJYTY+FT7lfnVRV/moQq0nO31h37Nf3CQZQCenmHwbCxyclbGLA7OMBuAFTKEtVXKE0UO27QpA9KbdulXOVbIUMBN89Dl/6tOlrgaAEtFunwOf/rrgm9gUOhAnlFKZC4A7eYOGjtpJti2hypUSfHwsItg7qejPR+Y6daF3qFcNwZgwBi1oK+i33i3ahc5xxuQNq4A5jxSCdYWQgkeN7hIZZfcAyBojmTXmYwm08MisNJMbB7/9QsibgEFHYUMH
  template:
    metadata:
      name: pingvinshare-oidc-credentials
      namespace: pingvinshare
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-setup-key
  namespace: traefik
spec:
  encryptedData:
    setupkey: AgABoc3LquXGAbkQTZyhKvRT+JTSSFemrCPZr+td5H3a/WO3OWLO8peO0K1MMe8LkJ5VjQvarFgAzxD4om7X/9OhnvjwY94sOtr7licFT2pMCQF+HkEcTaNVfWZZA9AUa17FQNM3XsSe2pJaw77fgoWDVguH7Y+inLeFt5vQ/SfnCGrXbCu3aEfjzwT+WgRMq+UNScX9Ry+wY3F4NrToGSR0zwkxHFPcd0jvsCUTFJGwYkM51+JpB7Ju/UTzOIyC3xZQTgzhpFvLwk9XbQqHKYhcP6fVKn5WUEAj93yzSowxfOcvqZTFWtFa+Nu7QgjimqyWmQqvOEzCn7SFj8iSqYEd0ocQp1g0Lv1Avt/2zaVlc7xweHAS68k0iZ5vYBinFCqla1fPmlUQdwydhsnVVnEctwuoaOEt6iztoVkN9u+W1hIn9aoacH1Ag6ieIbSfZBeDDbjV0OxPy8D1BLVQhIsHY8u8IlppYDXrYd6YPvr6Yc/n2+OIgsG8r6hVbdPeSjTfbRAXTPgqpDI3aThLugPHe1oPZW7mmoMhcWOlZkxv/8ciBTx2M0xP0Uz/hiXehyfOGC+XNgNuxm9wXyZ6xAVnCuGNVoTyahSGfNGPVkstWZpYhMtKscZupy6E8j6ObFvKkN2Nasvj0wDYhEAE+daLfovUNpdUhx0uWRg89iNyV2tZmQWBH/yv2QEyMF0qNAUftxVQU22DVHHnNfg/SyvS5BGnYtCJDdOammxEtt9lmSAkSWM=
  template:
    metadata:
      name: netbird-setup-key
      namespace: traefik
---
apiVersion: cdi.kubevirt.io/v1beta1
kind: CDI
metadata:
  name: cdi
spec:
  config:
    podResourceRequirements:
      limits:
        cpu: 750m
        memory: 2Gi
      requests:
        cpu: 10m
        memory: 60M
    scratchSpaceStorageClass: longhorn
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
spec:
  dnsNames:
    - coturn.homelab.olav.ninja
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt
  secretName: coturn
  usages:
    - digital signature
    - key encipherment
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: keycloak-jwt-signing-certificate
  namespace: keycloak
spec:
  commonName: homelab
  duration: 8760h
  isCA: false
  issuerRef:
    group: cert-manager.io
    kind: Issuer
    name: self-signed-issuer
  privateKey:
    algorithm: RSA
    size: 2048
  renewBefore: 720h
  secretName: keycloak-jwt-signing-certificate
  subject:
    organizations:
      - homelab
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-serving-cert
  namespace: netbird
spec:
  dnsNames:
    - netbird-kubernetes-operator-webhook-service.netbird.svc
    - netbird-kubernetes-operator-webhook-service.netbird.svc.cluster.local
  issuerRef:
    kind: Issuer
    name: netbird-kubernetes-operator-selfsigned-issuer
  secretName: netbird-kubernetes-operator-tls
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
  namespace: cert-manager
spec:
  acme:
    email: olav.s.th@gmail.com
    privateKeySecretRef:
      name: letsencrypt-issuer-account-key
    server: https://acme-v02.api.letsencrypt.org/directory
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              key: cloudflare-dns-api-token
              name: cloudflare-api-credentials
            email: olav.s.th@gmail.com
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: self-signed-issuer
  namespace: keycloak
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-selfsigned-issuer
  namespace: netbird
spec:
  selfSigned: {}
---
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: default-l2-announcement-policy
  namespace: kube-system
spec:
  externalIPs: true
  interfaces:
    - br0
  loadBalancerIPs: true
---
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: default-ip-pool
  namespace: kube-system
spec:
  blocks:
    - start: 192.168.0.90
      stop: 192.168.0.99
---
apiVersion: group.keycloak.crossplane.io/v1alpha1
kind: Group
metadata:
  name: homelab
  namespace: keycloak
spec:
  forProvider:
    name: homelab
    realmIdRef:
      name: homelab
---
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: hostnet-bridge
  namespace: homeassistant
spec:
  config: '{ "cniVersion": "0.3.1", "name": "hostnet", "plugins": [ { "type": "bridge", "bridge": "br0", "disableContainerInterface": true, "macspoofchk": false } ] }'
---
apiVersion: keycloak.crossplane.io/v1beta1
kind: ProviderConfig
metadata:
  name: default
  namespace: crossplane
spec:
  credentials:
    secretRef:
      key: credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    source: Secret
---
apiVersion: kubevirt.io/v1
kind: KubeVirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  configuration:
    developerConfiguration:
      featureGates:
        - LiveMigration
        - NetworkBindingPlugins
        - HostDevices
    permittedHostDevices:
      pciHostDevices:
        - pciVendorSelector: 8086:54f0
          resourceName: kubevirt.io/ax201-wifi-adapter
      usb:
        - resourceName: kubevirt.io/nabu-casa-skyconnect
          selectors:
            - product: ea60
              vendor: 10c4
        - resourceName: kubevirt.io/ax201-bluetooth-adapter
          selectors:
            - product: "0026"
              vendor: "8087"
    smbios:
      family: ccio
      manufacturer: Talos Virtualization
      product: talosvm
      sku: TalosCloud
      version: v0.1.0
  workloadUpdateStrategy:
    workloadUpdateMethods:
      - LiveMigrate
---
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: homeassistant-vm
  namespace: homeassistant
spec:
  dataVolumeTemplates:
    - metadata:
        name: homeassistant-vm-disk
      spec:
        source:
          http:
            url: https://github.com/home-assistant/operating-system/releases/download/16.0/haos_ova-16.0.qcow2.xz
        storage:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 32Gi
          storageClassName: longhorn
  running: true
  template:
    metadata:
      annotations:
        io.cilium.no-track-port: 443,8123
        kubevirt.io/allow-live-migration: "false"
        kubevirt.io/allow-pod-bridge-network-live-migration: "true"
      labels:
        kubevirt.io/vm: homeassistant-vm
    spec:
      domain:
        cpu:
          cores: 2
        devices:
          disks:
            - disk:
                bus: virtio
              name: homeassistant-vm-disk
          hostDevices:
            - deviceName: kubevirt.io/nabu-casa-skyconnect
              name: skyconnect
            - deviceName: kubevirt.io/ax201-bluetooth-adapter
              name: bluetooth-adapter
            - deviceName: kubevirt.io/ax201-wifi-adapter
              name: wifi-adapter
          interfaces:
            - masquerade: {}
              name: podnet
              ports:
                - name: web-ui-http
                  port: 8123
                  protocol: TCP
                - name: web-ui-https
                  port: 443
                  protocol: TCP
                - name: mqtt
                  port: 1883
                  protocol: TCP
                - name: mqtts
                  port: 8883
                  protocol: TCP
            - bridge: {}
              macAddress: EE:88:64:5F:EA:95
              name: hostnet
        firmware:
          bootloader:
            efi:
              secureBoot: false
        resources:
          requests:
            memory: 4G
      evictionStrategy: None
      livenessProbe:
        httpGet:
          path: /
          port: 443
          scheme: HTTPS
        initialDelaySeconds: 300
        periodSeconds: 20
        timeoutSeconds: 10
      networks:
        - name: podnet
          pod: {}
        - multus:
            networkName: hostnet-bridge
          name: hostnet
      volumes:
        - name: homeassistant-vm-disk
          persistentVolumeClaim:
            claimName: homeassistant-vm-disk
---
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: volume-backup-job
  namespace: longhorn-system
spec:
  concurrency: 1
  cron: 0 3 * * 6
  groups:
    - default
  retain: 1
  task: backup
---
apiVersion: netbird.io/v1
kind: NBGroup
metadata:
  name: homelab
  namespace: netbird
spec:
  name: homelab
---
apiVersion: netbird.io/v1
kind: NBPolicy
metadata:
  finalizers:
    - netbird.io/cleanup
  labels:
    app.kubernetes.io/component: operator
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: cluster
  namespace: netbird
spec:
  bidirectional: true
  description: Allow connections between peers in the homelab group and cluster routing peers
  name: Homelab <-> Cluster Allow All
  ports:
    - 80
    - 443
    - 445
    - 1883
    - 8080
    - 8883
  sourceGroups:
    - homelab
---
apiVersion: netbird.io/v1
kind: NBRoutingPeer
metadata:
  finalizers:
    - netbird.io/cleanup
  labels:
    app.kubernetes.io/component: operator
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: router
  namespace: netbird
spec:
  replicas: 1
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Bambu Studio
    hajimari.io/enable: "true"
    hajimari.io/icon: https://avatars.githubusercontent.com/u/96461528
    traefik.ingress.kubernetes.io/router.middlewares: traefik-sablier-bambustudio@kubernetescrd
  labels:
    app.kubernetes.io/instance: bambustudio
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: bambustudio
    helm.sh/chart: app-template-4.3.0
  name: bambustudio
  namespace: bambustudio
spec:
  rules:
    - host: bambustudio.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: bambustudio
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - bambustudio.homelab.olav.ninja
      secretName: bambustudio-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://upload.wikimedia.org/wikipedia/commons/b/bb/Gitea_Logo.svg
    ingress.kubernetes.io/proxy-body-size: 10000m
    traefik.ingress.kubernetes.io/router.middlewares: gitea-login-redirect-keycloak@kubernetescrd,traefik-sablier-gitea@kubernetescrd
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea
  namespace: gitea
spec:
  ingressClassName: null
  rules:
    - host: gitea.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: gitea-http
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - gitea.homelab.olav.ninja
      secretName: gitea-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "false"
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  rules:
    - host: homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hajimari
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - homelab.olav.ninja
      secretName: hajimari-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Immich
    hajimari.io/enable: "true"
    hajimari.io/icon: https://user-images.githubusercontent.com/27055614/182044984-2ee6d1ed-c4a7-4331-8a4b-64fcde77fe1f.png
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-server
  namespace: immich
spec:
  rules:
    - host: immich.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: immich-server
                port:
                  number: 2283
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - immich.homelab.olav.ninja
      secretName: immich-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloakx
    app.kubernetes.io/version: 26.3.3
    helm.sh/chart: keycloakx-7.1.3
  name: keycloak-keycloakx
  namespace: keycloak
spec:
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak-keycloakx-http
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak-tls-secret
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  rules:
    - host: hubble.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hubble-ui
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - hubble.homelab.olav.ninja
      secretName: hubble-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  labels:
    app: longhorn-ingress
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.10.0
    helm.sh/chart: longhorn-1.10.0
  name: longhorn-ingress
  namespace: longhorn-system
spec:
  rules:
    - host: longhorn.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: longhorn-frontend
                port:
                  number: 80
            path: /
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - longhorn.homelab.olav.ninja
      secretName: longhorn-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /api
            pathType: Prefix
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /management.ManagementService/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-relay
                port:
                  number: 80
            path: /relay
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-signal
                port:
                  number: 80
            path: /signalexchange.SignalExchange/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.12.0
    helm.sh/chart: netbird-dashboard-1.2.0
  name: netbird-dashboard
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-dashboard
                port:
                  number: 80
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://nextcloud.com/wp-content/uploads/2022/08/nextcloud-logo-icon.svg
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 32.0.0
    helm.sh/chart: nextcloud-8.3.0
  name: nextcloud
  namespace: nextcloud
spec:
  rules:
    - host: nextcloud.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: nextcloud
                port:
                  number: 8080
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - nextcloud.homelab.olav.ninja
      secretName: nextcloud-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.middlewares: traefik-sablier-ollama-openwebui@kubernetescrd
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.12.2
    helm.sh/chart: ollama-1.30.0
  name: ollama
  namespace: ollama
spec:
  rules:
    - host: ollama.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: ollama
                port:
                  number: 11434
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - ollama.homelab.olav.ninja
      secretName: ollama-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://avatars.githubusercontent.com/u/158137808
    traefik.ingress.kubernetes.io/router.middlewares: traefik-sablier-ollama-openwebui@kubernetescrd
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: open-webui
    app.kubernetes.io/version: 0.6.32
    helm.sh/chart: open-webui-8.9.0
  name: open-webui
  namespace: openwebui
spec:
  rules:
    - host: openwebui.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: open-webui
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - openwebui.homelab.olav.ninja
      secretName: openwebui-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Pingvin Share
    hajimari.io/enable: "true"
    hajimari.io/icon: https://user-images.githubusercontent.com/58886915/166198400-c2134044-1198-4647-a8b6-da9c4a204c68.svg
    traefik.ingress.kubernetes.io/router.middlewares: traefik-sablier-pingvinshare@kubernetescrd
  labels:
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pingvinshare
    helm.sh/chart: app-template-4.3.0
  name: pingvinshare
  namespace: pingvinshare
spec:
  rules:
    - host: pingvin.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: pingvinshare
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - pingvin.homelab.olav.ninja
      secretName: pingvinshare-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-37.1.2
  name: traefik
spec:
  controller: traefik.io/ingress-controller
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.3
    helm.sh/chart: valkey-3.0.31
  name: gitea-valkey
  namespace: gitea
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis
  namespace: immich
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 8.0.1
    helm.sh/chart: redis-21.1.3
  name: nextcloud-redis
  namespace: nextcloud
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: Client
metadata:
  name: builtin-homelab-realm-management
  namespace: keycloak
spec:
  forProvider:
    clientId: realm-management
    realmIdRef:
      name: homelab
  managementPolicies:
    - Observe
  providerConfigRef:
    name: default
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: ClientScope
metadata:
  name: netbird-api
  namespace: netbird
spec:
  forProvider:
    consentScreenText: Netbird Management API
    includeInTokenScope: true
    name: netbird-api
    realmIdRef:
      name: homelab
---
apiVersion: pkg.crossplane.io/v1
kind: Provider
metadata:
  name: provider-keycloak
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/provider-keycloak:v2.6.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-auto-ready
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-auto-ready:v0.5.1
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-go-templating
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-go-templating:v0.11.0
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  bootstrap:
    initdb:
      database: gitea
      owner: gitea
  instances: 1
  postgresql:
    parameters:
      timezone: Europe/Oslo
  storage:
    pvcTemplate:
      accessModes:
        - ReadWriteOnce
      storageClassName: longhorn-static
      volumeName: gitea-postgresql
    size: 5G
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: immich-postgresql
  namespace: immich
spec:
  bootstrap:
    initdb:
      database: immich
      owner: immich
      postInitSQL:
        - CREATE EXTENSION IF NOT EXISTS "vectors";
        - CREATE EXTENSION IF NOT EXISTS "cube" CASCADE;
        - CREATE EXTENSION IF NOT EXISTS "earthdistance" CASCADE;
      secret:
        name: immich-postgresql-user
  imageName: ghcr.io/tensorchord/cloudnative-pgvecto.rs:16.5-v0.3.0@sha256:be3f025d79aa1b747817f478e07e71be43236e14d00d8a9eb3914146245035ba
  instances: 1
  managed:
    roles:
      - login: true
        name: immich
        superuser: true
  postgresql:
    parameters:
      timezone: Europe/Oslo
    shared_preload_libraries:
      - vectors.so
  storage:
    pvcTemplate:
      accessModes:
        - ReadWriteOnce
      storageClassName: longhorn-static
      volumeName: immich-postgresql
    size: 5G
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: keycloak-postgresql
  namespace: keycloak
spec:
  bootstrap:
    initdb:
      database: keycloak
      owner: keycloak
  instances: 1
  postgresql:
    parameters:
      timezone: Europe/Oslo
  storage:
    size: 5G
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: nextcloud-postgresql
  namespace: nextcloud
spec:
  bootstrap:
    initdb:
      database: nextcloud
      owner: nextcloud
  instances: 1
  postgresql:
    parameters:
      timezone: Europe/Oslo
  storage:
    pvcTemplate:
      accessModes:
        - ReadWriteOnce
      storageClassName: longhorn-static
      volumeName: nextcloud-postgresql
    size: 5G
---
apiVersion: pubip.olav.ninja/v1
kind: PublicIPUpdater
metadata:
  name: external-dns
  namespace: pubip-operator
spec:
  sources:
    - akami
    - aws_checkip
    - ipify
    - ipinfo
  targets:
    - apiVersion: externaldns.k8s.io/v1alpha1
      fieldPath: spec.endpoints[*].targets[0]
      kind: DNSEndpoint
      name: public-services
      namespace: external-dns
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: KeystoreRsa
metadata:
  name: jwt-signing-certificate
  namespace: keycloak
spec:
  forProvider:
    active: true
    algorithm: RS256
    certificateSecretRef:
      key: tls.crt
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    enabled: true
    name: jwt-signing-certificate
    priority: 110
    privateKeySecretRef:
      key: tls.key
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    providerId: rsa
    realmIdRef:
      name: homelab
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: Realm
metadata:
  name: homelab
  namespace: keycloak
spec:
  forProvider:
    realm: homelab
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: gitea-ssh
  namespace: gitea
spec:
  entryPoints:
    - ssh
  routes:
    - match: HostSNI(`*`)
      services:
        - name: gitea-ssh
          port: 22
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: homeassistant
  namespace: homeassistant
spec:
  entryPoints:
    - websecure
  routes:
    - match: HostSNI(`homeassistant.homelab.olav.ninja`)
      services:
        - name: homeassistant
          port: 443
  tls:
    passthrough: true
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: external-cluster-ingressroute
  namespace: traefik
spec:
  entryPoints:
    - webpublic
  routes:
    - match: HostSNIRegexp(`jiyoung.cloud|{subdomain:[a-z]+}.jiyoung.cloud`)
      services:
        - name: external-cluster
          port: ingress-port
  tls:
    passthrough: true
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: login-redirect-keycloak
  namespace: gitea
spec:
  redirectRegex:
    permanent: false
    regex: ^(https?://[^/]+)/user/login(\?.*)?$
    replacement: ${1}/user/oauth2/keycloak${2}
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: sablier-bambustudio
  namespace: traefik
spec:
  plugin:
    sablier:
      dynamic:
        displayName: Bambu Studio
        refreshFrequency: 5s
        showDetails: "true"
        theme: ghost
      group: bambustudio
      sablierUrl: http://sablier-sablier.sablier.svc.cluster.local:10000
      sessionDuration: 1h
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: sablier-gitea
  namespace: traefik
spec:
  plugin:
    sablier:
      dynamic:
        displayName: Gitea
        refreshFrequency: 5s
        showDetails: "true"
        theme: ghost
      group: gitea
      sablierUrl: http://sablier-sablier.sablier.svc.cluster.local:10000
      sessionDuration: 1h
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: sablier-ollama-openwebui
  namespace: traefik
spec:
  plugin:
    sablier:
      dynamic:
        displayName: Ollama / Open WebUI
        refreshFrequency: 5s
        showDetails: "true"
        theme: ghost
      group: ollama-openwebui
      sablierUrl: http://sablier-sablier.sablier.svc.cluster.local:10000
      sessionDuration: 1h
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: sablier-pingvinshare
  namespace: traefik
spec:
  plugin:
    sablier:
      dynamic:
        displayName: Pingvin Share
        refreshFrequency: 5s
        showDetails: "true"
        theme: ghost
      group: pingvinshare
      sablierUrl: http://sablier-sablier.sablier.svc.cluster.local:10000
      sessionDuration: 1h
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: securityheaders
  namespace: traefik
spec:
  headers:
    customFrameOptionsValue: SAMEORIGIN
    forceSTSHeader: false
    referrerPolicy: same-origin
    sslRedirect: true
    stsPreload: false
    stsSeconds: 15552000
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: Groups
metadata:
  name: homelab-jiyoung
  namespace: keycloak
spec:
  forProvider:
    exhaustive: true
    groupIdsRefs:
      - name: homelab
    realmIdRef:
      name: homelab
    userIdRef:
      name: jiyoung
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: Groups
metadata:
  name: homelab-olav
  namespace: keycloak
spec:
  forProvider:
    exhaustive: true
    groupIdsRefs:
      - name: homelab
    realmIdRef:
      name: homelab
    userIdRef:
      name: olav
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: jiyoung
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - valueSecretRef:
          key: jiyoung
          name: keycloak-initial-passwords
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: jiyoung
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: olav
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - valueSecretRef:
          key: olav
          name: keycloak-initial-passwords
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: olav
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.24.6
    helm.sh/chart: gitea-12.3.0
    version: 1.24.6
  name: gitea-test-connection
  namespace: gitea
spec:
  containers:
    - args:
        - gitea-http:3000
      command:
        - wget
      image: busybox:latest
      name: wget
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.12.2
    helm.sh/chart: ollama-1.30.0
  name: ollama-test-connection
  namespace: ollama
spec:
  containers:
    - args:
        - ollama:11434
      command:
        - wget
      image: busybox
      name: wget
  restartPolicy: Never
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /mutate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    rules:
      - apiGroups:
          - cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
        resources:
          - certificaterequests
    sideEffects: None
    timeoutSeconds: 30
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-mutating-webhook-configuration
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-backup
        port: 443
    failurePolicy: Fail
    name: mbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - backups
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-cluster
        port: 443
    failurePolicy: Fail
    name: mcluster.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - clusters
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-database
        port: 443
    failurePolicy: Fail
    name: mdatabase.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - databases
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-scheduledbackup
        port: 443
    failurePolicy: Fail
    name: mscheduledbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - scheduledbackups
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: netbird/netbird-kubernetes-operator-serving-cert
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-mpod-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: netbird-kubernetes-operator-webhook-service
        namespace: netbird
        path: /mutate--v1-pod
    failurePolicy: Fail
    name: mpod-v1.netbird.io
    objectSelector:
      matchExpressions:
        - key: app.kubernetes.io/name
          operator: NotIn
          values:
            - kubernetes-operator
    rules:
      - apiGroups:
          - ""
        apiVersions:
          - v1
        operations:
          - CREATE
        resources:
          - pods
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.2
    helm.sh/chart: cert-manager-v1.18.2
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /validate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    namespaceSelector:
      matchExpressions:
        - key: cert-manager.io/disable-validation
          operator: NotIn
          values:
            - "true"
    rules:
      - apiGroups:
          - cert-manager.io
          - acme.cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - '*/*'
    sideEffects: None
    timeoutSeconds: 30
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.27.0
    helm.sh/chart: cloudnative-pg-0.26.0
  name: cnpg-validating-webhook-configuration
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-backup
        port: 443
    failurePolicy: Fail
    name: vbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - backups
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-cluster
        port: 443
    failurePolicy: Fail
    name: vcluster.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - clusters
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-scheduledbackup
        port: 443
    failurePolicy: Fail
    name: vscheduledbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - scheduledbackups
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-database
        port: 443
    failurePolicy: Fail
    name: vdatabase.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - databases
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-pooler
        port: 443
    failurePolicy: Fail
    name: vpooler.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - poolers
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: netbird/netbird-kubernetes-operator-serving-cert
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-vnbgroup-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: netbird-kubernetes-operator-webhook-service
        namespace: netbird
        path: /validate-netbird-io-v1-nbgroup
    failurePolicy: Fail
    name: vnbgroup-v1.netbird.io
    rules:
      - apiGroups:
          - netbird.io
        apiVersions:
          - v1
        operations:
          - DELETE
        resources:
          - nbgroups
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: netbird/netbird-kubernetes-operator-serving-cert
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-vnbresource-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: netbird-kubernetes-operator-webhook-service
        namespace: netbird
        path: /validate-netbird-io-v1-nbresource
    failurePolicy: Fail
    name: vnbresource-v1.netbird.io
    rules:
      - apiGroups:
          - netbird.io
        apiVersions:
          - v1
        operations:
          - DELETE
        resources:
          - nbresources
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: netbird/netbird-kubernetes-operator-serving-cert
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-vnbroutingpeer-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: netbird-kubernetes-operator-webhook-service
        namespace: netbird
        path: /validate-netbird-io-v1-nbroutingpeer
    failurePolicy: Fail
    name: vnbroutingpeer-v1.netbird.io
    rules:
      - apiGroups:
          - netbird.io
        apiVersions:
          - v1
        operations:
          - DELETE
        resources:
          - nbroutingpeers
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: netbird/netbird-kubernetes-operator-serving-cert
  labels:
    app.kubernetes.io/instance: netbird
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubernetes-operator
    app.kubernetes.io/version: 0.1.4
    helm.sh/chart: kubernetes-operator-0.1.13
  name: netbird-kubernetes-operator-vnbsetupkey-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: netbird-kubernetes-operator-webhook-service
        namespace: netbird
        path: /validate-netbird-io-v1-nbsetupkey
    failurePolicy: Fail
    name: vnbsetupkey-v1.netbird.io
    rules:
      - apiGroups:
          - netbird.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - nbsetupkeys
    sideEffects: None
