apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
---
apiVersion: v1
kind: Namespace
metadata:
  name: cnpg-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: coturn
---
apiVersion: v1
kind: Namespace
metadata:
  name: crossplane
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: csi-proxmox
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: Namespace
metadata:
  name: hajimari
---
apiVersion: v1
kind: Namespace
metadata:
  name: immich
---
apiVersion: v1
kind: Namespace
metadata:
  name: keycloak
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: kube-system
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: netbird
---
apiVersion: v1
kind: Namespace
metadata:
  name: nextcloud
---
apiVersion: v1
kind: Namespace
metadata:
  name: ollama
---
apiVersion: v1
kind: Namespace
metadata:
  name: openwebui
---
apiVersion: v1
kind: Namespace
metadata:
  name: pingvinshare
---
apiVersion: v1
kind: Namespace
metadata:
  name: sealed-secrets
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: traefik
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: proxmox-csi
mountOptions:
  - noatime
parameters:
  cache: writethrough
  csi.storage.k8s.io/fstype: ext4
  ssd: "true"
  storage: local-zfs
provisioner: csi.proxmox.sinextra.dev
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-startupapicheck
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook
  namespace: cert-manager
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-cloudnative-pg
  namespace: cnpg-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: rbac-manager
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-primary
  namespace: gitea
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-envoy
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-operator
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-generate-certs
  namespace: kube-system
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  name: hubble-relay
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-ui
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.14
  name: proxmox-cloud-controller-manager
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.9.0
    helm.sh/chart: ollama-1.19.0
  name: ollama
  namespace: ollama
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.13
    helm.sh/chart: open-webui-6.19.0
  name: open-webui
  namespace: openwebui
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets
  namespace: sealed-secrets
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
  namespace: traefik
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-cainjector-leader-election
      - cert-manager-cainjector-leader-election-core
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificaterequests
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-tokenrequest
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager-webhook-ca
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-controller
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - watch
      - list
      - delete
      - update
      - create
  - apiGroups:
      - storage.k8s.io
    resources:
      - csistoragecapacities
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator-tlsinterception-secrets
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - delete
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-tlsinterception-secrets
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - ""
    resourceNames:
      - hubble-server-certs
      - hubble-relay-client-certs
      - hubble-relay-server-certs
      - hubble-metrics-server-certs
      - hubble-ui-client-certs
    resources:
      - secrets
    verbs:
      - update
  - apiGroups:
      - ""
    resourceNames:
      - cilium-ca
    resources:
      - secrets
    verbs:
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - sealed-secrets-key
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - 'http:sealed-secrets:'
      - http:sealed-secrets:http
      - sealed-secrets
    resources:
      - services/proxy
    verbs:
      - create
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
rules:
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
      - create
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - csinodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattributesclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattachments
    verbs:
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattachments/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - create
      - update
      - patch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
  name: cert-manager-cluster-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-approve:cert-manager-io
rules:
  - apiGroups:
      - cert-manager.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - approve
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-certificates
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificates/status
      - certificaterequests
      - certificaterequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/finalizers
      - certificaterequests/finalizers
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
    verbs:
      - create
      - delete
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-certificatesigningrequests
rules:
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - certificates.k8s.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - sign
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-clusterissuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - clusterissuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-ingress-shim
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
    verbs:
      - create
      - update
      - delete
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/finalizers
    verbs:
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways
      - httproutes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways/finalizers
      - httproutes/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-issuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - issuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-orders
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - orders/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - create
      - delete
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-dns01-controller-challenges
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - challenges/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: cert-manager-edit
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/status
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-http01-controller-challenges
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - challenges/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - httproutes
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - route.openshift.io
    resources:
      - routes/custom-host
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: cert-manager-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook:subjectaccessreviews
rules:
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - services
      - pods
      - endpoints
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
      - list
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - list
      - watch
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumbgppeeringpolicies
      - ciliumbgpnodeconfigs
      - ciliumbgpadvertisements
      - ciliumbgppeerconfigs
      - ciliumclusterwideenvoyconfigs
      - ciliumclusterwidenetworkpolicies
      - ciliumegressgatewaypolicies
      - ciliumendpoints
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumidentities
      - ciliumlocalredirectpolicies
      - ciliumnetworkpolicies
      - ciliumnodes
      - ciliumnodeconfigs
      - ciliumcidrgroups
      - ciliuml2announcementpolicies
      - ciliumpodippools
    verbs:
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
      - ciliumendpoints
      - ciliumnodes
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
    verbs:
      - delete
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
      - ciliumnodes/status
    verbs:
      - get
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints/status
      - ciliumendpoints
      - ciliuml2announcementpolicies/status
      - ciliumbgpnodeconfigs/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - ""
    resourceNames:
      - cilium-config
    resources:
      - configmaps
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services/status
    verbs:
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies
      - ciliumclusterwidenetworkpolicies
    verbs:
      - create
      - update
      - deletecollection
      - patch
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies/status
      - ciliumclusterwidenetworkpolicies/status
    verbs:
      - patch
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
      - ciliumidentities
    verbs:
      - delete
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes/status
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumbgppeerconfigs
      - ciliumbgpadvertisements
      - ciliumbgpnodeconfigs
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
      - patch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumbgpclusterconfigs/status
      - ciliumbgppeerconfigs/status
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - create
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resourceNames:
      - ciliumloadbalancerippools.cilium.io
      - ciliumbgppeeringpolicies.cilium.io
      - ciliumbgpclusterconfigs.cilium.io
      - ciliumbgppeerconfigs.cilium.io
      - ciliumbgpadvertisements.cilium.io
      - ciliumbgpnodeconfigs.cilium.io
      - ciliumbgpnodeconfigoverrides.cilium.io
      - ciliumclusterwideenvoyconfigs.cilium.io
      - ciliumclusterwidenetworkpolicies.cilium.io
      - ciliumegressgatewaypolicies.cilium.io
      - ciliumendpoints.cilium.io
      - ciliumendpointslices.cilium.io
      - ciliumenvoyconfigs.cilium.io
      - ciliumexternalworkloads.cilium.io
      - ciliumidentities.cilium.io
      - ciliumlocalredirectpolicies.cilium.io
      - ciliumnetworkpolicies.cilium.io
      - ciliumnodes.cilium.io
      - ciliumnodeconfigs.cilium.io
      - ciliumcidrgroups.cilium.io
      - ciliuml2announcementpolicies.cilium.io
      - ciliumpodippools.cilium.io
    resources:
      - customresourcedefinitions
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumpodippools
      - ciliumbgppeeringpolicies
      - ciliumbgpclusterconfigs
      - ciliumbgpnodeconfigoverrides
      - ciliumbgppeerconfigs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumpodippools
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools/status
    verbs:
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-cloudnative-pg
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - mutatingwebhookconfigurations
      - validatingwebhookconfigurations
    verbs:
      - get
      - patch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusterimagecatalogs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
      - secrets
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps/status
      - secrets/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
      - pods
      - pods/exec
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - watch
  - apiGroups:
      - ""
    resources:
      - pods/status
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - create
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups
      - clusters
      - databases
      - poolers
      - publications
      - scheduledbackups
      - subscriptions
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups/status
      - databases/status
      - publications/status
      - scheduledbackups/status
      - subscriptions/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - imagecatalogs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusters/finalizers
      - poolers/finalizers
    verbs:
      - update
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusters/status
      - poolers/status
    verbs:
      - get
      - patch
      - update
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - rolebindings
      - roles
    verbs:
      - create
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshots
    verbs:
      - create
      - get
      - list
      - patch
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-cloudnative-pg-edit
rules:
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups
      - clusters
      - databases
      - poolers
      - publications
      - scheduledbackups
      - subscriptions
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-cloudnative-pg-view
rules:
  - apiGroups:
      - postgresql.cnpg.io
    resources:
      - backups
      - clusters
      - databases
      - poolers
      - publications
      - scheduledbackups
      - subscriptions
    verbs:
      - get
      - list
      - watch
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-crossplane: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-admin
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-browse: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-browse
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-edit: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-edit
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-rbac-manager
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions/finalizers
    verbs:
      - update
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - escalate
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
    verbs:
      - bind
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
    verbs:
      - '*'
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-view: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-view
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-admin: "true"
  name: crossplane:aggregate-to-admin
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
      - namespaces
    verbs:
      - '*'
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - rolebindings
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-browse: "true"
  name: crossplane:aggregate-to-browse
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-edit: "true"
  name: crossplane:aggregate-to-edit
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-view: "true"
  name: crossplane:aggregate-to-view
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-allowed-provider-permissions: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane:allowed-provider-permissions
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    crossplane.io/scope: system
    helm.sh/chart: crossplane-1.20.0
    rbac.crossplane.io/aggregate-to-crossplane: "true"
  name: crossplane:system:aggregate-to-crossplane
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
      - customresourcedefinitions/status
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
      - services
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
      - pkg.crossplane.io
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - extensions
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - delete
      - watch
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
rules:
  - apiGroups:
      - ""
      - extensions
      - networking.k8s.io
      - discovery.k8s.io
    resources:
      - ingresses
      - namespaces
      - endpointslices
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-ui
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - componentstatuses
      - endpoints
      - namespaces
      - nodes
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets-unsealer
rules:
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - update
      - delete
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.14
  name: system:proxmox-cloud-controller-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - create
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - create
      - get
  - apiGroups:
      - ""
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik-traefik
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - nodes
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingressclasses
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.io
    resources:
      - ingressroutes
      - ingressroutetcps
      - ingressrouteudps
      - middlewares
      - middlewaretcps
      - serverstransports
      - serverstransporttcps
      - tlsoptions
      - tlsstores
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-cainjector:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cert-manager-tokenrequest
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-tokenrequest
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-startupapicheck:create-cert
subjects:
  - kind: ServiceAccount
    name: cert-manager-startupapicheck
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-webhook:dynamic-serving
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxmox-csi-plugin-controller
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-controller
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-config-agent
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator-tlsinterception-secrets
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-operator-tlsinterception-secrets
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-tlsinterception-secrets
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-tlsinterception-secrets
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hubble-generate-certs
subjects:
  - kind: ServiceAccount
    name: hubble-generate-certs
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: system:proxmox-cloud-controller-manager:extension-apiserver-authentication-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
  - kind: ServiceAccount
    name: proxmox-cloud-controller-manager
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-key-admin
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-service-proxier
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:authenticated
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-cainjector
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-approve:cert-manager-io
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-approve:cert-manager-io
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-certificates
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificates
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-certificatesigningrequests
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificatesigningrequests
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-clusterissuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-clusterissuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-ingress-shim
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-ingress-shim
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-issuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-issuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-controller-orders
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-orders
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-dns01-controller-challenges
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-dns01-controller-challenges
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-http01-controller-challenges
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-http01-controller-challenges
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook:subjectaccessreviews
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-webhook:subjectaccessreviews
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium-operator
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-cloudnative-pg
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cnpg-cloudnative-pg
subjects:
  - kind: ServiceAccount
    name: cnpg-cloudnative-pg
    namespace: cnpg-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane
subjects:
  - kind: ServiceAccount
    name: crossplane
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: crossplane:masters
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
  name: crossplane-rbac-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-rbac-manager
subjects:
  - kind: ServiceAccount
    name: rbac-manager
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hajimari
subjects:
  - kind: ServiceAccount
    name: hajimari
    namespace: hajimari
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-ui
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hubble-ui
subjects:
  - kind: ServiceAccount
    name: hubble-ui
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oidc-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:olav
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:jiyoung
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxmox-csi-plugin-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxmox-csi-plugin-controller
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-controller
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxmox-csi-plugin-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxmox-csi-plugin-node
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-node
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets-sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sealed-secrets-unsealer
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:proxmox-cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:proxmox-cloud-controller-manager
subjects:
  - kind: ServiceAccount
    name: proxmox-cloud-controller-manager
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik-traefik
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-traefik
subjects:
  - kind: ServiceAccount
    name: traefik
    namespace: traefik
---
apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-controller-manager-config
  namespace: cnpg-system
---
apiVersion: v1
data:
  queries: |
    backends:
      query: |
       SELECT sa.datname
           , sa.usename
           , sa.application_name
           , states.state
           , COALESCE(sa.count, 0) AS total
           , COALESCE(sa.max_tx_secs, 0) AS max_tx_duration_seconds
           FROM ( VALUES ('active')
               , ('idle')
               , ('idle in transaction')
               , ('idle in transaction (aborted)')
               , ('fastpath function call')
               , ('disabled')
               ) AS states(state)
           LEFT JOIN (
               SELECT datname
                   , state
                   , usename
                   , COALESCE(application_name, '') AS application_name
                   , COUNT(*)
                   , COALESCE(EXTRACT (EPOCH FROM (max(now() - xact_start))), 0) AS max_tx_secs
               FROM pg_catalog.pg_stat_activity
               GROUP BY datname, state, usename, application_name
           ) sa ON states.state = sa.state
           WHERE sa.usename IS NOT NULL
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of the database"
        - usename:
            usage: "LABEL"
            description: "Name of the user"
        - application_name:
            usage: "LABEL"
            description: "Name of the application"
        - state:
            usage: "LABEL"
            description: "State of the backend"
        - total:
            usage: "GAUGE"
            description: "Number of backends"
        - max_tx_duration_seconds:
            usage: "GAUGE"
            description: "Maximum duration of a transaction in seconds"

    backends_waiting:
      query: |
       SELECT count(*) AS total
       FROM pg_catalog.pg_locks blocked_locks
       JOIN pg_catalog.pg_locks blocking_locks
         ON blocking_locks.locktype = blocked_locks.locktype
         AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
         AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
         AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
         AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
         AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
         AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
         AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
         AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
         AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
         AND blocking_locks.pid != blocked_locks.pid
       JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
       WHERE NOT blocked_locks.granted
      metrics:
        - total:
            usage: "GAUGE"
            description: "Total number of backends that are currently waiting on other queries"

    pg_database:
      query: |
        SELECT datname
          , pg_catalog.pg_database_size(datname) AS size_bytes
          , pg_catalog.age(datfrozenxid) AS xid_age
          , pg_catalog.mxid_age(datminmxid) AS mxid_age
        FROM pg_catalog.pg_database
        WHERE datallowconn
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of the database"
        - size_bytes:
            usage: "GAUGE"
            description: "Disk space used by the database"
        - xid_age:
            usage: "GAUGE"
            description: "Number of transactions from the frozen XID to the current one"
        - mxid_age:
            usage: "GAUGE"
            description: "Number of multiple transactions (Multixact) from the frozen XID to the current one"

    pg_postmaster:
      query: |
        SELECT EXTRACT(EPOCH FROM pg_postmaster_start_time) AS start_time
        FROM pg_catalog.pg_postmaster_start_time()
      metrics:
        - start_time:
            usage: "GAUGE"
            description: "Time at which postgres started (based on epoch)"

    pg_replication:
      query: "SELECT CASE WHEN (
                NOT pg_catalog.pg_is_in_recovery()
                OR pg_catalog.pg_last_wal_receive_lsn() = pg_catalog.pg_last_wal_replay_lsn())
              THEN 0
              ELSE GREATEST (0,
                EXTRACT(EPOCH FROM (now() - pg_catalog.pg_last_xact_replay_timestamp())))
              END AS lag,
              pg_catalog.pg_is_in_recovery() AS in_recovery,
              EXISTS (TABLE pg_stat_wal_receiver) AS is_wal_receiver_up,
              (SELECT count(*) FROM pg_catalog.pg_stat_replication) AS streaming_replicas"
      metrics:
        - lag:
            usage: "GAUGE"
            description: "Replication lag behind primary in seconds"
        - in_recovery:
            usage: "GAUGE"
            description: "Whether the instance is in recovery"
        - is_wal_receiver_up:
            usage: "GAUGE"
            description: "Whether the instance wal_receiver is up"
        - streaming_replicas:
            usage: "GAUGE"
            description: "Number of streaming replicas connected to the instance"

    pg_replication_slots:
      query: |
        SELECT slot_name,
          slot_type,
          database,
          active,
          (CASE pg_catalog.pg_is_in_recovery()
            WHEN TRUE THEN pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_last_wal_receive_lsn(), restart_lsn)
            ELSE pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), restart_lsn)
          END) as pg_wal_lsn_diff
        FROM pg_catalog.pg_replication_slots
        WHERE NOT temporary
      metrics:
        - slot_name:
            usage: "LABEL"
            description: "Name of the replication slot"
        - slot_type:
            usage: "LABEL"
            description: "Type of the replication slot"
        - database:
            usage: "LABEL"
            description: "Name of the database"
        - active:
            usage: "GAUGE"
            description: "Flag indicating whether the slot is active"
        - pg_wal_lsn_diff:
            usage: "GAUGE"
            description: "Replication lag in bytes"

    pg_stat_archiver:
      query: |
        SELECT archived_count
          , failed_count
          , COALESCE(EXTRACT(EPOCH FROM (now() - last_archived_time)), -1) AS seconds_since_last_archival
          , COALESCE(EXTRACT(EPOCH FROM (now() - last_failed_time)), -1) AS seconds_since_last_failure
          , COALESCE(EXTRACT(EPOCH FROM last_archived_time), -1) AS last_archived_time
          , COALESCE(EXTRACT(EPOCH FROM last_failed_time), -1) AS last_failed_time
          , COALESCE(CAST(CAST('x'||pg_catalog.right(pg_catalog.split_part(last_archived_wal, '.', 1), 16) AS pg_catalog.bit(64)) AS pg_catalog.int8), -1) AS last_archived_wal_start_lsn
          , COALESCE(CAST(CAST('x'||pg_catalog.right(pg_catalog.split_part(last_failed_wal, '.', 1), 16) AS pg_catalog.bit(64)) AS pg_catalog.int8), -1) AS last_failed_wal_start_lsn
          , EXTRACT(EPOCH FROM stats_reset) AS stats_reset_time
        FROM pg_catalog.pg_stat_archiver
      metrics:
        - archived_count:
            usage: "COUNTER"
            description: "Number of WAL files that have been successfully archived"
        - failed_count:
            usage: "COUNTER"
            description: "Number of failed attempts for archiving WAL files"
        - seconds_since_last_archival:
            usage: "GAUGE"
            description: "Seconds since the last successful archival operation"
        - seconds_since_last_failure:
            usage: "GAUGE"
            description: "Seconds since the last failed archival operation"
        - last_archived_time:
            usage: "GAUGE"
            description: "Epoch of the last time WAL archiving succeeded"
        - last_failed_time:
            usage: "GAUGE"
            description: "Epoch of the last time WAL archiving failed"
        - last_archived_wal_start_lsn:
            usage: "GAUGE"
            description: "Archived WAL start LSN"
        - last_failed_wal_start_lsn:
            usage: "GAUGE"
            description: "Last failed WAL LSN"
        - stats_reset_time:
            usage: "GAUGE"
            description: "Time at which these statistics were last reset"

    pg_stat_bgwriter:
      runonserver: "<17.0.0"
      query: |
        SELECT checkpoints_timed
          , checkpoints_req
          , checkpoint_write_time
          , checkpoint_sync_time
          , buffers_checkpoint
          , buffers_clean
          , maxwritten_clean
          , buffers_backend
          , buffers_backend_fsync
          , buffers_alloc
        FROM pg_catalog.pg_stat_bgwriter
      metrics:
        - checkpoints_timed:
            usage: "COUNTER"
            description: "Number of scheduled checkpoints that have been performed"
        - checkpoints_req:
            usage: "COUNTER"
            description: "Number of requested checkpoints that have been performed"
        - checkpoint_write_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in milliseconds"
        - checkpoint_sync_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in milliseconds"
        - buffers_checkpoint:
            usage: "COUNTER"
            description: "Number of buffers written during checkpoints"
        - buffers_clean:
            usage: "COUNTER"
            description: "Number of buffers written by the background writer"
        - maxwritten_clean:
            usage: "COUNTER"
            description: "Number of times the background writer stopped a cleaning scan because it had written too many buffers"
        - buffers_backend:
            usage: "COUNTER"
            description: "Number of buffers written directly by a backend"
        - buffers_backend_fsync:
            usage: "COUNTER"
            description: "Number of times a backend had to execute its own fsync call (normally the background writer handles those even when the backend does its own write)"
        - buffers_alloc:
            usage: "COUNTER"
            description: "Number of buffers allocated"

    pg_stat_bgwriter_17:
      runonserver: ">=17.0.0"
      name: pg_stat_bgwriter
      query: |
        SELECT buffers_clean
          , maxwritten_clean
          , buffers_alloc
          , EXTRACT(EPOCH FROM stats_reset) AS stats_reset_time
        FROM pg_catalog.pg_stat_bgwriter
      metrics:
        - buffers_clean:
            usage: "COUNTER"
            description: "Number of buffers written by the background writer"
        - maxwritten_clean:
            usage: "COUNTER"
            description: "Number of times the background writer stopped a cleaning scan because it had written too many buffers"
        - buffers_alloc:
            usage: "COUNTER"
            description: "Number of buffers allocated"
        - stats_reset_time:
            usage: "GAUGE"
            description: "Time at which these statistics were last reset"

    pg_stat_checkpointer:
      runonserver: ">=17.0.0"
      query: |
        SELECT num_timed AS checkpoints_timed
          , num_requested AS checkpoints_req
          , restartpoints_timed
          , restartpoints_req
          , restartpoints_done
          , write_time
          , sync_time
          , buffers_written
          , EXTRACT(EPOCH FROM stats_reset) AS stats_reset_time
        FROM pg_catalog.pg_stat_checkpointer
      metrics:
        - checkpoints_timed:
            usage: "COUNTER"
            description: "Number of scheduled checkpoints that have been performed"
        - checkpoints_req:
            usage: "COUNTER"
            description: "Number of requested checkpoints that have been performed"
        - restartpoints_timed:
            usage: "COUNTER"
            description: "Number of scheduled restartpoints due to timeout or after a failed attempt to perform it"
        - restartpoints_req:
            usage: "COUNTER"
            description: "Number of requested restartpoints that have been performed"
        - restartpoints_done:
            usage: "COUNTER"
            description: "Number of restartpoints that have been performed"
        - write_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of processing checkpoints and restartpoints where files are written to disk, in milliseconds"
        - sync_time:
            usage: "COUNTER"
            description: "Total amount of time that has been spent in the portion of processing checkpoints and restartpoints where files are synchronized to disk, in milliseconds"
        - buffers_written:
            usage: "COUNTER"
            description: "Number of buffers written during checkpoints and restartpoints"
        - stats_reset_time:
            usage: "GAUGE"
            description: "Time at which these statistics were last reset"

    pg_stat_database:
      query: |
        SELECT datname
          , xact_commit
          , xact_rollback
          , blks_read
          , blks_hit
          , tup_returned
          , tup_fetched
          , tup_inserted
          , tup_updated
          , tup_deleted
          , conflicts
          , temp_files
          , temp_bytes
          , deadlocks
          , blk_read_time
          , blk_write_time
        FROM pg_catalog.pg_stat_database
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of this database"
        - xact_commit:
            usage: "COUNTER"
            description: "Number of transactions in this database that have been committed"
        - xact_rollback:
            usage: "COUNTER"
            description: "Number of transactions in this database that have been rolled back"
        - blks_read:
            usage: "COUNTER"
            description: "Number of disk blocks read in this database"
        - blks_hit:
            usage: "COUNTER"
            description: "Number of times disk blocks were found already in the buffer cache, so that a read was not necessary (this only includes hits in the PostgreSQL buffer cache, not the operating system's file system cache)"
        - tup_returned:
            usage: "COUNTER"
            description: "Number of rows returned by queries in this database"
        - tup_fetched:
            usage: "COUNTER"
            description: "Number of rows fetched by queries in this database"
        - tup_inserted:
            usage: "COUNTER"
            description: "Number of rows inserted by queries in this database"
        - tup_updated:
            usage: "COUNTER"
            description: "Number of rows updated by queries in this database"
        - tup_deleted:
            usage: "COUNTER"
            description: "Number of rows deleted by queries in this database"
        - conflicts:
            usage: "COUNTER"
            description: "Number of queries canceled due to conflicts with recovery in this database"
        - temp_files:
            usage: "COUNTER"
            description: "Number of temporary files created by queries in this database"
        - temp_bytes:
            usage: "COUNTER"
            description: "Total amount of data written to temporary files by queries in this database"
        - deadlocks:
            usage: "COUNTER"
            description: "Number of deadlocks detected in this database"
        - blk_read_time:
            usage: "COUNTER"
            description: "Time spent reading data file blocks by backends in this database, in milliseconds"
        - blk_write_time:
            usage: "COUNTER"
            description: "Time spent writing data file blocks by backends in this database, in milliseconds"

    pg_stat_replication:
      primary: true
      query: |
       SELECT usename
         , COALESCE(application_name, '') AS application_name
         , COALESCE(client_addr::text, '') AS client_addr
         , COALESCE(client_port::text, '') AS client_port
         , EXTRACT(EPOCH FROM backend_start) AS backend_start
         , COALESCE(pg_catalog.age(backend_xmin), 0) AS backend_xmin_age
         , pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), sent_lsn) AS sent_diff_bytes
         , pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), write_lsn) AS write_diff_bytes
         , pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), flush_lsn) AS flush_diff_bytes
         , COALESCE(pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), replay_lsn),0) AS replay_diff_bytes
         , COALESCE((EXTRACT(EPOCH FROM write_lag)),0)::float AS write_lag_seconds
         , COALESCE((EXTRACT(EPOCH FROM flush_lag)),0)::float AS flush_lag_seconds
         , COALESCE((EXTRACT(EPOCH FROM replay_lag)),0)::float AS replay_lag_seconds
       FROM pg_catalog.pg_stat_replication
      metrics:
        - usename:
            usage: "LABEL"
            description: "Name of the replication user"
        - application_name:
            usage: "LABEL"
            description: "Name of the application"
        - client_addr:
            usage: "LABEL"
            description: "Client IP address"
        - client_port:
            usage: "LABEL"
            description: "Client TCP port"
        - backend_start:
            usage: "COUNTER"
            description: "Time when this process was started"
        - backend_xmin_age:
            usage: "COUNTER"
            description: "The age of this standby's xmin horizon"
        - sent_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location sent on this connection"
        - write_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location written to disk by this standby server"
        - flush_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location flushed to disk by this standby server"
        - replay_diff_bytes:
            usage: "GAUGE"
            description: "Difference in bytes from the last write-ahead log location replayed into the database on this standby server"
        - write_lag_seconds:
            usage: "GAUGE"
            description: "Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written it"
        - flush_lag_seconds:
            usage: "GAUGE"
            description: "Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written and flushed it"
        - replay_lag_seconds:
            usage: "GAUGE"
            description: "Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written, flushed and applied it"

    pg_settings:
      query: |
        SELECT name,
        CASE setting WHEN 'on' THEN '1' WHEN 'off' THEN '0' ELSE setting END AS setting
        FROM pg_catalog.pg_settings
        WHERE vartype IN ('integer', 'real', 'bool')
        ORDER BY 1
      metrics:
        - name:
            usage: "LABEL"
            description: "Name of the setting"
        - setting:
            usage: "GAUGE"
            description: "Setting value"

    pg_extensions:
      query: |
        SELECT
         current_database() as datname,
         name as extname,
         default_version,
         installed_version,
         CASE
           WHEN default_version = installed_version THEN 0
           ELSE 1
        END AS update_available
        FROM pg_catalog.pg_available_extensions
        WHERE installed_version IS NOT NULL
      metrics:
        - datname:
            usage: "LABEL"
            description: "Name of the database"
        - extname:
            usage: "LABEL"
            description: "Extension name"
        - default_version:
            usage: "LABEL"
            description: "Default version"
        - installed_version:
            usage: "LABEL"
            description: "Installed version"
        - update_available:
            usage: "GAUGE"
            description: "An update is available"
      target_databases:
        - '*'
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    cnpg.io/reload: ""
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-default-monitoring
  namespace: cnpg-system
---
apiVersion: v1
data:
  turnserver.conf: "# Coturn TURN SERVER configuration file\n#\n# Boolean values note: where a boolean value is supposed to be used,\n# you can use '0', 'off', 'no', 'false', or 'f' as 'false',\n# and you can use '1', 'on', 'yes', 'true', or 't' as 'true'\n# If the value is missing, then it means 'true' by default.\n#\n\n# Listener interface device (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#listening-device=eth0\n\n# TURN listener port for UDP and TCP (Default: 3478).\n# Note: actually, TLS & DTLS sessions can connect to the\n# \"plain\" TCP & UDP port(s), too - if allowed by configuration.\n#\n#listening-port=3478\n\n# TURN listener port for TLS (Default: 5349).\n# Note: actually, \"plain\" TCP & UDP sessions can connect to the TLS & DTLS\n# port(s), too - if allowed by configuration. The TURN server\n# \"automatically\" recognizes the type of traffic. Actually, two listening\n# endpoints (the \"plain\" one and the \"tls\" one) are equivalent in terms of\n# functionality; but Coturn keeps both endpoints to satisfy the RFC 5766 specs.\n# For secure TCP connections, Coturn currently supports SSL version 3 and\n# TLS version 1.0, 1.1 and 1.2.\n# For secure UDP connections, Coturn supports DTLS version 1.\n#\n#tls-listening-port=5349\n\n# Alternative listening port for UDP and TCP listeners;\n# default (or zero) value means \"listening port plus one\".\n# This is needed for RFC 5780 support\n# (STUN extension specs, NAT behavior discovery). The TURN Server\n# supports RFC 5780 only if it is started with more than one\n# listening IP address of the same family (IPv4 or IPv6).\n# RFC 5780 is supported only by UDP protocol, other protocols\n# are listening to that endpoint only for \"symmetry\".\n#\n#alt-listening-port=0\n\n# Alternative listening port for TLS and DTLS protocols.\n# Default (or zero) value means \"TLS listening port plus one\".\n#\n#alt-tls-listening-port=0\n\n# Some network setups will require using a TCP reverse proxy in front\n# of the STUN server. If the proxy port option is set a single listener\n# is started on the given port that accepts connections using the\n# haproxy proxy protocol v2.\n# (https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)\n#\n#tcp-proxy-port=5555\n\n# Listener IP address of relay server. Multiple listeners can be specified.\n# If no IP(s) specified in the config file or in the command line options,\n# then all IPv4 and IPv6 system IPs will be used for listening.\n#\n#listening-ip=172.17.19.101\n#listening-ip=10.207.21.238\n#listening-ip=2607:f0d0:1002:51::4\n\n# Auxiliary STUN/TURN server listening endpoint.\n# Aux servers have almost full TURN and STUN functionality.\n# The (minor) limitations are:\n#\n# 1) Auxiliary servers do not have alternative ports and\n# they do not support STUN RFC 5780 functionality (CHANGE REQUEST).\n#\n# 2) Auxiliary servers also are never returning ALTERNATIVE-SERVER reply.\n#\n# Valid formats are 1.2.3.4:5555 for IPv4 and [1:2::3:4]:5555 for IPv6.\n#\n# There may be multiple aux-server options, each will be used for listening\n# to client requests.\n#\n#aux-server=172.17.19.110:33478\n#aux-server=[2607:f0d0:1002:51::4]:33478\n\n# (recommended for older Linuxes only)\n# Automatically balance UDP traffic over auxiliary servers (if configured).\n# The load balancing is using the ALTERNATE-SERVER mechanism.\n# The TURN client must support 300 ALTERNATE-SERVER response for this\n# functionality.\n#\n#udp-self-balance\n\n# Relay interface device for relay sockets (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#relay-device=eth1\n\n# Relay address (the local IP address that will be used to relay the\n# packets to the peer).\n# Multiple relay addresses may be used.\n# The same IP(s) can be used as both listening IP(s) and relay IP(s).\n#\n# If no relay IP(s) specified, then the turnserver will apply the default\n# policy: it will decide itself which relay addresses to be used, and it\n# will always be using the client socket IP address as the relay IP address\n# of the TURN session (if the requested relay address family is the same\n# as the family of the client socket).\n#\n#relay-ip=172.17.19.105\n#relay-ip=2607:f0d0:1002:51::5\n\n# For Amazon EC2 users:\n#\n# TURN Server public/private address mapping, if the server is behind NAT.\n# In that situation, if a -X is used in form \"-X <ip>\" then that ip will be reported\n# as relay IP address of all allocations. This scenario works only in a simple case\n# when one single relay address is be used, and no RFC5780 functionality is required.\n# That single relay address must be mapped by NAT to the 'external' IP.\n# The \"external-ip\" value, if not empty, is returned in XOR-RELAYED-ADDRESS field.\n# For that 'external' IP, NAT must forward ports directly (relayed port 12345\n# must be always mapped to the same 'external' port 12345).\n#\n# In more complex case when more than one IP address is involved,\n# that option must be used several times, each entry must\n# have form \"-X <public-ip/private-ip>\", to map all involved addresses.\n# RFC5780 NAT discovery STUN functionality will work correctly,\n# if the addresses are mapped properly, even when the TURN server itself\n# is behind A NAT.\n#\n# By default, this value is empty, and no address mapping is used.\n#\n#external-ip=60.70.80.91\n#\n#OR:\n#\n#external-ip=60.70.80.91/172.17.19.101\n#external-ip=60.70.80.92/172.17.19.102\n\n\n# Number of the relay threads to handle the established connections\n# (in addition to authentication thread and the listener thread).\n# If explicitly set to 0 then application runs relay process in a\n# single thread, in the same thread with the listener process\n# (the authentication thread will still be a separate thread).\n#\n# If this parameter is not set, then the default OS-dependent\n# thread pattern algorithm will be employed. Usually the default\n# algorithm is optimal, so you have to change this option\n# if you want to make some fine tweaks.\n#\n# In the older systems (Linux kernel before 3.9),\n# the number of UDP threads is always one thread per network listening\n# endpoint - including the auxiliary endpoints - unless 0 (zero) or\n# 1 (one) value is set.\n#\n#relay-threads=0\n\n# Lower and upper bounds of the UDP relay endpoints:\n# (default values are 49152 and 65535)\n#\n#min-port=49152\n#max-port=65535\n\n# Uncomment to run TURN server in 'normal' 'moderate' verbose mode.\n# By default the verbose mode is off.\n#verbose\n\n# Uncomment to run TURN server in 'extra' verbose mode.\n# This mode is very annoying and produces lots of output.\n# Not recommended under normal circumstances.\n#\n#Verbose\n\n# Uncomment to use fingerprints in the TURN messages.\n# By default the fingerprints are off.\n#\n#fingerprint\n\n# Uncomment to use long-term credential mechanism.\n# By default no credentials mechanism is used (any user allowed).\n#\n#lt-cred-mech\n\n# This option is the opposite of lt-cred-mech.\n# (TURN Server with no-auth option allows anonymous access).\n# If neither option is defined, and no users are defined,\n# then no-auth is default. If at least one user is defined,\n# in this file, in command line or in usersdb file, then\n# lt-cred-mech is default.\n#\n#no-auth\n\n# Enable prometheus exporter\n# If enabled the turnserver will expose an endpoint with stats on a prometheus format\n# this endpoint is listening on a different port to not conflict with other configurations.\n#\n# You can simply run the turnserver and access the port 9641 and path /metrics\n#\n# For more info on the prometheus exporter and metrics\n# https://prometheus.io/docs/introduction/overview/\n# https://prometheus.io/docs/concepts/data_model/\n#\n#prometheus\n\n# TURN REST API flag.\n# (Time Limited Long Term Credential)\n# Flag that sets a special authorization option that is based upon authentication secret.\n#\n# This feature's purpose is to support \"TURN Server REST API\", see\n# \"TURN REST API\" link in the project's page\n# https://github.com/coturn/coturn/\n#\n# This option is used with timestamp:\n#\n# usercombo -> \"timestamp:userid\"\n# turn user -> usercombo\n# turn password -> base64(hmac(secret key, usercombo))\n#\n# This allows TURN credentials to be accounted for a specific user id.\n# If you don't have a suitable id, then the timestamp alone can be used.\n# This option is enabled by turning on secret-based authentication.\n# The actual value of the secret is defined either by the option static-auth-secret,\n# or can be found in the turn_secret table in the database (see below).\n#\n# Read more about it:\n#  - https://tools.ietf.org/html/draft-uberti-behave-turn-rest-00\n#  - https://www.ietf.org/proceedings/87/slides/slides-87-behave-10.pdf\n#\n# Be aware that use-auth-secret overrides some parts of lt-cred-mech.\n# The use-auth-secret feature depends internally on lt-cred-mech, so if you set\n# this option then it automatically enables lt-cred-mech internally\n# as if you had enabled both.\n#\n# Note that you can use only one auth mechanism at the same time! This is because,\n# both mechanisms conduct username and password validation in different ways.\n#\n# Use either lt-cred-mech or use-auth-secret in the conf\n# to avoid any confusion.\n#\n#use-auth-secret\n\n# 'Static' authentication secret value (a string) for TURN REST API only.\n# If not set, then the turn server\n# will try to use the 'dynamic' value in the turn_secret table\n# in the user database (if present). The database-stored  value can be changed on-the-fly\n# by a separate program, so this is why that mode is considered 'dynamic'.\n#\n#static-auth-secret=north\n\n# Server name used for\n# the oAuth authentication purposes.\n# The default value is the realm name.\n#\n#server-name=blackdow.carleon.gov\n\n# Flag that allows oAuth authentication.\n#\n#oauth\n\n# 'Static' user accounts for the long term credentials mechanism, only.\n# This option cannot be used with TURN REST API.\n# 'Static' user accounts are NOT dynamically checked by the turnserver process,\n# so they can NOT be changed while the turnserver is running.\n#\n#user=username1:key1\n#user=username2:key2\n# OR:\n#user=username1:password1\n#user=username2:password2\n#\n# Keys must be generated by turnadmin utility. The key value depends\n# on user name, realm, and password:\n#\n# Example:\n# $ turnadmin -k -u ninefingers -r north.gov -p youhavetoberealistic\n# Output: 0xbc807ee29df3c9ffa736523fb2c4e8ee\n# ('0x' in the beginning of the key is what differentiates the key from\n# password. If it has 0x then it is a key, otherwise it is a password).\n#\n# The corresponding user account entry in the config file will be:\n#\n#user=ninefingers:0xbc807ee29df3c9ffa736523fb2c4e8ee\n# Or, equivalently, with open clear password (less secure):\n#user=ninefingers:youhavetoberealistic\n#\n\n# SQLite database file name.\n#\n# The default file name is /var/db/turndb or /usr/local/var/db/turndb or\n# /var/lib/turn/turndb.\n#\n#userdb=/var/db/turndb\n\n# PostgreSQL database connection string in the case that you are using PostgreSQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# See http://www.postgresql.org/docs/8.4/static/libpq-connect.html for 8.x PostgreSQL\n# versions connection string format, see\n# http://www.postgresql.org/docs/9.2/static/libpq-connect.html#LIBPQ-CONNSTRING\n# for 9.x and newer connection string formats.\n#\n#psql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> connect_timeout=30\"\n\n# MySQL database connection string in the case that you are using MySQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n#\n# Optional connection string parameters for the secure communications (SSL):\n# ca, capath, cert, key, cipher\n# (see http://dev.mysql.com/doc/refman/5.1/en/ssl-options.html for the\n# command options description).\n#\n# Use the string format below (space separated parameters, all optional):\n#\n#mysql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> port=<port> connect_timeout=<seconds> read_timeout=<seconds>\"\n\n# If you want to use an encrypted password in the MySQL connection string,\n# then set the MySQL password encryption secret key file with this option.\n#\n# Warning: If this option is set, then the mysql password must be set in \"mysql-userdb\" in an encrypted format!\n# If you want to use a cleartext password then do not set this option!\n#\n# This is the file path for the aes encrypted secret key used for password encryption.\n#\n#secret-key-file=/path/\n\n# MongoDB database connection string in the case that you are using MongoDB\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format described at http://hergert.me/docs/mongo-c-driver/mongoc_uri.html\n#\n#mongo-userdb=\"mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\"\n\n# Redis database connection string in the case that you are using Redis\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-userdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Redis status and statistics database connection string, if used (default - empty, no Redis stats DB used).\n# This database keeps allocations status information, and it can be also used for publishing\n# and delivering traffic and allocation event notifications.\n# The connection string has the same parameters as redis-userdb connection string.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-statsdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# The default realm to be used for the users when no explicit\n# origin/realm relationship is found in the database, or if the TURN\n# server is not using any database (just the commands-line settings\n# and the userdb file). Must be used with long-term credentials\n# mechanism or with TURN REST API.\n#\n# Note: If the default realm is not specified, then realm falls back to the host domain name.\n#       If the domain name string is empty, or set to '(None)', then it is initialized as an empty string.\n#\n#realm=mycompany.org\n\n# This flag sets the origin consistency\n# check. Across the session, all requests must have the same\n# main ORIGIN attribute value (if the ORIGIN was\n# initially used by the session).\n#\n#check-origin-consistency\n\n# Per-user allocation quota.\n# default value is 0 (no quota, unlimited number of sessions per user).\n# This option can also be set through the database, for a particular realm.\n#\n#user-quota=0\n\n# Total allocation quota.\n# default value is 0 (no quota).\n# This option can also be set through the database, for a particular realm.\n#\n#total-quota=0\n\n# Max bytes-per-second bandwidth a TURN session is allowed to handle\n# (input and output network streams are treated separately). Anything above\n# that limit will be dropped or temporarily suppressed (within\n# the available buffer limits).\n# This option can also be set through the database, for a particular realm.\n#\n#max-bps=0\n\n#\n# Maximum server capacity.\n# Total bytes-per-second bandwidth the TURN server is allowed to allocate\n# for the sessions, combined (input and output network streams are treated separately).\n#\n#bps-capacity=0\n\n# Uncomment if no UDP client listener is desired.\n# By default UDP client listener is always started.\n#\n#no-udp\n\n# Uncomment if no TCP client listener is desired.\n# By default TCP client listener is always started.\n#\n#no-tcp\n\n# Uncomment if no TLS client listener is desired.\n# By default TLS client listener is always started.\n#\n#no-tls\n\n# Uncomment if no DTLS client listener is desired.\n# By default DTLS client listener is always started.\n#\n#no-dtls\n\n# Uncomment if no UDP relay endpoints are allowed.\n# By default UDP relay endpoints are enabled (like in RFC 5766).\n#\n#no-udp-relay\n\n# Uncomment if no TCP relay endpoints are allowed.\n# By default TCP relay endpoints are enabled (like in RFC 6062).\n#\n#no-tcp-relay\n\n# Uncomment if extra security is desired,\n# with nonce value having a limited lifetime.\n# The nonce value is unique for a session.\n# Set this option to limit the nonce lifetime.\n# Set it to 0 for unlimited lifetime.\n# It defaults to 600 secs (10 min) if no value is provided. After that delay,\n# the client will get 438 error and will have to re-authenticate itself.\n#\n#stale-nonce=600\n\n# Uncomment if you want to set the maximum allocation\n# time before it has to be refreshed.\n# Default is 3600s.\n#\n#max-allocate-lifetime=3600\n\n\n# Uncomment to set the lifetime for the channel.\n# Default value is 600 secs (10 minutes).\n# This value MUST not be changed for production purposes.\n#\n#channel-lifetime=600\n\n# Uncomment to set the permission lifetime.\n# Default to 300 secs (5 minutes).\n# In production this value MUST not be changed,\n# however it can be useful for test purposes.\n#\n#permission-lifetime=300\n\n# Certificate file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#cert=/usr/local/etc/turn_server_cert.pem\n\n# Private key file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#pkey=/usr/local/etc/turn_server_pkey.pem\n\n# Private key file password, if it is in encoded format.\n# This option has no default value.\n#\n#pkey-pwd=...\n\n# Allowed OpenSSL cipher list for TLS/DTLS connections.\n# Default value is \"DEFAULT\".\n#\n#cipher-list=\"DEFAULT\"\n\n# CA file in OpenSSL format.\n# Forces TURN server to verify the client SSL certificates.\n# By default this is not set: there is no default value and the client\n# certificate is not checked.\n#\n# Example:\n#CA-file=/etc/ssh/id_rsa.cert\n\n# Curve name for EC ciphers, if supported by OpenSSL\n# library (TLS and DTLS). The default value is prime256v1,\n# if pre-OpenSSL 1.0.2 is used. With OpenSSL 1.0.2+,\n# an optimal curve will be automatically calculated, if not defined\n# by this option.\n#\n#ec-curve-name=prime256v1\n\n# Use 566 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh566\n\n# Use 1066 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh1066\n\n# Use custom DH TLS key, stored in PEM format in the file.\n# Flags --dh566 and --dh1066 are ignored when the DH key is taken from a file.\n#\n#dh-file=<DH-PEM-file-name>\n\n# Flag to prevent stdout log messages.\n# By default, all log messages go to both stdout and to\n# the configured log file. With this option everything will\n# go to the configured log only (unless the log file itself is stdout).\n#\n#no-stdout-log\n\n# Option to set the log file name.\n# By default, the turnserver tries to open a log file in\n# /var/log, /var/tmp, /tmp and the current directory\n# (Whichever file open operation succeeds first will be used).\n# With this option you can set the definite log file name.\n# The special names are \"stdout\" and \"-\" - they will force everything\n# to the stdout. Also, the \"syslog\" name will force everything to\n# the system log (syslog).\n# In the runtime, the logfile can be reset with the SIGHUP signal\n# to the turnserver process.\n#\n#log-file=/var/tmp/turn.log\n\n# Option to redirect all log output into system log (syslog).\n#\n#syslog\n\n# Set syslog facility for syslog messages\n# Default values is ''.\n#\n#syslog-facility=\"LOG_LOCAL1\"\n\n# This flag means that no log file rollover will be used, and the log file\n# name will be constructed as-is, without PID and date appendage.\n# This option can be used, for example, together with the logrotate tool.\n#\n#simple-log\n\n# Enable full ISO-8601 timestamp in all logs.\n#new-log-timestamp\n\n# Set timestamp format (in strftime(1) format). Depends on new-log-timestamp to be enabled.\n#new-log-timestamp-format \"%FT%T%z\"\n\n# Disabled by default binding logging in verbose log mode to avoid DoS attacks.\n# Enable binding logging and UDP endpoint logs in verbose log mode.\n#log-binding\n\n# Option to set the \"redirection\" mode. The value of this option\n# will be the address of the alternate server for UDP & TCP service in the form of\n# <ip>[:<port>]. The server will send this value in the attribute\n# ALTERNATE-SERVER, with error 300, on ALLOCATE request, to the client.\n# Client will receive only values with the same address family\n# as the client network endpoint address family.\n# See RFC 5389 and RFC 5766 for the description of ALTERNATE-SERVER functionality.\n# The client must use the obtained value for subsequent TURN communications.\n# If more than one --alternate-server option is provided, then the functionality\n# can be more accurately described as \"load-balancing\" than a mere \"redirection\".\n# If the port number is omitted, then the default port\n# number 3478 for the UDP/TCP protocols will be used.\n# Colon (:) characters in IPv6 addresses may conflict with the syntax of\n# the option. To alleviate this conflict, literal IPv6 addresses are enclosed\n# in square brackets in such resource identifiers, for example:\n# [2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478 .\n# Multiple alternate servers can be set. They will be used in the\n# round-robin manner. All servers in the pool are considered of equal weight and\n# the load will be distributed equally. For example, if you have 4 alternate servers,\n# then each server will receive 25% of ALLOCATE requests. A alternate TURN server\n# address can be used more than one time with the alternate-server option, so this\n# can emulate \"weighting\" of the servers.\n#\n# Examples:\n#alternate-server=1.2.3.4:5678\n#alternate-server=11.22.33.44:56789\n#alternate-server=5.6.7.8\n#alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to set alternative server for TLS & DTLS services in form of\n# <ip>:<port>. If the port number is omitted, then the default port\n# number 5349 for the TLS/DTLS protocols will be used. See the previous\n# option for the functionality description.\n#\n# Examples:\n#tls-alternate-server=1.2.3.4:5678\n#tls-alternate-server=11.22.33.44:56789\n#tls-alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to suppress TURN functionality, only STUN requests will be processed.\n# Run as STUN server only, all TURN requests will be ignored.\n# By default, this option is NOT set.\n#\n#stun-only\n\n# Option to hide software version. Enhance security when used in production.\n# Revealing the specific software version of the agent through the\n# SOFTWARE attribute might allow them to become more vulnerable to\n# attacks against software that is known to contain security holes.\n# Implementers SHOULD make usage of the SOFTWARE attribute a\n# configurable option (https://tools.ietf.org/html/rfc5389#section-16.1.2)\n#\n#no-software-attribute\n\n# Option to suppress STUN functionality, only TURN requests will be processed.\n# Run as TURN server only, all STUN requests will be ignored.\n# By default, this option is NOT set.\n#\n#no-stun\n\n# This is the timestamp/username separator symbol (character) in TURN REST API.\n# The default value is ':'.\n#\n#rest-api-separator=:\n\n# Flag that can be used to allow peers on the loopback addresses (127.x.x.x and ::1).\n# This is an extra security measure.\n#\n# (To avoid any security issue that allowing loopback access may raise,\n# the no-loopback-peers option is replaced by allow-loopback-peers.)\n#\n# Allow it only for testing in a development environment!\n# In production it adds a possible security vulnerability, so for security reasons\n# it is not allowed using it together with empty cli-password.\n#\n#allow-loopback-peers\n\n# Flag that can be used to disallow peers on well-known broadcast addresses (224.0.0.0 and above, and FFXX:*).\n# This is an extra security measure.\n#\n#no-multicast-peers\n\n# Option to set the max time, in seconds, allowed for full allocation establishment.\n# Default is 60 seconds.\n#\n#max-allocate-timeout=60\n\n# Option to allow or ban specific ip addresses or ranges of ip addresses.\n# If an ip address is specified as both allowed and denied, then the ip address is\n# considered to be allowed. This is useful when you wish to ban a range of ip\n# addresses, except for a few specific ips within that range.\n#\n# This can be used when you do not want users of the turn server to be able to access\n# machines reachable by the turn server, but would otherwise be unreachable from the\n# internet (e.g. when the turn server is sitting behind a NAT)\n#\n# Examples:\n# denied-peer-ip=83.166.64.0-83.166.95.255\n# allowed-peer-ip=83.166.68.45\n\n# File name to store the pid of the process.\n# Default is /var/run/turnserver.pid (if superuser account is used) or\n# /var/tmp/turnserver.pid .\n#\n#pidfile=\"/var/run/turnserver.pid\"\n\n# Require authentication of the STUN Binding request.\n# By default, the clients are allowed anonymous access to the STUN Binding functionality.\n#\n#secure-stun\n\n# Mobility with ICE (MICE) specs support.\n#\n#mobility\n\n# Allocate Address Family according (DEPRECATED and will be removed in favor of allocation-default-address-family)\n# If enabled then TURN server allocates address family according  the TURN\n# Client <=> Server communication address family.\n# (By default Coturn works according RFC 6156.)\n# !!Warning: Enabling this option breaks RFC6156 section-4.2 (violates use default IPv4)!!\n#\n#keep-address-family\n\n# TURN server allocates address family according TURN client requested address family.\n# If address family not requested explicitly by the client, then it falls back to this default.\n# The standard RFC explicitly define that this default must be IPv4, \n# so use other option values with care! \n# Possible values: \"ipv4\" or \"ipv6\" or \"keep\" \n# \"keep\" sets the allocation default address family according to \n# the TURN client allocation request connection address family.\n#\n#allocation-default-address-family=\"ipv4\"\n#allocation-default-address-family=\"ipv4\"\n\n# User name to run the process. After the initialization, the turnserver process\n# will attempt to change the current user ID to that user.\n#\n#proc-user=<user-name>\n\n# Group name to run the process. After the initialization, the turnserver process\n# will attempt to change the current group ID to that group.\n#\n#proc-group=<group-name>\n\n# Turn OFF the CLI support.\n# By default it is always ON.\n# See also options cli-ip and cli-port.\n#\n#no-cli\n\n#Local system IP address to be used for CLI server endpoint. Default value\n# is 127.0.0.1.\n#\n#cli-ip=127.0.0.1\n\n# CLI server port. Default is 5766.\n#\n#cli-port=5766\n\n# CLI access password. Default is empty (no password).\n# For the security reasons, it is recommended that you use the encrypted\n# form of the password (see the -P command in the turnadmin utility).\n#\n# Secure form for password 'qwerty':\n#\n#cli-password=$5$79a316b350311570$81df9cfb9af7f5e5a76eada31e7097b663a0670f99a3c07ded3f1c8e59c5658a\n#\n# Or unsecure form for the same password:\n#\n#cli-password=qwerty\n\n# Enable Web-admin support on https. By default it is Disabled.\n# If it is enabled it also enables a http a simple static banner page\n# with a small reminder that the admin page is available only on https.\n# Not supported if no-tls option used\n#\n#web-admin\n\n# Local system IP address to be used for Web-admin server endpoint. Default value is 127.0.0.1.\n#\n#web-admin-ip=127.0.0.1\n\n# Web-admin server port. Default is 8080.\n#\n#web-admin-port=8080\n\n# Web-admin server listen on STUN/TURN worker threads\n# By default it is disabled for security reasons! (Not recommended in any production environment!)\n#\n#web-admin-listen-on-workers\n\n# Redirect ACME, i.e. HTTP GET requests matching '^/.well-known/acme-challenge/(.*)' to '<URL>$1'.\n# Default is '', i.e. no special handling for such requests.\n#\n#acme-redirect=http://redirectserver/.well-known/acme-challenge/\n\n# Server relay. NON-STANDARD AND DANGEROUS OPTION.\n# Only for those applications when you want to run\n# server applications on the relay endpoints.\n# This option eliminates the IP permissions check on\n# the packets incoming to the relay endpoints.\n#\n#server-relay\n\n# Maximum number of output sessions in ps CLI command.\n# This value can be changed on-the-fly in CLI. The default value is 256.\n#\n#cli-max-output-sessions\n\n# Set network engine type for the process (for internal purposes).\n#\n#ne=[1|2|3]\n\n# Do not allow an TLS/DTLS version of protocol\n#\n#no-tlsv1\n#no-tlsv1_1\n#no-tlsv1_2\n\n# Disable RFC5780 (NAT behavior discovery).\n#\n# Originally, if there are more than one listener address from the same\n# address family, then by default the NAT behavior discovery feature enabled.\n# This option disables the original behavior, because the NAT behavior\n# discovery adds extra attributes to response, and this increase the\n# possibility of an amplification attack.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-rfc5780\n\n# Disable handling old STUN Binding requests and disable MAPPED-ADDRESS\n# attribute in binding response (use only the XOR-MAPPED-ADDRESS).\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-stun-backward-compatibility\n\n# Only send RESPONSE-ORIGIN attribute in binding response if RFC5780 is enabled.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nresponse-origin-only-with-rfc5780"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
---
apiVersion: v1
data:
  primary.conf: |-
    dir /data
    # User-supplied primary configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of primary configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
  valkey.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://valkey.io/docs/topics/persistence.html
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-configuration
  namespace: gitea
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PASSWORD_FILE ]] && export VALKEY_PASSWORD="$(< "${VALKEY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h localhost \
        -p $VALKEY_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_primary.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_primary.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_primary.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PRIMARY_PASSWORD_FILE ]] && export VALKEY_PRIMARY_PASSWORD="$(< "${VALKEY_PRIMARY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PRIMARY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PRIMARY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h $VALKEY_PRIMARY_HOST \
        -p $VALKEY_PRIMARY_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PASSWORD_FILE ]] && export VALKEY_PASSWORD="$(< "${VALKEY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h localhost \
        -p $VALKEY_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_primary.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_primary.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_primary.sh: |-
    #!/bin/bash

    [[ -f $VALKEY_PRIMARY_PASSWORD_FILE ]] && export VALKEY_PRIMARY_PASSWORD="$(< "${VALKEY_PRIMARY_PASSWORD_FILE}")"
    [[ -n "$VALKEY_PRIMARY_PASSWORD" ]] && export REDISCLI_AUTH="$VALKEY_PRIMARY_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      valkey-cli \
        -h $VALKEY_PRIMARY_HOST \
        -p $VALKEY_PRIMARY_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-health
  namespace: gitea
---
apiVersion: v1
data:
  start-primary.sh: |
    #!/bin/bash

    [[ -f $VALKEY_PASSWORD_FILE ]] && export VALKEY_PASSWORD="$(< "${VALKEY_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/valkey/mounted-etc/primary.conf ]];then
        cp /opt/bitnami/valkey/mounted-etc/primary.conf /opt/bitnami/valkey/etc/primary.conf
    fi
    if [[ -f /opt/bitnami/valkey/mounted-etc/valkey.conf ]];then
        cp /opt/bitnami/valkey/mounted-etc/valkey.conf /opt/bitnami/valkey/etc/valkey.conf
    fi
    ARGS=("--port" "${VALKEY_PORT}")
    ARGS+=("--requirepass" "${VALKEY_PASSWORD}")
    ARGS+=("--primaryauth" "${VALKEY_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/valkey/etc/valkey.conf")
    ARGS+=("--include" "/opt/bitnami/valkey/etc/primary.conf")
    exec valkey-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-scripts
  namespace: gitea
---
apiVersion: v1
data:
  config.yaml: |-
    customApps:
    - apps:
      - icon: https://brands.home-assistant.io/homeassistant/icon.png
        name: Home Assistant
        url: https://homeassistant.homelab.olav.ninja
      group: Applications
    darkTheme: tron
    defaultEnable: false
    globalBookmarks:
    - bookmarks:
      - name: Hubble
        url: https://hubble.homelab.olav.ninja
      - name: Keycloak
        url: https://keycloak.homelab.olav.ninja
      - name: Netbird
        url: https://netbird.homelab.olav.ninja
      group: Utilities
    - bookmarks:
      - name: OpenWrt
        url: http://192.168.0.1
      - name: Proxmox
        url: https://proxmox.homelab.olav.ninja:8006
      group: Infrastructure
    - bookmarks:
      - name: Cloudflare Dashboard
        url: https://dash.cloudflare.com
      - name: Github Repo
        url: https://github.com/olav-st/homelab
      group: External
    instanceName: null
    lightTheme: gazette
    name: Olav
    namespaceSelector:
      any: true
      matchNames:
      - media
    title: Homelab
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari-settings
  namespace: hajimari
---
apiVersion: v1
data:
  immich-config.yaml: |
    placeholder: foo
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: immich
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-immich-config
  namespace: immich
---
apiVersion: v1
data:
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
  users.acl: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-configuration
  namespace: immich
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-health
  namespace: immich
---
apiVersion: v1
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/users.acl ]];then
        cp /opt/bitnami/redis/mounted-etc/users.acl /opt/bitnami/redis/etc/users.acl
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-scripts
  namespace: immich
---
apiVersion: v1
data:
  JAVA_OPTS_APPEND: -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
  KC_BOOTSTRAP_ADMIN_USERNAME: admin
  KC_CACHE_CONFIG_FILE: cache-ispn.xml
  KC_CACHE_STACK: kubernetes
  KC_CACHE_TYPE: ispn
  KEYCLOAK_DATABASE_HOST: keycloak-postgresql
  KEYCLOAK_DATABASE_NAME: bitnami_keycloak
  KEYCLOAK_DATABASE_PORT: "5432"
  KEYCLOAK_DATABASE_USER: bn_keycloak
  KEYCLOAK_ENABLE_HTTPS: "false"
  KEYCLOAK_ENABLE_STATISTICS: "false"
  KEYCLOAK_HOSTNAME: https://keycloak.homelab.olav.ninja/
  KEYCLOAK_HOSTNAME_STRICT: "false"
  KEYCLOAK_HTTP_PORT: "8080"
  KEYCLOAK_LOG_LEVEL: INFO
  KEYCLOAK_LOG_OUTPUT: default
  KEYCLOAK_PRODUCTION: "true"
  KEYCLOAK_PROXY_HEADERS: xforwarded
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak-env-vars
  namespace: keycloak
---
apiVersion: v1
data:
  agent-not-ready-taint-key: node.cilium.io/agent-not-ready
  arping-refresh-period: 30s
  auto-direct-node-routes: "false"
  bpf-distributed-lru: "false"
  bpf-events-drop-enabled: "true"
  bpf-events-policy-verdict-enabled: "true"
  bpf-events-trace-enabled: "true"
  bpf-lb-acceleration: disabled
  bpf-lb-algorithm-annotation: "false"
  bpf-lb-external-clusterip: "false"
  bpf-lb-map-max: "65536"
  bpf-lb-mode-annotation: "false"
  bpf-lb-sock: "false"
  bpf-lb-source-range-all-types: "false"
  bpf-map-dynamic-size-ratio: "0.0025"
  bpf-policy-map-max: "16384"
  bpf-root: /sys/fs/bpf
  cgroup-root: /sys/fs/cgroup
  cilium-endpoint-gc-interval: 5m0s
  cluster-id: "0"
  cluster-name: default
  clustermesh-enable-endpoint-sync: "false"
  clustermesh-enable-mcs-api: "false"
  cni-exclusive: "true"
  cni-log-file: /var/run/cilium/cilium-cni.log
  custom-cni-conf: "false"
  datapath-mode: veth
  debug: "false"
  debug-verbose: ""
  default-lb-service-ipam: lbipam
  direct-routing-skip-unreachable: "false"
  dnsproxy-enable-transparent-mode: "true"
  dnsproxy-socket-linger-timeout: "10"
  egress-gateway-reconciliation-trigger-interval: 1s
  enable-auto-protect-node-port-range: "true"
  enable-bpf-clock-probe: "false"
  enable-endpoint-health-checking: "true"
  enable-endpoint-lockdown-on-policy-overflow: "false"
  enable-experimental-lb: "false"
  enable-health-check-loadbalancer-ip: "false"
  enable-health-check-nodeport: "true"
  enable-health-checking: "true"
  enable-hubble: "true"
  enable-internal-traffic-policy: "true"
  enable-ipv4: "true"
  enable-ipv4-big-tcp: "false"
  enable-ipv4-masquerade: "true"
  enable-ipv6: "false"
  enable-ipv6-big-tcp: "false"
  enable-ipv6-masquerade: "true"
  enable-k8s-networkpolicy: "true"
  enable-k8s-terminating-endpoint: "true"
  enable-l2-announcements: "true"
  enable-l2-neigh-discovery: "true"
  enable-l7-proxy: "true"
  enable-lb-ipam: "true"
  enable-local-redirect-policy: "false"
  enable-masquerade-to-route-source: "false"
  enable-metrics: "true"
  enable-node-selector-labels: "false"
  enable-non-default-deny-policies: "true"
  enable-policy: default
  enable-policy-secrets-sync: "true"
  enable-runtime-device-detection: "true"
  enable-sctp: "false"
  enable-source-ip-verification: "true"
  enable-svc-source-range-check: "true"
  enable-tcx: "true"
  enable-vtep: "false"
  enable-well-known-identities: "false"
  enable-xt-socket-fallback: "true"
  envoy-access-log-buffer-size: "4096"
  envoy-base-id: "0"
  envoy-keep-cap-netbindservice: "false"
  external-envoy-proxy: "true"
  health-check-icmp-failure-threshold: "3"
  http-retry-count: "3"
  hubble-disable-tls: "false"
  hubble-export-file-max-backups: "5"
  hubble-export-file-max-size-mb: "10"
  hubble-listen-address: :4244
  hubble-socket-path: /var/run/cilium/hubble.sock
  hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt
  hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt
  hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key
  identity-allocation-mode: crd
  identity-gc-interval: 15m0s
  identity-heartbeat-timeout: 30m0s
  install-no-conntrack-iptables-rules: "false"
  ipam: kubernetes
  ipam-cilium-node-update-rate: 15s
  iptables-random-fully: "false"
  k8s-client-burst: "100"
  k8s-client-qps: "50"
  k8s-require-ipv4-pod-cidr: "false"
  k8s-require-ipv6-pod-cidr: "false"
  kube-proxy-replacement: "true"
  kube-proxy-replacement-healthz-bind-address: ""
  max-connected-clusters: "255"
  mesh-auth-enabled: "true"
  mesh-auth-gc-interval: 5m0s
  mesh-auth-queue-size: "1024"
  mesh-auth-rotated-identities-queue-size: "1024"
  monitor-aggregation: medium
  monitor-aggregation-flags: all
  monitor-aggregation-interval: 5s
  nat-map-stats-entries: "32"
  nat-map-stats-interval: 30s
  node-port-bind-protection: "true"
  nodeport-addresses: ""
  nodes-gc-interval: 5m0s
  operator-api-serve-addr: 127.0.0.1:9234
  operator-prometheus-serve-addr: :9963
  policy-cidr-match-mode: ""
  policy-secrets-namespace: cilium-secrets
  policy-secrets-only-from-secrets-namespace: "true"
  preallocate-bpf-maps: "false"
  procfs: /host/proc
  proxy-connect-timeout: "2"
  proxy-idle-timeout-seconds: "60"
  proxy-initial-fetch-timeout: "30"
  proxy-max-concurrent-retries: "128"
  proxy-max-connection-duration-seconds: "0"
  proxy-max-requests-per-connection: "0"
  proxy-xff-num-trusted-hops-egress: "0"
  proxy-xff-num-trusted-hops-ingress: "0"
  remove-cilium-node-taints: "true"
  routing-mode: tunnel
  service-no-backend-response: reject
  set-cilium-is-up-condition: "true"
  set-cilium-node-taints: "true"
  synchronize-k8s-nodes: "true"
  tofqdns-dns-reject-response-code: refused
  tofqdns-enable-dns-compression: "true"
  tofqdns-endpoint-max-ip-per-hostname: "1000"
  tofqdns-idle-connection-grace-period: 0s
  tofqdns-max-deferred-connection-deletes: "10000"
  tofqdns-proxy-response-max-delay: 100ms
  tunnel-protocol: vxlan
  tunnel-source-port-range: 0-0
  unmanaged-pod-watcher-interval: "15"
  vtep-cidr: ""
  vtep-endpoint: ""
  vtep-mac: ""
  vtep-mask: ""
  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
---
apiVersion: v1
data:
  bootstrap-config.json: |
    {"admin":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/admin.sock"}}},"applicationLogConfig":{"logFormat":{"textFormat":"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v"}},"bootstrapExtensions":[{"name":"envoy.bootstrap.internal_listener","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener"}}],"dynamicResources":{"cdsConfig":{"apiConfigSource":{"apiType":"GRPC","grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}],"setNodeOnFirstMessageOnly":true,"transportApiVersion":"V3"},"initialFetchTimeout":"30s","resourceApiVersion":"V3"},"ldsConfig":{"apiConfigSource":{"apiType":"GRPC","grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}],"setNodeOnFirstMessageOnly":true,"transportApiVersion":"V3"},"initialFetchTimeout":"30s","resourceApiVersion":"V3"}},"node":{"cluster":"ingress-cluster","id":"host~127.0.0.1~no-id~localdomain"},"overloadManager":{"resourceMonitors":[{"name":"envoy.resource_monitors.global_downstream_max_connections","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig","max_active_downstream_connections":"50000"}}]},"staticResources":{"clusters":[{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"ingress-cluster","type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"egress-cluster-tls","transportSocket":{"name":"cilium.tls_wrapper","typedConfig":{"@type":"type.googleapis.com/cilium.UpstreamTlsWrapperContext"}},"type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"upstreamHttpProtocolOptions":{},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"egress-cluster","type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"useDownstreamProtocolConfig":{}}}},{"circuitBreakers":{"thresholds":[{"maxRetries":128}]},"cleanupInterval":"2.500s","connectTimeout":"2s","lbPolicy":"CLUSTER_PROVIDED","name":"ingress-cluster-tls","transportSocket":{"name":"cilium.tls_wrapper","typedConfig":{"@type":"type.googleapis.com/cilium.UpstreamTlsWrapperContext"}},"type":"ORIGINAL_DST","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","commonHttpProtocolOptions":{"idleTimeout":"60s","maxConnectionDuration":"0s","maxRequestsPerConnection":0},"upstreamHttpProtocolOptions":{},"useDownstreamProtocolConfig":{}}}},{"connectTimeout":"2s","loadAssignment":{"clusterName":"xds-grpc-cilium","endpoints":[{"lbEndpoints":[{"endpoint":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/xds.sock"}}}}]}]},"name":"xds-grpc-cilium","type":"STATIC","typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions","explicitHttpConfig":{"http2ProtocolOptions":{}}}}},{"connectTimeout":"2s","loadAssignment":{"clusterName":"/envoy-admin","endpoints":[{"lbEndpoints":[{"endpoint":{"address":{"pipe":{"path":"/var/run/cilium/envoy/sockets/admin.sock"}}}}]}]},"name":"/envoy-admin","type":"STATIC"}],"listeners":[{"address":{"socketAddress":{"address":"0.0.0.0","portValue":9964}},"filterChains":[{"filters":[{"name":"envoy.filters.network.http_connection_manager","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager","httpFilters":[{"name":"envoy.filters.http.router","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}],"internalAddressConfig":{"cidrRanges":[{"addressPrefix":"10.0.0.0","prefixLen":8},{"addressPrefix":"172.16.0.0","prefixLen":12},{"addressPrefix":"192.168.0.0","prefixLen":16},{"addressPrefix":"127.0.0.1","prefixLen":32}]},"routeConfig":{"virtualHosts":[{"domains":["*"],"name":"prometheus_metrics_route","routes":[{"match":{"prefix":"/metrics"},"name":"prometheus_metrics_route","route":{"cluster":"/envoy-admin","prefixRewrite":"/stats/prometheus"}}]}]},"statPrefix":"envoy-prometheus-metrics-listener","streamIdleTimeout":"0s"}}]}],"name":"envoy-prometheus-metrics-listener"},{"address":{"socketAddress":{"address":"127.0.0.1","portValue":9878}},"filterChains":[{"filters":[{"name":"envoy.filters.network.http_connection_manager","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager","httpFilters":[{"name":"envoy.filters.http.router","typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}],"internalAddressConfig":{"cidrRanges":[{"addressPrefix":"10.0.0.0","prefixLen":8},{"addressPrefix":"172.16.0.0","prefixLen":12},{"addressPrefix":"192.168.0.0","prefixLen":16},{"addressPrefix":"127.0.0.1","prefixLen":32}]},"routeConfig":{"virtual_hosts":[{"domains":["*"],"name":"health","routes":[{"match":{"prefix":"/healthz"},"name":"health","route":{"cluster":"/envoy-admin","prefixRewrite":"/ready"}}]}]},"statPrefix":"envoy-health-listener","streamIdleTimeout":"0s"}}]}],"name":"envoy-health-listener"}]}}
kind: ConfigMap
metadata:
  name: cilium-envoy-config
  namespace: kube-system
---
apiVersion: v1
data:
  config.yaml: "cluster-name: default\npeer-service: \"hubble-peer.kube-system.svc.cluster.local.:443\"\nlisten-address: :4245\ngops: true\ngops-port: \"9893\"\nretry-timeout: \nsort-buffer-len-max: \nsort-buffer-drain-timeout: \ntls-hubble-client-cert-file: /var/lib/hubble-relay/tls/client.crt\ntls-hubble-client-key-file: /var/lib/hubble-relay/tls/client.key\ntls-hubble-server-ca-files: /var/lib/hubble-relay/tls/hubble-server-ca.crt\n\ndisable-server-tls: true\n"
kind: ConfigMap
metadata:
  name: hubble-relay-config
  namespace: kube-system
---
apiVersion: v1
data:
  nginx.conf: "server {\n    listen       8081;\n    listen       [::]:8081;\n    server_name  localhost;\n    root /app;\n    index index.html;\n    client_max_body_size 1G;\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n\n        location /api {\n            proxy_http_version 1.1;\n            proxy_pass_request_headers on;\n            proxy_pass http://127.0.0.1:8090;\n        }\n        location / {\n            # double `/index.html` is required here \n            try_files $uri $uri/ /index.html /index.html;\n        }\n\n        # Liveness probe\n        location /healthz {\n            access_log off;\n            add_header Content-Type text/plain;\n            return 200 'ok';\n        }\n    }\n}"
kind: ConfigMap
metadata:
  name: hubble-ui-nginx
  namespace: kube-system
---
apiVersion: v1
data:
  management.tmpl.json: |-
    {
        "Stuns": [
            {
                "Proto": "udp",
                "URI": "${NETBIRD_STUN_URI}",
                "Username": "",
                "Password": null
            }
        ],
        "TURNConfig": {
            "Turns": [
                {
                    "Proto": "udp",
                    "URI": "${NETBIRD_TURN_URI}",
                    "Username": "${NETBIRD_TURN_USER}",
                    "Password": "${NETBIRD_TURN_PASSWORD}"
                }
            ],
            "CredentialsTTL": "12h",
            "Secret": "secret",
            "TimeBasedCredentials": false
        },
        "Signal": {
            "Proto": "${NETBIRD_SIGNAL_PROTOCOL}",
            "URI": "${NETBIRD_SIGNAL_URI}",
            "Username": "",
            "Password": null
        },
        "Datadir": "",
        "HttpConfig": {
            "Address": "0.0.0.0:80",
            "AuthAudience": "${NETBIRD_AUTH_AUDIENCE}",
            "AuthUserIDClaim": "${NETBIRD_AUTH_USER_ID_CLAIM:-sub}",
            "CertFile": "${NETBIRD_MGMT_API_CERT_FILE}",
            "CertKey": "${NETBIRD_MGMT_API_CERT_KEY_FILE}",
            "IdpSignKeyRefreshEnabled": true,
            "OIDCConfigEndpoint": "${NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT}"
        },
        "IdpManagerConfig": {
            "ManagerType": "${NETBIRD_IDP_MANAGER_TYPE}",
            "${NETBIRD_IDP_MANAGER_TYPE^}ClientCredentials": {
                "ClientID": "${NETBIRD_IDP_CLIENT_ID}",
                "ClientSecret": "${NETBIRD_IDP_CLIENT_SECRET}",
                "GrantType": "${NETBIRD_IDP_GRANT_TYPE}",
                "Audience": "${NETBIRD_IDP_AUTH0_AUDIENCE}",
                "AuthIssuer": "${NETBIRD_IDP_AUTH0_AUTH_ISSUER}",
                "AdminEndpoint": "${NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT}",
                "TokenEndpoint": "${NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT}"
            }
        },
        "DeviceAuthorizationFlow": {
            "Provider": "${NETBIRD_AUTH_DEVICE_AUTH_PROVIDER}",
            "ProviderConfig": {
                "Audience": "${NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE}",
                "ClientID": "${NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID}",
                "DeviceAuthEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT}",
                "Domain": "${NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY}",
                "TokenEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT}",
                "Scope": "${NETBIRD_AUTH_DEVICE_AUTH_SCOPE}",
                "UseIDToken": ${NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN:-false}
            }
        },
        "Relay": {
            "Addresses": ["${NB_EXPOSED_ADDRESS}"],
            "CredentialsTTL": "24h",
            "Secret": "${NB_AUTH_SECRET}"
        }
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
data:
  .htaccess: |-
    # line below if for Apache 2.4
    <ifModule mod_authz_core.c>
    Require all denied
    </ifModule>
    # line below if for Apache 2.2
    <ifModule !mod_authz_core.c>
    deny from all
    </ifModule>
    # section for Apache 2.2 and 2.4
    <ifModule mod_autoindex.c>
    IndexIgnore *
    </ifModule>
  apache-pretty-urls.config.php: |-
    <?php
    $CONFIG = array (
      'htaccess.RewriteBase' => '/',
    );
  apcu.config.php: |-
    <?php
    $CONFIG = array (
      'memcache.local' => '\OC\Memcache\APCu',
    );
  apps.config.php: |-
    <?php
    $CONFIG = array (
      'apps_paths' => array (
          0 => array (
                  'path'     => OC::$SERVERROOT.'/apps',
                  'url'      => '/apps',
                  'writable' => false,
          ),
          1 => array (
                  'path'     => OC::$SERVERROOT.'/custom_apps',
                  'url'      => '/custom_apps',
                  'writable' => true,
          ),
      ),
    );
  autoconfig.php: |-
    <?php
    $autoconfig_enabled = false;
    if (getenv('SQLITE_DATABASE')) {
        $AUTOCONFIG["dbtype"] = "sqlite";
        $AUTOCONFIG["dbname"] = getenv('SQLITE_DATABASE');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE_FILE') && getenv('MYSQL_USER_FILE') && getenv('MYSQL_PASSWORD_FILE') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG['dbtype'] = 'mysql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('MYSQL_DATABASE_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('MYSQL_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('MYSQL_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE') && getenv('MYSQL_USER') && getenv('MYSQL_PASSWORD') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG["dbtype"] = "mysql";
        $AUTOCONFIG["dbname"] = getenv('MYSQL_DATABASE');
        $AUTOCONFIG["dbuser"] = getenv('MYSQL_USER');
        $AUTOCONFIG["dbpass"] = getenv('MYSQL_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB_FILE') && getenv('POSTGRES_USER_FILE') && getenv('POSTGRES_PASSWORD_FILE') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG['dbtype'] = 'pgsql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('POSTGRES_DB_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('POSTGRES_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('POSTGRES_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB') && getenv('POSTGRES_USER') && getenv('POSTGRES_PASSWORD') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG["dbtype"] = "pgsql";
        $AUTOCONFIG["dbname"] = getenv('POSTGRES_DB');
        $AUTOCONFIG["dbuser"] = getenv('POSTGRES_USER');
        $AUTOCONFIG["dbpass"] = getenv('POSTGRES_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    }
    if ($autoconfig_enabled) {
        $AUTOCONFIG["directory"] = getenv('NEXTCLOUD_DATA_DIR') ?: "/var/www/html/data";
    }
  mycustom.config.php: |-
    <?php
    $CONFIG = array(
      'trusted_proxies' => array('10.0.0.0/8'),
      'default_phone_region' => 'NO',
      'maintenance_window_start' => 1,
      );
  redis.config.php: |-
    <?php
    if (getenv('REDIS_HOST')) {
      $CONFIG = array(
        'memcache.distributed' => '\OC\Memcache\Redis',
        'memcache.locking' => '\OC\Memcache\Redis',
        'redis' => array(
          'host' => getenv('REDIS_HOST'),
          'password' => getenv('REDIS_HOST_PASSWORD_FILE') ? trim(file_get_contents(getenv('REDIS_HOST_PASSWORD_FILE'))) : (string) getenv('REDIS_HOST_PASSWORD'),
        ),
      );

      if (getenv('REDIS_HOST_PORT') !== false) {
        $CONFIG['redis']['port'] = (int) getenv('REDIS_HOST_PORT');
      } elseif (getenv('REDIS_HOST')[0] != '/') {
        $CONFIG['redis']['port'] = 6379;
      }
    }
  reverse-proxy.config.php: |-
    <?php
    $overwriteHost = getenv('OVERWRITEHOST');
    if ($overwriteHost) {
      $CONFIG['overwritehost'] = $overwriteHost;
    }

    $overwriteProtocol = getenv('OVERWRITEPROTOCOL');
    if ($overwriteProtocol) {
      $CONFIG['overwriteprotocol'] = $overwriteProtocol;
    }

    $overwriteCliUrl = getenv('OVERWRITECLIURL');
    if ($overwriteCliUrl) {
      $CONFIG['overwrite.cli.url'] = $overwriteCliUrl;
    }

    $overwriteWebRoot = getenv('OVERWRITEWEBROOT');
    if ($overwriteWebRoot) {
      $CONFIG['overwritewebroot'] = $overwriteWebRoot;
    }

    $overwriteCondAddr = getenv('OVERWRITECONDADDR');
    if ($overwriteCondAddr) {
      $CONFIG['overwritecondaddr'] = $overwriteCondAddr;
    }

    $trustedProxies = getenv('TRUSTED_PROXIES');
    if ($trustedProxies) {
      $CONFIG['trusted_proxies'] = array_filter(array_map('trim', explode(' ', $trustedProxies)));
    }

    $forwardedForHeaders = getenv('FORWARDED_FOR_HEADERS');
    if ($forwardedForHeaders) {
      $CONFIG['forwarded_for_headers'] = array_filter(array_map('trim', explode(' ', $forwardedForHeaders)));
    }
  s3.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_S3_BUCKET')) {
      $use_ssl = getenv('OBJECTSTORE_S3_SSL');
      $use_path = getenv('OBJECTSTORE_S3_USEPATH_STYLE');
      $use_legacyauth = getenv('OBJECTSTORE_S3_LEGACYAUTH');
      $autocreate = getenv('OBJECTSTORE_S3_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => array(
          'class' => '\OC\Files\ObjectStore\S3',
          'arguments' => array(
            'bucket' => getenv('OBJECTSTORE_S3_BUCKET'),
            'region' => getenv('OBJECTSTORE_S3_REGION') ?: '',
            'hostname' => getenv('OBJECTSTORE_S3_HOST') ?: '',
            'port' => getenv('OBJECTSTORE_S3_PORT') ?: '',
            'storageClass' => getenv('OBJECTSTORE_S3_STORAGE_CLASS') ?: '',
            'objectPrefix' => getenv("OBJECTSTORE_S3_OBJECT_PREFIX") ? getenv("OBJECTSTORE_S3_OBJECT_PREFIX") : "urn:oid:",
            'autocreate' => strtolower($autocreate) !== 'false',
            'use_ssl' => strtolower($use_ssl) !== 'false',
            // required for some non Amazon S3 implementations
            'use_path_style' => $use_path == true && strtolower($use_path) !== 'false',
            // required for older protocol versions
            'legacy_auth' => $use_legacyauth == true && strtolower($use_legacyauth) !== 'false'
          )
        )
      );

      if (getenv('OBJECTSTORE_S3_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_KEY')) {
        $CONFIG['objectstore']['arguments']['key'] = getenv('OBJECTSTORE_S3_KEY');
      } else {
        $CONFIG['objectstore']['arguments']['key'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SECRET_FILE')) {
        $CONFIG['objectstore']['arguments']['secret'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SECRET_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SECRET')) {
        $CONFIG['objectstore']['arguments']['secret'] = getenv('OBJECTSTORE_S3_SECRET');
      } else {
        $CONFIG['objectstore']['arguments']['secret'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SSE_C_KEY')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = getenv('OBJECTSTORE_S3_SSE_C_KEY');
      }
    }
  smtp.config.php: |-
    <?php
    if (getenv('SMTP_HOST') && getenv('MAIL_FROM_ADDRESS') && getenv('MAIL_DOMAIN')) {
      $CONFIG = array (
        'mail_smtpmode' => 'smtp',
        'mail_smtphost' => getenv('SMTP_HOST'),
        'mail_smtpport' => getenv('SMTP_PORT') ?: (getenv('SMTP_SECURE') ? 465 : 25),
        'mail_smtpsecure' => getenv('SMTP_SECURE') ?: '',
        'mail_smtpauth' => getenv('SMTP_NAME') && (getenv('SMTP_PASSWORD') || getenv('SMTP_PASSWORD_FILE')),
        'mail_smtpauthtype' => getenv('SMTP_AUTHTYPE') ?: 'LOGIN',
        'mail_smtpname' => getenv('SMTP_NAME') ?: '',
        'mail_from_address' => getenv('MAIL_FROM_ADDRESS'),
        'mail_domain' => getenv('MAIL_DOMAIN'),
      );

      if (getenv('SMTP_PASSWORD_FILE')) {
          $CONFIG['mail_smtppassword'] = trim(file_get_contents(getenv('SMTP_PASSWORD_FILE')));
      } elseif (getenv('SMTP_PASSWORD')) {
          $CONFIG['mail_smtppassword'] = getenv('SMTP_PASSWORD');
      } else {
          $CONFIG['mail_smtppassword'] = '';
      }
    }
  swift.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_SWIFT_URL')) {
        $autocreate = getenv('OBJECTSTORE_SWIFT_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => [
          'class' => 'OC\\Files\\ObjectStore\\Swift',
          'arguments' => [
            'autocreate' => $autocreate == true && strtolower($autocreate) !== 'false',
            'user' => [
              'name' => getenv('OBJECTSTORE_SWIFT_USER_NAME'),
              'password' => getenv('OBJECTSTORE_SWIFT_USER_PASSWORD'),
              'domain' => [
                'name' => (getenv('OBJECTSTORE_SWIFT_USER_DOMAIN')) ?: 'Default',
              ],
            ],
            'scope' => [
              'project' => [
                'name' => getenv('OBJECTSTORE_SWIFT_PROJECT_NAME'),
                'domain' => [
                  'name' => (getenv('OBJECTSTORE_SWIFT_PROJECT_DOMAIN')) ?: 'Default',
                ],
              ],
            ],
            'serviceName' => (getenv('OBJECTSTORE_SWIFT_SERVICE_NAME')) ?: 'swift',
            'region' => getenv('OBJECTSTORE_SWIFT_REGION'),
            'url' => getenv('OBJECTSTORE_SWIFT_URL'),
            'bucket' => getenv('OBJECTSTORE_SWIFT_CONTAINER_NAME'),
          ]
        ]
      );
    }
  upgrade-disable-web.config.php: |-
    <?php
    $CONFIG = array (
      'upgrade.disable-web' => true,
    );
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud-config
  namespace: nextcloud
---
apiVersion: v1
data:
  my.cnf: |-
    [mysqld]
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mariadb
    datadir=/bitnami/mariadb/data
    plugin_dir=/opt/bitnami/mariadb/plugin
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    tmpdir=/opt/bitnami/mariadb/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
    log-error=/opt/bitnami/mariadb/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    long_query_time=10.0

    [client]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mariadb/plugin

    [manager]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
---
apiVersion: v1
data:
  uploadLimit.ini: |-
    upload_max_filesize = 1G
    post_max_size = 1G
    max_input_time = 5400
    max_execution_time = 5400
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud-phpconfig
  namespace: nextcloud
---
apiVersion: v1
data:
  config.yaml: |
    general:
      appName: Pingvin Share
      appUrl: https://pingvin.homelab.olav.ninja
      secureCookies: "true"
      showHomePage: "false"
      sessionDuration: 1 week

    share:
      allowRegistration: "false"
      allowUnauthenticatedShares: "true"
      maxExpiration: 180 days
      shareIdLength: "8"
      maxSize: "10000000000"
      zipCompressionLevel: "9"
      chunkSize: "10000000"
      autoOpenShareModal: "false"

    cache:
      redis-enabled: "false"

    email:
      enableShareEmailRecipients: "false"

    smtp:
      enabled: "false"

    ldap:
      enabled: "false"

    oauth:
      allowRegistration: "true"
      ignoreTotp: "true"
      disablePassword: "true"
      oidc-enabled: "true"
      oidc-discoveryUri: "https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration"
      oidc-signOut: "true"
      oidc-scope: openid email profile
      oidc-usernameClaim: "preferred_username"
      oidc-clientId: "pingvinshare"
      oidc-clientSecret: "${OIDC_CLIENT_SECRET}"

    s3:
      enabled: "false"

    legal:
      enabled: "false"

    initUser:
      enabled: false
kind: ConfigMap
metadata:
  name: pingvinshare-config
  namespace: pingvinshare
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-root-ca
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-client
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-server
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea
  namespace: gitea
stringData:
  assertions: ""
  config_environment.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    function env2ini::log() {
      printf "${1}\n"
    }

    function env2ini::read_config_to_env() {
      local section="${1}"
      local line="${2}"

      if [[ -z "${line}" ]]; then
        # skip empty line
        return
      fi

      # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line
      local setting="$(awk -F '=' '{print $1}' <<< "${line}" | xargs echo -n)"

      if [[ -z "${setting}" ]]; then
        env2ini::log '  ! invalid setting'
        exit 1
      fi

      local value=''
      local regex="^${setting}(\s*)=(\s*)(.*)"
      if [[ $line =~ $regex ]]; then
        value="${BASH_REMATCH[3]}"
      else
        env2ini::log '  ! invalid setting'
        exit 1
      fi

      env2ini::log "    + '${setting}'"

      if [[ -z "${section}" ]]; then
        export "GITEA____${setting^^}=${value}"                           # '^^' makes the variable content uppercase
        return
      fi

      local masked_section="${section//./_0X2E_}"                            # '//' instructs to replace all matches
      masked_section="${masked_section//-/_0X2D_}"

      export "GITEA__${masked_section^^}__${setting^^}=${value}"        # '^^' makes the variable content uppercase
    }

    function env2ini::reload_preset_envs() {
      env2ini::log "Reloading preset envs..."

      while read -r line; do
        if [[ -z "${line}" ]]; then
          # skip empty line
          return
        fi

        # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line
        local setting="$(awk -F '=' '{print $1}' <<< "${line}" | xargs echo -n)"

        if [[ -z "${setting}" ]]; then
          env2ini::log '  ! invalid setting'
          exit 1
        fi

        local value=''
        local regex="^${setting}(\s*)=(\s*)(.*)"
        if [[ $line =~ $regex ]]; then
          value="${BASH_REMATCH[3]}"
        else
          env2ini::log '  ! invalid setting'
          exit 1
        fi

        env2ini::log "  + '${setting}'"

        export "${setting^^}=${value}"                           # '^^' makes the variable content uppercase
      done < "$TMP_EXISTING_ENVS_FILE"

      rm $TMP_EXISTING_ENVS_FILE
    }


    function env2ini::process_config_file() {
      local config_file="${1}"
      local section="$(basename "${config_file}")"

      if [[ $section == '_generals_' ]]; then
        env2ini::log "  [ini root]"
        section=''
      else
        env2ini::log "  ${section}"
      fi

      while read -r line; do
        env2ini::read_config_to_env "${section}" "${line}"
      done < <(awk 1 "${config_file}")                             # Helm .toYaml trims the trailing new line which breaks line processing; awk 1 ... adds it back while reading
    }

    function env2ini::load_config_sources() {
      local path="${1}"

      if [[ -d "${path}" ]]; then
        env2ini::log "Processing $(basename "${path}")..."

        while read -d '' configFile; do
          env2ini::process_config_file "${configFile}"
        done < <(find "${path}" -type l -not -name '..data' -print0)

        env2ini::log "\n"
      fi
    }

    function env2ini::generate_initial_secrets() {
      # These environment variables will either be
      #   - overwritten with user defined values,
      #   - initially used to set up Gitea
      # Anyway, they won't harm existing app.ini files

      export GITEA__SECURITY__INTERNAL_TOKEN=$(gitea generate secret INTERNAL_TOKEN)
      export GITEA__SECURITY__SECRET_KEY=$(gitea generate secret SECRET_KEY)
      export GITEA__OAUTH2__JWT_SECRET=$(gitea generate secret JWT_SECRET)
      export GITEA__SERVER__LFS_JWT_SECRET=$(gitea generate secret LFS_JWT_SECRET)

      env2ini::log "...Initial secrets generated\n"
    }

    # save existing envs prior to script execution. Necessary to keep order of preexisting and custom envs
    env | (grep -e '^GITEA__' || [[ $? == 1 ]]) > $TMP_EXISTING_ENVS_FILE

    # MUST BE CALLED BEFORE OTHER CONFIGURATION
    env2ini::generate_initial_secrets

    env2ini::load_config_sources "$ENV_TO_INI_MOUNT_POINT/inlines/"
    env2ini::load_config_sources "$ENV_TO_INI_MOUNT_POINT/additionals/"

    # load existing envs to override auto generated envs
    env2ini::reload_preset_envs

    env2ini::log "=== All configuration sources loaded ===\n"

    # safety to prevent rewrite of secret keys if an app.ini already exists
    if [ -f ${GITEA_APP_INI} ]; then
      env2ini::log 'An app.ini file already exists. To prevent overwriting secret keys, these settings are dropped and remain unchanged:'
      env2ini::log '  - security.INTERNAL_TOKEN'
      env2ini::log '  - security.SECRET_KEY'
      env2ini::log '  - oauth2.JWT_SECRET'
      env2ini::log '  - server.LFS_JWT_SECRET'

      unset GITEA__SECURITY__INTERNAL_TOKEN
      unset GITEA__SECURITY__SECRET_KEY
      unset GITEA__OAUTH2__JWT_SECRET
      unset GITEA__SERVER__LFS_JWT_SECRET
    fi

    environment-to-ini -o $GITEA_APP_INI
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-init
  namespace: gitea
stringData:
  configure_gitea.sh: "#!/usr/bin/env bash\n\nset -euo pipefail\n\necho '==== BEGIN GITEA CONFIGURATION ===='\n\n{ # try\n  gitea migrate\n} || { # catch\n  echo \"Gitea migrate might fail due to database connection...This init-container will try again in a few seconds\"\n  exit 1\n}\nfunction test_valkey_connection() {\n  local RETRY=0\n  local MAX=30\n  \n  echo 'Wait for valkey to become avialable...'\n  until [ \"${RETRY}\" -ge \"${MAX}\" ]; do\n    nc -vz -w2 gitea-valkey-headless.gitea.svc.cluster.local 6379 && break\n    RETRY=$[${RETRY}+1]\n    echo \"...not ready yet (${RETRY}/${MAX})\"\n  done\n\n  if [ \"${RETRY}\" -ge \"${MAX}\" ]; then\n    echo \"Valkey not reachable after '${MAX}' attempts!\"\n    exit 1\n  fi\n}\n\ntest_valkey_connection\nfunction configure_admin_user() {\n  local full_admin_list=$(gitea admin user list --admin)\n  local actual_user_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+Username\\s+Email\\s+IsActive.*)\"\n  if [[ \"${full_admin_list}\" =~ $regex ]]; then\n    actual_user_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_admin_user' was not able to determine the current list of admin users.\"\n    echo \"       Please review the output of 'gitea admin user list --admin' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-gitea/issues.\"\n    echo \"DEBUG: Output of 'gitea admin user list --admin'\"\n    echo \"--\"\n    echo \"${full_admin_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local ACCOUNT_ID=$(echo \"${actual_user_table}\" | grep -E \"\\s+${GITEA_ADMIN_USERNAME}\\s+\" | awk -F \" \" \"{printf \\$1}\")\n  if [[ -z \"${ACCOUNT_ID}\" ]]; then\n    local -a create_args\n    create_args=(--admin --username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\" --email \"gitea@local.domain\")\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = initialOnlyRequireReset ]]; then\n      create_args+=(--must-change-password=true)\n    else\n      create_args+=(--must-change-password=false)\n    fi\n    echo \"No admin user '${GITEA_ADMIN_USERNAME}' found. Creating now...\"\n    gitea admin user create \"${create_args[@]}\"\n    echo '...created.'\n  else\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = keepUpdated ]]; then\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist. Running update to sync password...\"\n      # See https://gitea.com/gitea/helm-gitea/issues/673\n      # --must-change-password argument was added to change-password, defaulting to true, counter to the previous behavior\n      #   which acted as if it were provided with =false. If the argument is present in this version of gitea, then we\n      #   should add it to prevent requiring frequent admin password resets.\n      local -a change_args\n      change_args=(--username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\")\n      if gitea admin user change-password --help | grep -qF -- '--must-change-password'; then\n        change_args+=(--must-change-password=false)\n      fi\n      gitea admin user change-password \"${change_args[@]}\"\n      echo '...password sync done.'\n    else\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist, but update mode is set to '${GITEA_ADMIN_PASSWORD_MODE}'. Skipping.\"\n    fi\n  fi\n}\n\nconfigure_admin_user\n\nfunction configure_ldap() {\n    echo 'no ldap configuration... skipping.'\n}\n\nconfigure_ldap\n\nfunction configure_oauth() {\n  local OAUTH_NAME='keycloak'\n  local full_auth_list=$(gitea admin auth list --vertical-bars)\n  local actual_auth_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+\\|Name\\s+\\|Type\\s+\\|Enabled.*)\"\n  if [[ \"${full_auth_list}\" =~ $regex ]]; then\n    actual_auth_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_oauth' was not able to determine the current list of authentication sources.\"\n    echo \"       Please review the output of 'gitea admin auth list --vertical-bars' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-gitea/issues.\"\n    echo \"DEBUG: Output of 'gitea admin auth list --vertical-bars'\"\n    echo \"--\"\n    echo \"${full_auth_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local AUTH_ID=$(echo \"${actual_auth_table}\" | grep -E \"\\|${OAUTH_NAME}\\s+\\|\" | grep -iE '\\|OAuth2\\s+\\|' | awk -F \" \"  \"{print \\$1}\")\n\n  if [[ -z \"${AUTH_ID}\" ]]; then\n    echo \"No oauth configuration found with name '${OAUTH_NAME}'. Installing it now...\"\n    gitea admin auth add-oauth --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...installed.'\n  else\n    echo \"Existing oauth configuration with name '${OAUTH_NAME}': '${AUTH_ID}'. Running update to sync settings...\"\n    gitea admin auth update-oauth --id \"${AUTH_ID}\" --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...sync settings done.'\n  fi\n}\n\nconfigure_oauth\n\necho '==== END GITEA CONFIGURATION ===='"
  configure_gpg_environment.sh: |
    #!/usr/bin/env bash
    set -eu

    gpg --batch --import "$TMP_RAW_GPG_KEY"
  init_directory_structure.sh: |-
    #!/usr/bin/env bash

    set -euo pipefail
    mkdir -pv /data/git/.ssh
    chmod -Rv 700 /data/git/.ssh
    [ ! -d /data/gitea/conf ] && mkdir -pv /data/gitea/conf

    # prepare temp directory structure
    mkdir -pv "${GITEA_TEMP}"
    chmod -v ug+rwx "${GITEA_TEMP}"
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-inline-config
  namespace: gitea
stringData:
  _generals_: ""
  cache: |-
    ADAPTER=redis
    HOST=redis://:changeme@gitea-valkey-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
  database: |-
    DB_TYPE=postgres
    HOST=gitea-postgresql.gitea.svc.cluster.local:5432
    NAME=gitea
    PASSWD=gitea
    USER=gitea
  indexer: ISSUE_INDEXER_TYPE=db
  metrics: ENABLED=false
  queue: |-
    CONN_STR=redis://:changeme@gitea-valkey-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
    TYPE=redis
  repository: ROOT=/data/git/gitea-repositories
  security: INSTALL_LOCK=true
  server: |-
    APP_DATA_PATH=/data
    DOMAIN=gitea.homelab.olav.ninja
    ENABLE_PPROF=false
    HTTP_PORT=3000
    LFS_START_SERVER=true
    OFFLINE_MODE=false
    PROTOCOL=http
    ROOT_URL=https://gitea.homelab.olav.ninja
    SSH_DOMAIN=gitea.homelab.olav.ninja
    SSH_LISTEN_PORT=2222
    SSH_PORT=22
    START_SSH_SERVER=true
  service: DISABLE_REGISTRATION=true
  session: |-
    PROVIDER=redis
    PROVIDER_CONFIG=redis://:changeme@gitea-valkey-headless.gitea.svc.cluster.local:6379/0?pool_size=100&idle_timeout=180s&
type: Opaque
---
apiVersion: v1
data:
  valkey-password: Y2hhbmdlbWU=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey
  namespace: gitea
type: Opaque
---
apiVersion: v1
kind: Endpoints
metadata:
  name: external-cluster
  namespace: traefik
subsets:
  - addresses:
      - ip: 192.168.0.80
    ports:
      - name: ingress-port
        port: 443
        protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager
  namespace: cert-manager
spec:
  ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: http-metrics
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cert-manager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
  selector:
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cainjector
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
  selector:
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: webhook
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-webhook-service
  namespace: cnpg-system
spec:
  ports:
    - name: webhook-server
      port: 443
      targetPort: webhook-server
  selector:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/name: cloudnative-pg
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    io.cilium/lb-ipam-ips: 192.168.0.91
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn-tcp
  namespace: coturn
spec:
  ports:
    - name: tcp
      port: 3478
      protocol: TCP
      targetPort: tcp
    - name: tcp-tls
      port: 5349
      protocol: TCP
      targetPort: tcp-tls
  selector:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/name: coturn
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    io.cilium/lb-ipam-ips: 192.168.0.91
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn-udp
  namespace: coturn
spec:
  ports:
    - name: udp
      port: 3478
      protocol: UDP
      targetPort: udp
    - name: udp-tls
      port: 5349
      protocol: UDP
      targetPort: udp-tls
  selector:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/name: coturn
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    release: crossplane
  name: crossplane-webhooks
  namespace: crossplane
spec:
  ports:
    - port: 9443
      protocol: TCP
      targetPort: 9443
  selector:
    app: crossplane
    release: crossplane
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-http
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: http
      port: 3000
      targetPort: null
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql-hl
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-ssh
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: 2222
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-headless
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: valkey
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-primary
  namespace: gitea
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: valkey
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/service: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/name: hajimari
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/service: immich-machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-machine-learning
  namespace: immich
spec:
  ports:
    - name: http
      port: 3003
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: machine-learning
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-headless
  namespace: immich
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/service: immich-server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-server
  namespace: immich
spec:
  ports:
    - name: http
      port: 2283
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: server
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  ports:
    - name: http
      nodePort: null
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak-headless
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql-hl
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "9964"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    io.cilium/app: proxy
    k8s-app: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: envoy-metrics
      port: 9964
      protocol: TCP
      targetPort: envoy-metrics
  selector:
    k8s-app: cilium-envoy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-peer
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: hubble-peer
  namespace: kube-system
spec:
  internalTrafficPolicy: Local
  ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
  selector:
    k8s-app: cilium
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-relay
  name: hubble-relay
  namespace: kube-system
spec:
  ports:
    - port: 80
      protocol: TCP
      targetPort: grpc
  selector:
    k8s-app: hubble-relay
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    k8s-app: hubble-ui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-management
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-relay
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  ports:
    - name: https
      port: 80
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-signal
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/name: netbird-dashboard
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud
  namespace: nextcloud
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: nextcloud
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  ports:
    - name: mysql
      nodePort: null
      port: 3306
      protocol: TCP
      targetPort: mysql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: mariadb
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.9.0
    helm.sh/chart: ollama-1.19.0
  name: ollama
  namespace: ollama
spec:
  ports:
    - name: http
      port: 11434
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/name: ollama
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.13
    helm.sh/chart: open-webui-6.19.0
  name: open-webui
  namespace: openwebui
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pingvinshare
    app.kubernetes.io/service: pingvinshare
    helm.sh/chart: app-template-4.0.1
  name: pingvinshare
  namespace: pingvinshare
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/controller: pingvinshare
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/name: pingvinshare
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  ports:
    - name: http
      nodePort: null
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/name: sealed-secrets
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: external-cluster
  namespace: traefik
spec:
  clusterIP: None
  ports:
    - name: ingress-port
      port: 443
      protocol: TCP
      targetPort: 443
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    netbird.io/expose: "true"
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
  namespace: traefik
spec:
  externalTrafficPolicy: Local
  internalTrafficPolicy: Local
  loadBalancerIP: 192.168.0.90
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: ssh
    - name: webpublic
      port: 9443
      protocol: TCP
      targetPort: webpublic
    - name: websecure
      port: 443
      protocol: TCP
      targetPort: websecure
  selector:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/name: traefik
  type: LoadBalancer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: gitea
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: gitea-postgresql
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: immich-library
  namespace: immich
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 150Gi
  storageClassName: proxmox-csi
  volumeName: immich-library
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: proxmox-csi
  volumeName: netbird-management
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: proxmox-csi
  volumeName: netbird-signal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-data
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 300Gi
  storageClassName: proxmox-csi
  volumeName: nextcloud-data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: nextcloud-mariadb
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.9.0
    helm.sh/chart: ollama-1.19.0
  name: ollama
  namespace: ollama
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 60Gi
  storageClassName: proxmox-csi
  volumeName: ollama
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: open-webui
  namespace: openwebui
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: open-webui
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pingvinshare
  namespace: pingvinshare
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: proxmox-csi
  volumeName: pingvinshare
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cert-manager
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cert-manager
        app.kubernetes.io/version: v1.18.0
        helm.sh/chart: cert-manager-v1.18.0
    spec:
      containers:
        - args:
            - --v=2
            - --cluster-resource-namespace=$(POD_NAMESPACE)
            - --leader-election-namespace=cert-manager
            - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.18.0
            - --max-concurrent-challenges=60
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.18.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
            - containerPort: 9403
              name: http-healthz
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cainjector
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cainjector
        app.kubernetes.io/version: v1.18.0
        helm.sh/chart: cert-manager-v1.18.0
    spec:
      containers:
        - args:
            - --v=2
            - --leader-election-namespace=cert-manager
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.18.0
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-cainjector
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: webhook
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: webhook
        app.kubernetes.io/version: v1.18.0
        helm.sh/chart: cert-manager-v1.18.0
    spec:
      containers:
        - args:
            - --v=2
            - --secure-port=10250
            - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
            - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
            - --dynamic-serving-dns-names=cert-manager-webhook
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.18.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
            - containerPort: 10250
              name: https
              protocol: TCP
            - containerPort: 6080
              name: healthcheck
              protocol: TCP
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-cloudnative-pg
  namespace: cnpg-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: cnpg
      app.kubernetes.io/name: cloudnative-pg
  template:
    metadata:
      annotations:
        checksum/config: b9237dbc244ee5efe8baa1d10c3a6cbd9b52ef95f4ab78b5118b274c39ffd8ce
        checksum/monitoring-config: 655cb80af844592951cf6b828dfea17fd5a7b9f24e72763b9607d7ef03a51f1d
        checksum/rbac: e2782c26570f196e5d27d767f1b6ea78c7aac8cf3f3b65e675142eb15d3a8f77
      labels:
        app.kubernetes.io/instance: cnpg
        app.kubernetes.io/name: cloudnative-pg
    spec:
      containers:
        - args:
            - controller
            - --leader-elect
            - --max-concurrent-reconciles=10
            - --config-map-name=cnpg-controller-manager-config
            - --webhook-port=9443
          command:
            - /manager
          env:
            - name: OPERATOR_IMAGE_NAME
              value: ghcr.io/cloudnative-pg/cloudnative-pg:1.26.0
            - name: OPERATOR_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MONITORING_QUERIES_CONFIGMAP
              value: cnpg-default-monitoring
          image: ghcr.io/cloudnative-pg/cloudnative-pg:1.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /readyz
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 3
          name: manager
          ports:
            - containerPort: 8080
              name: metrics
              protocol: TCP
            - containerPort: 9443
              name: webhook-server
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /readyz
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 10001
            runAsUser: 10001
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 6
            httpGet:
              path: /readyz
              port: 9443
              scheme: HTTPS
            periodSeconds: 5
          volumeMounts:
            - mountPath: /controller
              name: scratch-data
            - mountPath: /run/secrets/cnpg.io/webhook
              name: webhook-certificates
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cnpg-cloudnative-pg
      terminationGracePeriodSeconds: 10
      volumes:
        - emptyDir: {}
          name: scratch-data
        - name: webhook-certificates
          secret:
            defaultMode: 420
            optional: true
            secretName: cnpg-webhook-cert
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: coturn
      app.kubernetes.io/name: coturn
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: coturn
        app.kubernetes.io/name: coturn
    spec:
      containers:
        - args:
            - --listening-port=3478
            - --tls-listening-port=5349
            - --user=netbird:$$COTURN_USER_netbird_KEY
            - --cert=/usr/local/etc/tls.crt
            - --pkey=/usr/local/etc/tls.key
            - --realm=coturn.homelab.olav.ninja
            - --log-file=stdout
            - --no-software-attribute
            - --no-cli
            - --listening-ip=0.0.0.0
          env:
            - name: COTURN_USER_netbird_KEY
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
          image: coturn/coturn:4.6.3-r3
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            tcpSocket:
              port: tcp
          name: coturn
          ports:
            - containerPort: 3478
              name: tcp
              protocol: TCP
            - containerPort: 3478
              name: udp
              protocol: UDP
            - containerPort: 5349
              name: tcp-tls
              protocol: TCP
            - containerPort: 5349
              name: udp-tls
              protocol: UDP
          readinessProbe:
            tcpSocket:
              port: tcp
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/turnserver.conf
              name: config
              subPath: turnserver.conf
            - mountPath: /usr/local/etc
              name: certs
      securityContext: {}
      serviceAccountName: coturn
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      volumes:
        - configMap:
            name: coturn
          name: config
        - name: certs
          secret:
            secretName: coturn
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    release: crossplane
  name: crossplane
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 1.20.0
        helm.sh/chart: crossplane-1.20.0
        release: crossplane
    spec:
      containers:
        - args:
            - core
            - start
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: LEADER_ELECTION
              value: "true"
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_SERVER_CERTS_DIR
              value: /tls/server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
            - name: TLS_CLIENT_CERTS_DIR
              value: /tls/client
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane
          ports:
            - containerPort: 8081
              name: readyz
            - containerPort: 9443
              name: webhooks
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
          startupProbe:
            failureThreshold: 30
            periodSeconds: 2
            tcpSocket:
              port: readyz
          volumeMounts:
            - mountPath: /cache/xpkg
              name: package-cache
            - mountPath: /cache/xfn
              name: function-cache
            - mountPath: /tls/server
              name: tls-server-certs
            - mountPath: /tls/client
              name: tls-client-certs
      hostNetwork: false
      initContainers:
        - args:
            - core
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: WEBHOOK_SERVICE_NAME
              value: crossplane-webhooks
            - name: WEBHOOK_SERVICE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: WEBHOOK_SERVICE_PORT
              value: "9443"
            - name: TLS_CA_SECRET_NAME
              value: crossplane-root-ca
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: crossplane
      volumes:
        - emptyDir:
            medium: null
            sizeLimit: 20Mi
          name: package-cache
        - emptyDir:
            medium: null
            sizeLimit: 512Mi
          name: function-cache
        - name: tls-server-certs
          secret:
            secretName: crossplane-tls-server
        - name: tls-client-certs
          secret:
            secretName: crossplane-tls-client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane-rbac-manager
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.20.0
    helm.sh/chart: crossplane-1.20.0
    release: crossplane
  name: crossplane-rbac-manager
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane-rbac-manager
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane-rbac-manager
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 1.20.0
        helm.sh/chart: crossplane-1.20.0
        release: crossplane
    spec:
      containers:
        - args:
            - rbac
            - start
            - --provider-clusterrole=crossplane:allowed-provider-permissions
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: LEADER_ELECTION
              value: "true"
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      initContainers:
        - args:
            - rbac
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
          image: xpkg.crossplane.io/crossplane/crossplane:v1.20.0
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: rbac-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: proxmox-csi-plugin
      app.kubernetes.io/name: proxmox-csi-plugin
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: ce080eff0c26b50fe73bf9fcda017c8ad47c1000729fd0c555cfe3535c6d6222
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: proxmox-csi-plugin
        app.kubernetes.io/name: proxmox-csi-plugin
    spec:
      containers:
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --cloud-config=/etc/proxmox/config.yaml
          image: ghcr.io/sergelogvinov/proxmox-csi-controller:v0.12.0
          imagePullPolicy: IfNotPresent
          name: proxmox-csi-plugin-controller
          ports: null
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
            - mountPath: /etc/proxmox/
              name: cloud-config
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --leader-election
            - --default-fstype=ext4
          image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --leader-election
            - --enable-capacity
            - --capacity-ownerref-level=2
            - --default-fstype=ext4
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          image: registry.k8s.io/sig-storage/csi-provisioner:v5.3.0
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --handle-volume-inuse-error=false
            - --leader-election
          image: registry.k8s.io/sig-storage/csi-resizer:v1.13.2
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
          image: registry.k8s.io/sig-storage/livenessprobe:v2.15.0
          imagePullPolicy: IfNotPresent
          name: liveness-probe
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
      enableServiceLinks: false
      hostAliases: []
      initContainers: []
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroup: 65532
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: proxmox-csi-plugin-controller
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: controller
              app.kubernetes.io/instance: proxmox-csi-plugin
              app.kubernetes.io/name: proxmox-csi-plugin
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
      volumes:
        - emptyDir: {}
          name: socket-dir
        - name: cloud-config
          secret:
            secretName: proxmox-csi-plugin
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: gitea
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: 3ffed20ea9c55171e092c1d8fc17b75051edfe5685b7f5f2f8d47dd3a1943274
        checksum/oauth_0: 01764dcd9b49e15763176a4daa8f8e10f4902756616d3e834ef1341e80062ad5
      labels:
        app: gitea
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: gitea
        app.kubernetes.io/version: 1.23.8
        helm.sh/chart: gitea-12.0.0
        version: 1.23.8
    spec:
      containers:
        - env:
            - name: SSH_LISTEN_PORT
              value: "2222"
            - name: SSH_PORT
              value: "22"
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMPDIR
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 200
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: gitea
          ports:
            - containerPort: 2222
              name: ssh
            - containerPort: 3000
              name: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      initContainers:
        - command:
            - /usr/sbinx/init_directory_structure.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          name: init-directories
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbinx
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
        - command:
            - /usr/sbinx/config_environment.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMP_EXISTING_ENVS_FILE
              value: /tmp/existing-envs
            - name: ENV_TO_INI_MOUNT_POINT
              value: /env-to-ini-mounts
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          name: init-app-ini
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbinx
              name: config
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
            - mountPath: /env-to-ini-mounts/inlines/
              name: inline-config-sources
        - command:
            - /usr/sbinx/configure_gitea.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
            - name: GITEA_OAUTH_KEY_0
              valueFrom:
                secretKeyRef:
                  key: key
                  name: gitea-oidc-credentials
            - name: GITEA_OAUTH_SECRET_0
              valueFrom:
                secretKeyRef:
                  key: secret
                  name: gitea-oidc-credentials
            - name: GITEA_ADMIN_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD_MODE
              value: keepUpdated
          image: docker.gitea.com/gitea:1.23.8-rootless
          imagePullPolicy: IfNotPresent
          name: configure-gitea
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/sbinx
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      securityContext:
        fsGroup: 1000
      terminationGracePeriodSeconds: 60
      volumes:
        - name: init
          secret:
            defaultMode: 110
            secretName: gitea-init
        - name: config
          secret:
            defaultMode: 110
            secretName: gitea
        - name: inline-config-sources
          secret:
            secretName: gitea-inline-config
        - emptyDir: {}
          name: temp
        - name: data
          persistentVolumeClaim:
            claimName: gitea
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: hajimari
      app.kubernetes.io/name: hajimari
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: hajimari
        app.kubernetes.io/name: hajimari
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: TZ
              value: UTC
          image: ghcr.io/toboshii/hajimari:v0.3.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          name: hajimari
          ports:
            - containerPort: 3000
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 0
            periodSeconds: 5
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config/config.yaml
              name: hajimari-settings
              subPath: config.yaml
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: hajimari
      volumes:
        - configMap:
            name: hajimari-settings
          name: hajimari-settings
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-machine-learning
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: machine-learning
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: machine-learning
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql-rw
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: immich-postgresql-user
            - name: DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: immich-postgresql-user
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
            - name: TRANSFORMERS_CACHE
              value: /cache
          envFrom:
            - secretRef:
                name: immich-postgresql-user
          image: ghcr.io/immich-app/immich-machine-learning:v1.132.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-machine-learning
          ports:
            - containerPort: 3003
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 3
              ephemeral-storage: 10Gi
            requests:
              cpu: 10m
              memory: 2Gi
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /cache
              name: cache
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - emptyDir: {}
          name: cache
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-server
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: server
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 88c813a390a4e10d83e7da3a7df99e9aa98f2f6ce28cc391662822301a54b78e
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: server
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql-rw
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: immich-postgresql-user
            - name: DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: immich-postgresql-user
            - name: IMMICH_CONFIG_FILE
              value: /config/immich-config.yaml
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
          envFrom:
            - secretRef:
                name: immich-postgresql-user
          image: ghcr.io/immich-app/immich-server:v1.132.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-server
          ports:
            - containerPort: 2283
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /usr/src/app/upload
              name: library
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - name: config
          secret:
            secretName: immich-config
        - name: library
          persistentVolumeClaim:
            claimName: immich-library
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: cilium-operator
    app.kubernetes.io/part-of: cilium
    io.cilium/app: operator
    name: cilium-operator
  name: cilium-operator
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      io.cilium/app: operator
      name: cilium-operator
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 100%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/port: "9963"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: cilium-operator
        app.kubernetes.io/part-of: cilium
        io.cilium/app: operator
        name: cilium-operator
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  io.cilium/app: operator
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
            - --debug=$(CILIUM_DEBUG)
          command:
            - cilium-operator-generic
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_DEBUG
              valueFrom:
                configMapKeyRef:
                  key: debug
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/operator-generic:v1.17.4@sha256:a3906412f477b09904f46aac1bed28eb522bef7899ed7dd81c15f78b7aa1b9b5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
          name: cilium-operator
          ports:
            - containerPort: 9963
              hostPort: 9963
              name: prometheus
              protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp/cilium/config-map
              name: cilium-config-path
              readOnly: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      serviceAccountName: cilium-operator
      tolerations:
        - operator: Exists
      volumes:
        - configMap:
            name: cilium-config
          name: cilium-config-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-relay
  name: hubble-relay
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-relay
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cilium.io/hubble-relay-configmap-checksum: 0aebee6bdee393dd840ea0e068f2efeae387cc07114bb26becb030f0ab1e2397
      labels:
        app.kubernetes.io/name: hubble-relay
        app.kubernetes.io/part-of: cilium
        k8s-app: hubble-relay
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: false
      containers:
        - args:
            - serve
          command:
            - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.17.4@sha256:c16de12a64b8b56de62b15c1652d036253b40cd7fa643d7e1a404dc71dc66441
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 12
            grpc:
              port: 4222
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 10
          name: hubble-relay
          ports:
            - containerPort: 4245
              name: grpc
          readinessProbe:
            grpc:
              port: 4222
            timeoutSeconds: 3
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 4222
            initialDelaySeconds: 10
            periodSeconds: 3
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/hubble-relay
              name: config
              readOnly: true
            - mountPath: /var/lib/hubble-relay/tls
              name: tls
              readOnly: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: null
      restartPolicy: Always
      securityContext:
        fsGroup: 65532
      serviceAccountName: hubble-relay
      terminationGracePeriodSeconds: 1
      volumes:
        - configMap:
            items:
              - key: config.yaml
                path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: client.crt
                    - key: tls.key
                      path: client.key
                    - key: ca.crt
                      path: hubble-server-ca.crt
                  name: hubble-relay-client-certs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-ui
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cilium.io/hubble-ui-nginx-configmap-checksum: de069d2597e16e4de004ce684b15d74b2ab6051c717ae073d86199a76d91fcf1
      labels:
        app.kubernetes.io/name: hubble-ui
        app.kubernetes.io/part-of: cilium
        k8s-app: hubble-ui
    spec:
      automountServiceAccountToken: true
      containers:
        - image: quay.io/cilium/hubble-ui:v0.13.2@sha256:9e37c1296b802830834cc87342a9182ccbb71ffebb711971e849221bd9d59392
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
          name: frontend
          ports:
            - containerPort: 8081
              name: http
          readinessProbe:
            httpGet:
              path: /
              port: 8081
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/nginx/conf.d/default.conf
              name: hubble-ui-nginx-conf
              subPath: nginx.conf
            - mountPath: /tmp
              name: tmp-dir
        - env:
            - name: EVENTS_SERVER_PORT
              value: "8090"
            - name: FLOWS_API_ADDR
              value: hubble-relay:80
          image: quay.io/cilium/hubble-ui-backend:v0.13.2@sha256:a034b7e98e6ea796ed26df8f4e71f83fc16465a19d166eff67a03b822c0bfa15
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
            - containerPort: 8090
              name: grpc
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts: null
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: null
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      serviceAccountName: hubble-ui
      volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.14
  name: proxmox-cloud-controller-manager
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: proxmox-cloud-controller-manager
      app.kubernetes.io/name: proxmox-cloud-controller-manager
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: ce080eff0c26b50fe73bf9fcda017c8ad47c1000729fd0c555cfe3535c6d6222
      labels:
        app.kubernetes.io/instance: proxmox-cloud-controller-manager
        app.kubernetes.io/name: proxmox-cloud-controller-manager
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      containers:
        - args:
            - --v=4
            - --cloud-provider=proxmox
            - --cloud-config=/etc/proxmox/config.yaml
            - --controllers=cloud-node,cloud-node-lifecycle
            - --leader-elect-resource-name=cloud-controller-manager-proxmox
            - --use-service-account-credentials
            - --secure-port=10258
            - --authorization-always-allow-paths=/healthz,/livez,/readyz,/metrics
          image: ghcr.io/sergelogvinov/proxmox-cloud-controller-manager:v0.9.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 30
            timeoutSeconds: 5
          name: proxmox-cloud-controller-manager
          ports:
            - containerPort: 10258
              name: metrics
              protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /etc/proxmox
              name: cloud-config
              readOnly: true
      enableServiceLinks: false
      initContainers: []
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroup: 10258
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 10258
        runAsNonRoot: true
        runAsUser: 10258
      serviceAccountName: proxmox-cloud-controller-manager
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          operator: Exists
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: proxmox-cloud-controller-manager
              app.kubernetes.io/name: proxmox-cloud-controller-manager
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
      volumes:
        - name: cloud-config
          secret:
            defaultMode: 416
            items:
              - key: config.yaml
                path: config.yaml
            secretName: proxmox-ccm-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netbird-agent
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app: netbird-agent
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: netbird-agent
      labels:
        app: netbird-agent
    spec:
      containers:
        - args:
            - TCP-LISTEN:443,fork
            - TCP:traefik.traefik.svc.cluster.local:443
          image: alpine/socat:1.8.0.3
          name: traefik-proxy
          ports:
            - containerPort: 443
              name: http
              protocol: TCP
        - args:
            - TCP-LISTEN:22,fork
            - TCP:traefik.traefik.svc.cluster.local:22
          image: alpine/socat:1.8.0.3
          name: gitea-ssh-proxy
          ports:
            - containerPort: 22
              name: ssh
              protocol: TCP
        - env:
            - name: CLOUDFLARE_API_TOKEN
              valueFrom:
                secretKeyRef:
                  key: cloudflare-dns-api-token
                  name: cloudflare-api-credentials
            - name: DOMAINS
              value: homelab.olav.ninja,*.homelab.olav.ninja
            - name: IP4_PROVIDER
              value: local.iface:wt0
            - name: IP6_PROVIDER
              value: none
            - name: UPDATE_CRON
              value: '@every 5m'
          image: favonia/cloudflare-ddns:1.15.1
          name: cloudflare-dns-updater
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
        - env:
            - name: NB_SETUP_KEY
              valueFrom:
                secretKeyRef:
                  key: setupKey
                  name: netbird-agent-setup-key
            - name: NB_HOSTNAME
              value: k8s-agent
            - name: NB_LOG_LEVEL
              value: info
            - name: NB_MANAGEMENT_URL
              value: https://netbird.homelab.olav.ninja
            - name: NB_ADMIN_URL
              value: https://netbird.homelab.olav.ninja
            - name: NB_CONFIG
              value: /config/config.json
          image: netbirdio/netbird:0.46.0
          name: netbird-agent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_ADMIN
                - PERFMON
                - BPF
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /config
              name: config
      volumes:
        - emptyDir: {}
          name: config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-management
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: a7d495890f56d73e445f354af57bb002ed4c977911c8422a1650a4a531d69800
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-management
    spec:
      containers:
        - args:
            - --log-level
            - info
            - --log-file
            - console
            - --dns-domain
            - netbird
          image: netbirdio/management:0.46.0
          imagePullPolicy: IfNotPresent
          name: netbird-management
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /var/lib/netbird
              name: management
      initContainers:
        - args:
            - |
              go install github.com/drone/envsubst/cmd/envsubst@latest && envsubst < /tmp/netbird/management.tmpl.json > /etc/netbird/management.json && cat /etc/netbird/management.json
          command:
            - /bin/sh
            - -c
          env:
            - name: NETBIRD_SIGNAL_URI
              value: netbird.homelab.olav.ninja:443
            - name: NETBIRD_SIGNAL_PROTOCOL
              value: https
            - name: NETBIRD_STUN_URI
              value: stun:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_URI
              value: turn:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_USER
              value: netbird
            - name: NETBIRD_TURN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
            - name: NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: NETBIRD_MGMT_API_CERT_FILE
              value: ""
            - name: NETBIRD_MGMT_API_CERT_KEY_FILE
              value: ""
            - name: NETBIRD_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_USER_ID_CLAIM
            - name: NETBIRD_AUTH_DEVICE_AUTH_PROVIDER
              value: hosted
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/auth
            - name: NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
            - name: NETBIRD_AUTH_DEVICE_AUTH_SCOPE
              value: openid
            - name: NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN
              value: "false"
            - name: NETBIRD_IDP_MANAGER_TYPE
              value: keycloak
            - name: NETBIRD_IDP_CLIENT_ID
              value: netbird-backend
            - name: NETBIRD_IDP_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: netbird-backend-oidc-credentials
            - name: NETBIRD_IDP_GRANT_TYPE
              value: client_credentials
            - name: NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/admin/realms/homelab
            - name: NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
          envFrom:
            - secretRef:
                name: netbird-relay-secret
          image: golang:latest
          imagePullPolicy: IfNotPresent
          name: configure
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /tmp/netbird
              name: config-template
      securityContext: {}
      serviceAccountName: netbird-backend-management
      volumes:
        - emptyDir:
            medium: Memory
          name: config
        - configMap:
            name: netbird-backend-management
          name: config-template
        - name: management
          persistentVolumeClaim:
            claimName: netbird-backend-management
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-relay
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-relay
    spec:
      containers:
        - args:
            - --listen-address
            - :80
            - --log-level
            - info
            - --log-file
            - console
          envFrom:
            - secretRef:
                name: netbird-relay-secret
          image: netbirdio/relay:0.46.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: 80
          name: netbird-relay
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: 80
          resources: {}
          securityContext: null
      securityContext: null
      serviceAccountName: netbird-backend-relay
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-signal
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-signal
    spec:
      containers:
        - args:
            - --port
            - "80"
            - --log-level
            - info
            - --log-file
            - console
          image: netbirdio/signal:0.46.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: https
          name: netbird-signal
          ports:
            - containerPort: 80
              name: https
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: https
          resources: {}
          securityContext: null
          volumeMounts:
            - mountPath: /var/lib/netbird
              name: signal
      securityContext: null
      serviceAccountName: netbird-backend-signal
      volumes:
        - name: signal
          persistentVolumeClaim:
            claimName: netbird-backend-signal
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-dashboard
      app.kubernetes.io/name: netbird-dashboard
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-dashboard
        app.kubernetes.io/name: netbird-dashboard
    spec:
      containers:
        - args:
            - |
              sed -i 's/listen \[\:\:\]\:80 default_server\;//g' /etc/nginx/http.d/default.conf && /usr/bin/supervisord -c /etc/supervisord.conf
          command:
            - /bin/sh
            - -c
          env:
            - name: AUTH_AUDIENCE
              value: netbird
            - name: AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: AUTH_CLIENT_ID
              value: netbird
            - name: AUTH_SUPPORTED_SCOPES
              value: openid profile email offline_access netbird-api
            - name: USE_AUTH0
              value: "false"
            - name: NETBIRD_MGMT_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
            - name: NETBIRD_MGMT_GRPC_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
          image: netbirdio/dashboard:v2.13.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /
              port: http
          name: netbird-dashboard
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources: {}
          securityContext: {}
      securityContext: {}
      serviceAccountName: netbird-dashboard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud
  namespace: nextcloud
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: app
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: nextcloud
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204
        nextcloud-config-hash: 93a9742eed1608d05f159d9fa1830f38ea212bcc9440dceccdf0528d6ed6aefc
        php-config-hash: ef6725a34482d427b584b8ddff54804f14e7c98827ae18ff7642f7cf0dd254af
      labels:
        app.kubernetes.io/component: app
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/name: nextcloud
    spec:
      containers:
        - env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: MYSQL_HOST
              value: nextcloud-mariadb
            - name: MYSQL_DATABASE
              value: nextcloud
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
          image: nextcloud:30.0.10-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nextcloud
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
        - command:
            - /cron.sh
          env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: MYSQL_HOST
              value: nextcloud-mariadb
            - name: MYSQL_DATABASE
              value: nextcloud
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
          image: nextcloud:30.0.10-apache
          imagePullPolicy: IfNotPresent
          name: nextcloud-cron
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
      initContainers:
        - command:
            - sh
            - -c
            - until mysql --host=nextcloud-mariadb --user=${MYSQL_USER} --password=${MYSQL_PASSWORD} --execute="SELECT 1;"; do echo waiting for mysql; sleep 2; done;
          env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          name: mariadb-isalive
          resources: {}
          securityContext: {}
      securityContext:
        fsGroup: 33
      volumes:
        - name: nextcloud-main
          persistentVolumeClaim:
            claimName: nextcloud-data
        - configMap:
            name: nextcloud-config
          name: nextcloud-config
        - configMap:
            name: nextcloud-phpconfig
          name: nextcloud-phpconfig
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.9.0
    helm.sh/chart: ollama-1.19.0
  name: ollama
  namespace: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: ollama
      app.kubernetes.io/name: ollama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: ollama
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: ollama
        app.kubernetes.io/version: 0.9.0
        helm.sh/chart: ollama-1.19.0
    spec:
      containers:
        - args: null
          env:
            - name: OLLAMA_HOST
              value: 0.0.0.0:11434
          envFrom: null
          image: ollama/ollama:0.9.0-rocm
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: ollama
          ports:
            - containerPort: 11434
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              amd.com/gpu: 1
            requests: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /root/.ollama
              name: ollama-data
      securityContext: {}
      serviceAccountName: ollama
      tolerations: null
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/controller: pingvinshare
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pingvinshare
    helm.sh/chart: app-template-4.0.1
  name: pingvinshare
  namespace: pingvinshare
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/controller: pingvinshare
      app.kubernetes.io/instance: pingvinshare
      app.kubernetes.io/name: pingvinshare
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/controller: pingvinshare
        app.kubernetes.io/instance: pingvinshare
        app.kubernetes.io/name: pingvinshare
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: TRUST_PROXY
              value: "true"
          image: ghcr.io/stonith404/pingvin-share:v1.13.0
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          name: app
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 5m
              memory: 256Mi
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 0
            periodSeconds: 5
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /opt/app/config.yaml
              name: config
              subPath: config.yaml
            - mountPath: /data
              name: data
      dnsPolicy: ClusterFirst
      enableServiceLinks: false
      hostIPC: false
      hostNetwork: false
      hostPID: false
      initContainers:
        - command:
            - sh
            - -c
            - |
              apk add --no-cache gettext
              echo "Processing config file..."
              envsubst < /config-template/config.yaml > /config/config.yaml
              echo "Config file processed successfully"
          env:
            - name: OIDC_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: pingvinshare-oidc-credentials
          image: alpine:3.22
          name: config-processor
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /config-template
              name: config-template
              readOnly: true
            - mountPath: /data
              name: data
      serviceAccountName: default
      volumes:
        - emptyDir: {}
          name: config
        - configMap:
            name: pingvinshare-config
          name: config-template
        - name: data
          persistentVolumeClaim:
            claimName: pingvinshare
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: sealed-secrets
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: sealed-secrets
        app.kubernetes.io/version: 0.29.0
        helm.sh/chart: sealed-secrets-2.5.13
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: sealed-secrets
                    app.kubernetes.io/name: sealed-secrets
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - args:
            - --key-prefix
            - sealed-secrets-key
            - --update-status
            - --key-renew-period
            - "0"
          command:
            - controller
          image: docker.io/bitnami/sealed-secrets-controller:0.29.0-debian-12-r5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: sealed-secrets
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: sealed-secrets
      volumes:
        - emptyDir: {}
          name: empty-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
  namespace: traefik
spec:
  minReadySeconds: 0
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: traefik-traefik
      app.kubernetes.io/name: traefik
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9100"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: traefik-traefik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: traefik
        helm.sh/chart: traefik-35.4.0
    spec:
      automountServiceAccountToken: true
      containers:
        - args:
            - --global.checknewversion
            - --global.sendanonymoususage
            - --entryPoints.metrics.address=:9100/tcp
            - --entryPoints.ssh.address=:2222/tcp
            - --entryPoints.traefik.address=:8080/tcp
            - --entryPoints.web.address=:8000/tcp
            - --entryPoints.webpublic.address=:9443/tcp
            - --entryPoints.websecure.address=:8443/tcp
            - --entryPoints.websecure.asDefault=true
            - --api.dashboard=true
            - --ping=true
            - --metrics.prometheus=true
            - --metrics.prometheus.entrypoint=metrics
            - --providers.kubernetescrd
            - --providers.kubernetescrd.allowEmptyServices=true
            - --providers.kubernetesingress
            - --providers.kubernetesingress.allowEmptyServices=true
            - --providers.kubernetesingress.ingressendpoint.publishedservice=traefik/traefik
            - --entryPoints.webpublic.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.webpublic.http.tls=true
            - --entryPoints.webpublic.transport.respondingTimeouts.readTimeout=0
            - --entryPoints.websecure.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.websecure.http.tls=true
            - --entryPoints.websecure.transport.respondingTimeouts.readTimeout=0
            - --log.level=INFO
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: docker.io/traefik:v3.4.0
          imagePullPolicy: IfNotPresent
          lifecycle: null
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
            - containerPort: 9100
              name: metrics
              protocol: TCP
            - containerPort: 2222
              name: ssh
              protocol: TCP
            - containerPort: 8080
              name: traefik
              protocol: TCP
            - containerPort: 8000
              name: web
              protocol: TCP
            - containerPort: 9443
              name: webpublic
              protocol: TCP
            - containerPort: 8443
              name: websecure
              protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: null
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /tmp
              name: tmp
      hostNetwork: false
      securityContext:
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: traefik
      terminationGracePeriodSeconds: 60
      volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
  serviceName: gitea-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.5.0
        helm.sh/chart: postgresql-16.7.2
      name: gitea-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: gitea
            - name: POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/password
            - name: POSTGRES_POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/postgres-password
            - name: POSTGRES_DATABASE
              value: gitea
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/bitnami/postgresql:15-debian-11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/postgresql/secrets/
              name: postgresql-password
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: gitea-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            secretName: gitea-postgresql
        - emptyDir:
            medium: Memory
          name: dshm
        - name: data
          persistentVolumeClaim:
            claimName: gitea-postgresql
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey-primary
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey
  serviceName: gitea-valkey-headless
  template:
    metadata:
      annotations:
        checksum/configmap: c8cdc0c4c772ac3192446a08bf401c29f1e39f33614657d36e4bb1692e34b39f
        checksum/health: e3a0f06458110f02986bb8df4391c43567355d3582dd02f25447184391196fbc
        checksum/scripts: 791de9ea7b477455268cda3e85c8e58a1505e8d19ff8e851d78a246cb185a0f7
        checksum/secret: b6a1ec38f338ddc2549686770cd644719aa49f186495fe8a75c2d5a761d66bf9
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: valkey
        app.kubernetes.io/version: 8.1.1
        helm.sh/chart: valkey-3.0.4
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: valkey
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-primary.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: VALKEY_REPLICATION_MODE
              value: primary
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: VALKEY_PASSWORD_FILE
              value: /opt/bitnami/valkey/secrets/valkey-password
            - name: VALKEY_TLS_ENABLED
              value: "no"
            - name: VALKEY_PORT
              value: "6379"
          image: docker.io/bitnami/valkey:8.1.1-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: valkey
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /opt/bitnami/valkey/secrets/
              name: valkey-password
            - mountPath: /data
              name: valkey-data
            - mountPath: /opt/bitnami/valkey/mounted-etc
              name: config
            - mountPath: /opt/bitnami/valkey/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: gitea-valkey-primary
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: gitea-valkey-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: gitea-valkey-health
          name: health
        - name: valkey-password
          secret:
            items:
              - key: valkey-password
                path: valkey-password
            secretName: gitea-valkey
        - configMap:
            name: gitea-valkey-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: valkey-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  serviceName: immich-redis-headless
  template:
    metadata:
      annotations:
        checksum/configmap: 2a9ab4a5432825504d910f022638674ce88eaefe9f9f595ad8bc107377d104fb
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: bdec350b84a1ace7cc118db113a21c9f160bde3425b07714e1c0c1da722621cf
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: immich
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.4.3
        helm.sh/chart: redis-20.13.2
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          image: docker.io/bitnami/redis:7.4.3-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /data
              name: redis-data
            - mountPath: /opt/bitnami/redis/mounted-etc
              name: config
            - mountPath: /opt/bitnami/redis/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: immich-redis-master
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: immich-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: immich-redis-health
          name: health
        - configMap:
            name: immich-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: redis-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
  serviceName: keycloak-headless
  template:
    metadata:
      annotations:
        checksum/configmap-env-vars: d80c992feef3a8b41fed394f5fb5434812268953473a2893746fd449c0ae97c7
      labels:
        app.kubernetes.io/app-version: 26.2.5
        app.kubernetes.io/component: keycloak
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: keycloak
        app.kubernetes.io/version: 26.2.5
        helm.sh/chart: keycloak-24.7.3
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: keycloak
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - env:
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KC_BOOTSTRAP_ADMIN_PASSWORD_FILE
              value: /opt/bitnami/keycloak/secrets/pazzword
            - name: KEYCLOAK_DATABASE_PASSWORD_FILE
              value: /opt/bitnami/keycloak/secrets/db-password
            - name: KEYCLOAK_HTTP_RELATIVE_PATH
              value: /
            - name: KC_SPI_ADMIN_REALM
              value: master
          envFrom:
            - configMapRef:
                name: keycloak-env-vars
          image: docker.io/bitnami/keycloak:26.2.5-debian-12-r1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 300
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 5
          name: keycloak
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 7800
              name: discovery
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /realms/master
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1.5"
              ephemeral-storage: 2Gi
              memory: 3072Mi
            requests:
              cpu: "1.0"
              ephemeral-storage: 50Mi
              memory: 2048Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /bitnami/keycloak
              name: empty-dir
              subPath: app-volume-dir
            - mountPath: /opt/bitnami/keycloak/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/keycloak/lib/quarkus
              name: empty-dir
              subPath: app-quarkus-dir
            - mountPath: /opt/bitnami/keycloak/data
              name: empty-dir
              subPath: app-data-dir
            - mountPath: /opt/bitnami/keycloak/providers
              name: empty-dir
              subPath: app-providers-dir
            - mountPath: /opt/bitnami/keycloak/themes
              name: empty-dir
              subPath: app-themes-dir
            - mountPath: /opt/bitnami/keycloak/secrets
              name: keycloak-secrets
      enableServiceLinks: true
      initContainers:
        - args:
            - -ec
            - |
              . /opt/bitnami/scripts/liblog.sh

              info "Copying writable dirs to empty dir"
              # In order to not break the application functionality we need to make some
              # directories writable, so we need to copy it to an empty dir volume
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/data /emptydir/app-data-dir
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/providers /emptydir/app-providers-dir
              cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/themes /emptydir/app-themes-dir
              info "Copy operation completed"
          command:
            - /bin/bash
          image: docker.io/bitnami/keycloak:26.2.5-debian-12-r1
          imagePullPolicy: IfNotPresent
          name: prepare-write-dirs
          resources:
            limits:
              cpu: "1.5"
              ephemeral-storage: 2Gi
              memory: 3072Mi
            requests:
              cpu: "1.0"
              ephemeral-storage: 50Mi
              memory: 2048Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /emptydir
              name: empty-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: keycloak
      volumes:
        - emptyDir: {}
          name: empty-dir
        - name: keycloak-secrets
          projected:
            sources:
              - secret:
                  name: keycloak-admin-password
              - secret:
                  items:
                    - key: password
                      path: db-password
                  name: keycloak-db-credentials
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
  serviceName: keycloak-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.4.0
        helm.sh/chart: postgresql-16.6.6
      name: keycloak-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: bn_keycloak
            - name: POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/password
            - name: POSTGRES_POSTGRES_PASSWORD_FILE
              value: /opt/bitnami/postgresql/secrets/postgres-password
            - name: POSTGRES_DATABASE
              value: bitnami_keycloak
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/bitnami/postgresql:17.4.0-debian-12-r17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "bn_keycloak" -d "dbname=bitnami_keycloak" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "bn_keycloak" -d "dbname=bitnami_keycloak" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/postgresql/secrets/
              name: postgresql-password
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: keycloak-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            secretName: keycloak-db-credentials
        - emptyDir:
            medium: Memory
          name: dshm
        - emptyDir: {}
          name: data
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: mariadb
  serviceName: nextcloud-mariadb
  template:
    metadata:
      annotations:
        checksum/configuration: fb262c2f871ddf4c9fa925e29eb7198153cfc48305559085fcf1ef74cf716807
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mariadb
        app.kubernetes.io/version: 11.3.2
        helm.sh/chart: mariadb-18.2.0
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: mariadb
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: mariadb
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MARIADB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-root-password
                  name: nextcloud-database-auth
            - name: MARIADB_USER
              value: nextcloud
            - name: MARIADB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: MARIADB_DATABASE
              value: nextcloud
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mariadb
          ports:
            - containerPort: 3306
              name: mysql
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin ping -uroot -p"${password_aux}"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /bitnami/mariadb
              name: data
            - mountPath: /opt/bitnami/mariadb/conf/my.cnf
              name: config
              subPath: my.cnf
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/mariadb/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/mariadb/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/mariadb/logs
              name: empty-dir
              subPath: app-logs-dir
      initContainers:
        - args:
            - -ec
            - |
              #!/bin/bash

              . /opt/bitnami/scripts/libfs.sh
              # We copy the logs folder because it has symlinks to stdout and stderr
              if ! is_dir_empty /opt/bitnami/mariadb/logs; then
                cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir
              fi
          command:
            - /bin/bash
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          imagePullPolicy: IfNotPresent
          name: preserve-logs-symlinks
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /emptydir
              name: empty-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: nextcloud-mariadb
      volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            name: nextcloud-mariadb
          name: config
        - name: data
          persistentVolumeClaim:
            claimName: nextcloud-mariadb
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.13
    helm.sh/chart: open-webui-6.19.0
  name: open-webui
  namespace: openwebui
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: open-webui
      app.kubernetes.io/instance: openwebui
  serviceName: open-webui
  template:
    metadata:
      labels:
        app.kubernetes.io/component: open-webui
        app.kubernetes.io/instance: openwebui
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: 0.6.13
        helm.sh/chart: open-webui-6.19.0
    spec:
      automountServiceAccountToken: false
      containers:
        - env:
            - name: WEBUI_URL
              value: https://openwebui.homelab.olav.ninja
            - name: OLLAMA_BASE_URLS
              value: http://ollama.ollama.svc.cluster.local:11434
            - name: OPENAI_API_BASE_URL
              value: https://api.openai.com/v1
            - name: ENABLE_OPENAI_API
              value: "False"
            - name: ENABLE_OAUTH_SIGNUP
              value: "True"
            - name: OAUTH_MERGE_ACCOUNTS_BY_EMAIL
              value: "True"
            - name: OAUTH_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  key: clientId
                  name: openwebui-oidc-credentials
            - name: OAUTH_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: openwebui-oidc-credentials
            - name: OPENID_PROVIDER_URL
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: OAUTH_PROVIDER_NAME
              value: Keycloak
            - name: OAUTH_SCOPES
              value: openid email profile
          image: ghcr.io/open-webui/open-webui:0.6.13
          imagePullPolicy: IfNotPresent
          name: open-webui
          ports:
            - containerPort: 8080
              name: http
          tty: true
          volumeMounts:
            - mountPath: /app/backend/data
              name: data
      enableServiceLinks: false
      initContainers:
        - command:
            - sh
            - -c
            - cp -R -n /app/backend/data/* /tmp/app-data/
          image: ghcr.io/open-webui/open-webui:0.6.13
          imagePullPolicy: IfNotPresent
          name: copy-app-data
          volumeMounts:
            - mountPath: /tmp/app-data
              name: data
      serviceAccountName: open-webui
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: open-webui
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: energisk-cache-prime
  namespace: default
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - args:
                - |
                  URLS=" https://api.energisk.app/price?area=BE&currency=EUR https://api.energisk.app/price?area=BG&currency=BGN https://api.energisk.app/price?area=CH&currency=CHF https://api.energisk.app/price?area=CZ&currency=CZK https://api.energisk.app/price?area=DE&currency=EUR https://api.energisk.app/price?area=DK1&currency=DKK https://api.energisk.app/price?area=DK2&currency=DKK https://api.energisk.app/price?area=EE&currency=EUR https://api.energisk.app/price?area=ES&currency=EUR https://api.energisk.app/price?area=FI&currency=EUR https://api.energisk.app/price?area=FR&currency=EUR https://api.energisk.app/price?area=GR&currency=EUR https://api.energisk.app/price?area=HR&currency=EUR https://api.energisk.app/price?area=HU&currency=HUF https://api.energisk.app/price?area=IE&currency=EUR https://api.energisk.app/price?area=LT&currency=EUR https://api.energisk.app/price?area=LU&currency=EUR https://api.energisk.app/price?area=LV&currency=EUR https://api.energisk.app/price?area=NL&currency=EUR https://api.energisk.app/price?area=NO1&currency=NOK https://api.energisk.app/price?area=NO2&currency=NOK https://api.energisk.app/price?area=NO3&currency=NOK https://api.energisk.app/price?area=NO4&currency=NOK https://api.energisk.app/price?area=NO5&currency=NOK https://api.energisk.app/price?area=NORD&currency=EUR https://api.energisk.app/price?area=PL&currency=PLN https://api.energisk.app/price?area=PT&currency=EUR https://api.energisk.app/price?area=RO&currency=RON https://api.energisk.app/price?area=RS&currency=RSD https://api.energisk.app/price?area=SARD&currency=EUR https://api.energisk.app/price?area=SE1&currency=SEK https://api.energisk.app/price?area=SE2&currency=SEK https://api.energisk.app/price?area=SE3&currency=SEK https://api.energisk.app/price?area=SE4&currency=SEK https://api.energisk.app/price?area=SI&currency=EUR https://api.energisk.app/price?area=SK&currency=EUR https://api.energisk.app/price?area=SICI&currency=EUR https://api.energisk.app/price?area=SUD&currency=EUR "
                  for URL in $URLS; do

                    curl -s -o /dev/null -w '%{url_effective}\t%{http_code}\t%{time_total}s\n' $URL\&date=$(date -d tomorrow +'%Y-%m-%d');
                  done
              command:
                - /bin/sh
                - -c
              image: rockylinux:9-minimal
              name: curl-container
          restartPolicy: Never
  schedule: 0 13 * * *
---
apiVersion: batch/v1
kind: CronJob
metadata:
  labels:
    app.kubernetes.io/name: hubble-generate-certs
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-generate-certs
  name: hubble-generate-certs
  namespace: kube-system
spec:
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            k8s-app: hubble-generate-certs
        spec:
          affinity: null
          automountServiceAccountToken: true
          containers:
            - args:
                - --ca-generate=true
                - --ca-reuse-secret
                - --ca-secret-namespace=kube-system
                - --ca-secret-name=cilium-ca
                - --ca-common-name=Cilium CA
              command:
                - /usr/bin/cilium-certgen
              env:
                - name: CILIUM_CERTGEN_CONFIG
                  value: |
                    certs:
                    - name: hubble-server-certs
                      namespace: kube-system
                      commonName: "*.default.hubble-grpc.cilium.io"
                      hosts:
                      - "*.default.hubble-grpc.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - server auth
                      - client auth
                      validity: 8760h
                    - name: hubble-relay-client-certs
                      namespace: kube-system
                      commonName: "*.hubble-relay.cilium.io"
                      hosts:
                      - "*.hubble-relay.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - client auth
                      validity: 8760h
              image: quay.io/cilium/certgen:v0.2.1@sha256:ab6b1928e9c5f424f6b0f51c68065b9fd85e2f8d3e5f21fbd1a3cb27e6fb9321
              imagePullPolicy: IfNotPresent
              name: certgen
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
          hostNetwork: false
          restartPolicy: OnFailure
          securityContext:
            seccompProfile:
              type: RuntimeDefault
          serviceAccount: hubble-generate-certs
          serviceAccountName: hubble-generate-certs
      ttlSecondsAfterFinished: 1800
  schedule: 0 0 1 */4 *
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis-master
  namespace: immich
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: mariadb
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.29.0
    helm.sh/chart: sealed-secrets-2.5.13
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
---
apiVersion: apiextensions.crossplane.io/v1
kind: CompositeResourceDefinition
metadata:
  name: xoidcclients.oidc.homelab.olav.ninja
  namespace: crossplane
spec:
  group: oidc.homelab.olav.ninja
  names:
    kind: XOidcClient
    plural: xoidcclients
  versions:
    - name: v1alpha1
      referenceable: true
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                baseUrl:
                  description: Default URL to use when the auth server needs to redirect or link back to the client.
                  type: string
                clientId:
                  description: The Client ID for this client, referenced in the URI during authentication and in issued tokens
                  type: string
                clientSecretSecretRef:
                  description: |-
                    The client or client secret registered within the identity provider. This field is able to obtain its value from vault, use $${vault.ID} format.
                    Client Secret.
                  properties:
                    key:
                      description: The key to select.
                      type: string
                    name:
                      description: Name of the secret.
                      type: string
                    namespace:
                      description: Namespace of the secret.
                      type: string
                  required:
                    - key
                    - name
                    - namespace
                  type: object
                defaultScopes:
                  description: The default scopes to be requested when asking for authorization
                  items:
                    type: string
                  type: array
                description:
                  description: The description of this client in the GUI
                  type: string
                displayName:
                  description: The display name of this client in the GUI
                  type: string
                grantTypes:
                  description: A list of grant types that should be enabled for the client
                  items:
                    type: string
                  type: array
                postLogoutRedirectUris:
                  description: A list of valid URIs a browser is permitted to redirect to after a successful logout.
                  items:
                    type: string
                  type: array
                realm:
                  description: The realm this client is attached to
                  type: string
                redirectUris:
                  description: |-
                    A list of valid URIs a browser is permitted to redirect to after a successful login or logout. Simple
                    wildcards in the form of an asterisk can be used here. This attribute must be set if either standard_flow_enabled or implicit_flow_enabled
                    is set to true.
                  items:
                    type: string
                  type: array
                serviceAccountRoles:
                  description: A list of roles to assign to the clients service account
                  items:
                    properties:
                      client:
                        type: string
                      realm:
                        type: string
                      role:
                        type: string
                    type: object
                  type: array
                type:
                  description: Specifies the type of client
                  type: string
                webOrigins:
                  description: |-
                    A list of allowed CORS origins. To permit all valid
                    redirect URIs, add +. Note that this will not include the *
                    wildcard. To permit all origins, explicitly add *.
                  items:
                    type: string
                  type: array
              required:
                - clientId
                - realm
              type: object
          required:
            - spec
          type: object
      served: true
---
apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: keycloak-oidc-client
  namespace: crossplane
spec:
  compositeTypeRef:
    apiVersion: oidc.homelab.olav.ninja/v1alpha1
    kind: XOidcClient
  mode: Pipeline
  pipeline:
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: Client
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}
            spec:
              forProvider:
                name: {{ .observed.composite.resource.spec.displayName }}
                accessType: {{ .observed.composite.resource.spec.type }}
                clientId: {{ .observed.composite.resource.spec.clientId }}
                {{ with .observed.composite.resource.spec.clientSecretSecretRef }}
                clientSecretSecretRef: {{ toYaml . | nindent 6 }}
                {{ end }}
                description: {{ .observed.composite.resource.spec.description }}
                {{ with .observed.composite.resource.spec.baseUrl }}
                baseUrl: {{ . }}
                {{ end }}
                {{ with .observed.composite.resource.spec.redirectUris }}
                validRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.postLogoutRedirectUris }}
                validPostLogoutRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.webOrigins }}
                webOrigins: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ if has "client_credentials" .observed.composite.resource.spec.grantTypes }}
                serviceAccountsEnabled: true
                {{ end }}
                {{ if has "code" .observed.composite.resource.spec.grantTypes }}
                standardFlowEnabled: true
                {{ end }}
                {{ if has "device_code" .observed.composite.resource.spec.grantTypes }}
                oauth2DeviceAuthorizationGrantEnabled: true
                {{ end }}
                {{ if has "password" .observed.composite.resource.spec.grantTypes }}
                directAccessGrantsEnabled: true
                {{- end }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
        kind: GoTemplate
        source: Inline
      step: create-client
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-audience-mapper
            spec:
              forProvider:
                name: Set token audience
                protocol: openid-connect
                protocolMapper: oidc-audience-mapper
                config:
                  included.client.audience: "{{ .observed.composite.resource.spec.clientId }}"
                  id.token.claim: "false"
                  access.token.claim: "true"
                  introspection.token.claim: "true"
                  userinfo.token.claim: "false"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-audience-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-sub-mapper
            spec:
              forProvider:
                name: Username as sub claim
                protocol: openid-connect
                protocolMapper: oidc-usermodel-property-mapper
                config:
                  user.attribute: username
                  id.token.claim: "true"
                  access.token.claim: "true"
                  claim.name: sub
                  userinfo.token.claim: "true"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-usermodel-property-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            {{ if .observed.composite.resource.spec.defaultScopes }}
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientDefaultScopes
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-default-scopes
            spec:
              forProvider:
                {{ with .observed.composite.resource.spec.defaultScopes }}
                defaultScopes: {{ toYaml . | nindent 6 }}
                {{ end }}
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-client-default-scopes
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ range .observed.composite.resource.spec.serviceAccountRoles }}
            ---
            apiVersion: meta.gotemplating.fn.crossplane.io/v1alpha1
            kind: ExtraResources
            requirements:
              client:
                apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
                kind: Client
                matchName: {{ .client }}
              realm:
                apiVersion: realm.keycloak.crossplane.io/v1alpha1
                kind: Realm
                matchName: {{ .realm }}
            {{ end }}
            {{ if and (ne .observed.resources nil) (ne .extraResources nil) }}
            {{ range $i, $serviceAccountRole := .observed.composite.resource.spec.serviceAccountRoles }}
            {{ $client := (index (index $.extraResources "client").items $i).resource }}
            {{ $realm := (index (index $.extraResources "realm").items $i).resource }}
            ---
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientServiceAccountRole
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ $.observed.composite.resource.metadata.name }}-{{ .role }}
            spec:
              forProvider:
                clientId: {{ $client.status.atProvider.id }}
                realmId: {{ $realm.status.atProvider.id }}
                role: {{ $serviceAccountRole.role }}
                serviceAccountUserId: {{ ( index $.observed.resources $.observed.composite.resource.metadata.name ).resource.status.atProvider.serviceAccountUserId | default "null" }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-service-account-role
    - functionRef:
        name: function-auto-ready
      step: automatically-detect-ready-composed-resources
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.12.0
    helm.sh/chart: proxmox-csi-plugin-0.3.9
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: node
      app.kubernetes.io/instance: proxmox-csi-plugin
      app.kubernetes.io/name: proxmox-csi-plugin
  template:
    metadata:
      labels:
        app.kubernetes.io/component: node
        app.kubernetes.io/instance: proxmox-csi-plugin
        app.kubernetes.io/name: proxmox-csi-plugin
    spec:
      containers:
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --node-id=$(NODE_NAME)
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          image: ghcr.io/sergelogvinov/proxmox-csi-node:v0.12.0
          imagePullPolicy: IfNotPresent
          name: proxmox-csi-plugin-node
          resources: {}
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - CHOWN
                - DAC_OVERRIDE
              drop:
                - ALL
            privileged: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
            - mountPath: /var/lib/kubelet
              mountPropagation: Bidirectional
              name: kubelet
            - mountPath: /dev
              name: dev
            - mountPath: /sys
              name: sys
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.proxmox.sinextra.dev/csi.sock
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0
          imagePullPolicy: IfNotPresent
          name: csi-node-driver-registrar
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
            - mountPath: /registration
              name: registration
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
          image: registry.k8s.io/sig-storage/livenessprobe:v2.15.0
          imagePullPolicy: IfNotPresent
          name: liveness-probe
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
      enableServiceLinks: false
      priorityClassName: system-node-critical
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      serviceAccountName: proxmox-csi-plugin-node
      tolerations:
        - effect: NoSchedule
          key: node.kubernetes.io/unschedulable
          operator: Exists
        - effect: NoSchedule
          key: node.kubernetes.io/disk-pressure
          operator: Exists
      volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/csi.proxmox.sinextra.dev/
            type: DirectoryOrCreate
          name: socket
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: kubelet
        - hostPath:
            path: /dev
            type: Directory
          name: dev
        - hostPath:
            path: /sys
            type: Directory
          name: sys
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: amdgpu-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: amdgpu-dp-ds
  template:
    metadata:
      labels:
        name: amdgpu-dp-ds
    spec:
      containers:
        - image: rocm/k8s-device-plugin
          name: amdgpu-dp-cntr
          securityContext:
            capabilities:
              drop:
                - ALL
            privileged: true
          volumeMounts:
            - mountPath: /var/lib/kubelet/device-plugins
              name: dp
            - mountPath: /sys
              name: sys
      nodeSelector:
        kubernetes.io/arch: amd64
      priorityClassName: system-node-critical
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
      volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
          name: dp
        - hostPath:
            path: /sys
          name: sys
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-agent
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  template:
    metadata:
      annotations:
        cilium.io/cilium-configmap-checksum: f44a1b9b9aa3687038dfd8962782ff63b922b65c6e04f629a8dbb091db5e9f29
        container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
        container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      labels:
        app.kubernetes.io/name: cilium-agent
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
          command:
            - cilium-agent
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_CLUSTERMESH_CONFIG
              value: /var/lib/cilium/clustermesh/
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
          image: quay.io/cilium/cilium:v1.17.4@sha256:24a73fe795351cf3279ac8e84918633000b52a9654ff73a6b0d7223bcff4a67a
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                  - bash
                  - -c
                  - |
                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    # When running in AWS ENI mode, it's likely that 'aws-node' has
                    # had a chance to install SNAT iptables rules. These can result
                    # in dropped traffic, so we should attempt to remove them.
                    # We do it using a 'postStart' hook since this may need to run
                    # for nodes which might have already been init'ed but may still
                    # have dangling rules. This is safe because there are no
                    # dependencies on anything that is part of the startup script
                    # itself, and can be safely run multiple times per node (e.g. in
                    # case of a restart).
                    if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
                    then
                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
                    fi
                    echo 'Done!'
            preStop:
              exec:
                command:
                  - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
                - name: require-k8s-connectivity
                  value: "false"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - CHOWN
                - KILL
                - NET_ADMIN
                - NET_RAW
                - IPC_LOCK
                - SYS_ADMIN
                - SYS_RESOURCE
                - DAC_OVERRIDE
                - FOWNER
                - SETGID
                - SETUID
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /host/proc/sys/net
              name: host-proc-sys-net
            - mountPath: /host/proc/sys/kernel
              name: host-proc-sys-kernel
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
            - mountPath: /var/run/cilium/netns
              mountPropagation: HostToContainer
              name: cilium-netns
            - mountPath: /host/etc/cni/net.d
              name: etc-cni-netd
            - mountPath: /var/lib/cilium/clustermesh
              name: clustermesh-secrets
              readOnly: true
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /var/lib/cilium/tls/hubble
              name: hubble-tls
              readOnly: true
            - mountPath: /tmp
              name: tmp
      hostNetwork: true
      initContainers:
        - command:
            - cilium-dbg
            - build-config
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium:v1.17.4@sha256:24a73fe795351cf3279ac8e84918633000b52a9654ff73a6b0d7223bcff4a67a
          imagePullPolicy: IfNotPresent
          name: config
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp
              name: tmp
        - command:
            - sh
            - -ec
            - |
              cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
              nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
              rm /hostbin/cilium-sysctlfix
          env:
            - name: BIN_PATH
              value: /opt/cni/bin
          image: quay.io/cilium/cilium:v1.17.4@sha256:24a73fe795351cf3279ac8e84918633000b52a9654ff73a6b0d7223bcff4a67a
          imagePullPolicy: IfNotPresent
          name: apply-sysctl-overwrites
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - SYS_CHROOT
                - SYS_PTRACE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /hostproc
              name: hostproc
            - mountPath: /hostbin
              name: cni-path
        - args:
            - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
          command:
            - /bin/bash
            - -c
            - --
          image: quay.io/cilium/cilium:v1.17.4@sha256:24a73fe795351cf3279ac8e84918633000b52a9654ff73a6b0d7223bcff4a67a
          imagePullPolicy: IfNotPresent
          name: mount-bpf-fs
          securityContext:
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              mountPropagation: Bidirectional
              name: bpf-maps
        - command:
            - /init-container.sh
          env:
            - name: CILIUM_ALL_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-state
                  name: cilium-config
                  optional: true
            - name: CILIUM_BPF_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-bpf-state
                  name: cilium-config
                  optional: true
            - name: WRITE_CNI_CONF_WHEN_READY
              valueFrom:
                configMapKeyRef:
                  key: write-cni-conf-when-ready
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/cilium:v1.17.4@sha256:24a73fe795351cf3279ac8e84918633000b52a9654ff73a6b0d7223bcff4a67a
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_RESOURCE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              mountPropagation: HostToContainer
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
        - command:
            - /install-plugin.sh
          image: quay.io/cilium/cilium:v1.17.4@sha256:24a73fe795351cf3279ac8e84918633000b52a9654ff73a6b0d7223bcff4a67a
          imagePullPolicy: IfNotPresent
          name: install-cni-binaries
          resources:
            requests:
              cpu: 100m
              memory: 10Mi
          securityContext:
            capabilities:
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-path
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccountName: cilium
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - emptyDir: {}
          name: tmp
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /var/run/netns
            type: DirectoryOrCreate
          name: cilium-netns
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /sys/fs/cgroup
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - name: clustermesh-secrets
          projected:
            defaultMode: 256
            sources:
              - secret:
                  name: cilium-clustermesh
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: common-etcd-client.key
                    - key: tls.crt
                      path: common-etcd-client.crt
                    - key: ca.crt
                      path: common-etcd-client-ca.crt
                  name: clustermesh-apiserver-remote-cert
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: local-etcd-client.key
                    - key: tls.crt
                      path: local-etcd-client.crt
                    - key: ca.crt
                      path: local-etcd-client-ca.crt
                  name: clustermesh-apiserver-local-cert
                  optional: true
        - hostPath:
            path: /proc/sys/net
            type: Directory
          name: host-proc-sys-net
        - hostPath:
            path: /proc/sys/kernel
            type: Directory
          name: host-proc-sys-kernel
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: server.crt
                    - key: tls.key
                      path: server.key
                    - key: ca.crt
                      path: client-ca.crt
                  name: hubble-server-certs
                  optional: true
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium-envoy
    name: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium-envoy
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/cilium-envoy: unconfined
      labels:
        app.kubernetes.io/name: cilium-envoy
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium-envoy
        name: cilium-envoy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: cilium.io/no-schedule
                    operator: NotIn
                    values:
                      - "true"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium-envoy
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --
            - -c /var/run/cilium/envoy/bootstrap-config.json
            - --base-id 0
            - --log-level info
          command:
            - /usr/bin/cilium-envoy-starter
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium-envoy:v1.32.6-1746661844-0f602c28cb2aa57b29078195049fb257d5b5246c@sha256:a04218c6879007d60d96339a441c448565b6f86650358652da27582e0efbf182
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-envoy
          ports:
            - containerPort: 9964
              hostPort: 9964
              name: envoy-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /var/run/cilium/envoy/artifacts
              name: envoy-artifacts
              readOnly: true
            - mountPath: /var/run/cilium/envoy/
              name: envoy-config
              readOnly: true
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccountName: cilium-envoy
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - hostPath:
            path: /var/run/cilium/envoy/artifacts
            type: DirectoryOrCreate
          name: envoy-artifacts
        - configMap:
            defaultMode: 256
            items:
              - key: bootstrap-config.json
                path: bootstrap-config.json
            name: cilium-envoy-config
          name: envoy-config
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "1"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-startupapicheck
  namespace: cert-manager
spec:
  backoffLimit: 4
  template:
    metadata:
      labels:
        app: startupapicheck
        app.kubernetes.io/component: startupapicheck
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: startupapicheck
        app.kubernetes.io/version: v1.18.0
        helm.sh/chart: cert-manager-v1.18.0
    spec:
      containers:
        - args:
            - check
            - api
            - --wait=5m
            - -v
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-startupapicheck:v1.18.0
          imagePullPolicy: IfNotPresent
          name: cert-manager-startupapicheck
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-startupapicheck
  ttlSecondsAfterFinished: 60
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: cert-manager
spec:
  encryptedData:
    cloudflare-dns-api-token: AgAJ9ChXMAezXNfPI8Qhwn0OzJcKtw95ReqgVgLGFV8x3T4BeZtUTgEJGPBlUliwFJNGMCsQcnasTXe5lyPOiCRxFZk1t1ZrUY++K6ErzT1F74wN3nqQY/Sa1slTFidyAIvbWagOMIKgjPpnvXfqo+nl5IffReTSrahI3oDC6Z3AAAVyIvXhUquYeAuGwXds+CJOUWhZCogPxYQnJf5BZCBMLzg6HGtvOgAaOpNVEP7SG0N8yhIJRGv6k1Dc2mGqGMmOQf3Ok201iPq6NkZE+uC3aO9iQGilK54WKkY/pSaUna7YPZ3FjqHKLx7BgC1w2aWFOTVXHfGRUGwnP7mTOzkSpSVSSJahqGc2O2/Bw0R6QdIZm6NSLqKRGZ6tt9rKc5zxjYSk4AMvDE1v2KZ+2VuhPk+2090sHjYGi2cq7H2u0C4OJkIZU2qhxp8L75mw/dUhX2ZEp8CtnN/PQZATY4EOJB3S9jk3NbOV7tN/uHjhYNpFVTQOx0ZLkPpYtLA8GZibC+K+wqpdiPrrUKGsARoX2t7yEHhBjz/fWkyiV206rkCz9m55eTBAtJH2p7YKt2pdVKWHedJ8Lc5yXIpTpKFn9fbgEbrvgOTzia9V7iidizD1QDcLzLYM9Jf9uwj9+vldkGB6hn5Nx8sgdtR1p8JmbR1A24yIfJmSfAeDSsLIcutjNRkDKvK8TwwxbgZnzSNLWtAioxnDO24rPP58qLgVz8lNz6EVXVd5i5dU5I7BGFuyJlB9/9xb
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: cert-manager
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: coturn
spec:
  encryptedData:
    password: AgBqb41JE43pliaqXSNHlr73oVa2YULlDYLFmhnHWsFdAMGCAOrX/3KY1PJK1nSKILblkVlo4y4zYXk1sS1J/2+4uTUStd9zeqaUbaN145joLDKqIJy9UxUMh2G19idlJmhgZ4IXWYpUQIDKgXExZUmbqOmVrecPbGmTnSAAsR2HwW4N4RUW9CzH2ecbTUXuJ5jVzv1p3HhDRyN+A0nkm9TKZOQVTJm6QWNcLF6jk0mecTZZ7m5nVrCWlN2fToBWIaFEnLwB8jZMsJO0SSbHP6CLLtl+tefHVYZC+NkOP0/98kYzE8T2UoxOPlkKZW4m9hAGII3XzzkP6cy/9QA48p7sQne44IZ2QYLMHN9vJou3cn+uDQMY4SA+nYUcBTRzbWX2j2Gd8fzDFYMY5d7j4/XIEmKFI3HKfaMfvyPxKi6zSdFwrk0zGNvU+Km1hwix6rEK4n3UqxwizzdOM2rA5QFjXnH/JLidLCcxwZKWUyh1haiXZTWxqCoKj7W9xxQohNWwG7bV2Szobw4GP3utkmet5wKGe5ZN2buasre1cyMTa/VHr4FtPT23KGbIl5iaVtXO7VxisRwpCWTBJNA1EdbTibPtYVCWqXhM5vj0XOcM6qU3IlBgSGFXWL9JCLRFRiqCvR4OPGxT9VXxOI2AN04JSoYNEGFpOuV8HglANFrBwb54N0FeHgpZ1PNIXlV3Wp6x8IEyl3U7SXnB96O2jXpe
    username: AgCNW2mqiJHi1sqEj60miMBjc6b8XhzN4CVguWfch3mQQzOAY+zd2Mh57dHqmql7bht+oEOtBenxxfxKQQllY6HkyKXK7SWPPPSL0X20oheQEslNpGEYpkpaFBjyolO1Prrx2NM94oZxcf6rPeccXo0vApZ1E3YV1iIIWsdBDpoOZEA2avLb5jc6yIt8S9sbTGlzCf0F5UeVTM2WifnbRpr7WDU1GRgDID1v2dS8jQOtdu82YYD4ux/+I9fL4It+nyBREvfyUeV9TZphxG26xtQgPcXsMs6R+EwjOPbSxivep3ClQ+QbeLvMDl6LUdOeoh9Bx+MzJG+GWKgsLcpOCfRVh+nS8od8XKQRhGrPwOdyVbzGbStVhoIK3OomFyQt7qokaRMXkkVbmsb4vOx25Y2GYcdThB6ERadpzsVA2dcyoa1yvrYDn9AREBzct6gT8sprT89JTUNC7YdbaS0FTedvzHEvF5INpdj7yVoMgp5Zz1Rh5PEYYelpHcLg6Cm49Og4XkjdDVtrxvXr73HKonftioktFi/uIRg168yo+QRedTnqNa0vlY27tnrwuDFp/nxfN5X4QBLBmwhEAEJG4DfpZUiYRrCd4sStOWP61MdoROE7TaFzs9TU70mLEP4fcJL2zXC5LWeU3C+alvsp6c55lov8NtaBFox2z76ug6zTipTutWuenqLZNvpaDtEq4B/Y8oV+Yjqc
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: coturn
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-admin-user
  namespace: gitea
spec:
  encryptedData:
    password: AgCGBjrd5MQeC0yx5epmouyLnJ6+5cZkaCiyXlyS4r+KSgvh1bSrtL7yyTI3LXh5ix7scx1j9cpzyRaGHVexM9LNM9XWqC7y7GRNJb3opCoDnSGcZ8PwBuLMTg4sxwM+fIQDq5oci4YyR+Jz8RUnslRCSBVr8qIE6xz1tMlpl16o0ni+vsb9q1AjuO9TTIhnctU/IJ54P4NQI7UYHBV29S7dLiY6R5JwYm992EIqFls6GPEiDGKKzHbphZnznTmlPENmM3fnC2UQzvoUlJ/sP1rbN9HM1gT08GCVnj/3M/W3fUb7dDAE7KAFnFAZSNhhQW2+8+5ipwndQy+CkrV1d3s+TE4rFvsuce87Dm9UTRXMB4brYQb480adZWIJJLxKR5fXy/cWeWOw+mO78FiY3szxgu8pSLUWndCBfRAFoDpuVSoQJJfNyL/9KTJI+IcubLHl+xboF4HpWnfKNRLix2JdYJCs4AmsJNAOcYBGa/fdnOdg6YQ7YhDRuQqlNflgA6UM4lta1mUMFBPdP55qTsdCnpYba0Z1uAWLS4EERVNTgPl0aGLlmmvYqsi8vWpu8JpoNiMFKtk8if0+ldSAQ0Z8ozgSGK7jwz0XWNmbhQLeyf1297qIaenWiNo8GACZbvPHvVOZ9At+xq4jLpMoQA4obMv0WBXpiohaHYq7lLTkFMyN1xFa8ABuguvDR8NjisuuB9tgxTY0zMDAdeQ1qkLKcioao5nxeA4Q
    username: AgA4Du7ge2bBzpz+hHKV3E74rVdFAVoGNs7Qts6Jmr0r3E4dNobiqyroocqsfHK65fOWCfnzBdYN1vKeVml66XG7GalcF21JvnYll9KC0qDRGO5qUW67rtZjG6F6BBV5CJTP4+1+VpNOcQxXdBNnj3WV6M78MhL1oYX1IeB/1tRJjDRgKcRJUc5vArSQAUHoykXq9zNwsib0G+J4EL7MaTfNEZgjpF5pgdb63f2TXiELEufCNzYisF5KyYsEa96NexnCKRowBWQ6TtiQunq4RoYZnxNBNvL03mYhEDpGH5DU/SNWdDRKQf9byaaKaGSgxKKks7a+vUBwYJDoWUd01tQOVN/uaEQwSJ/BazLoQjo244wxhu6yDWuZLfwc2VqlFsG53urT2uPwTv7sNhW5L2PUIWKgOaRhMAlaA9MWgwTfQYvvAOBmlV2GsgwY2IxRGiO0uSNeeAWS5cIhXuc+aZ8vNwj5CZR98WTtZ0E6mmyDmUpXZcakDE8W5enuYR6iejN555jlxK3U4/HvMKe9ywNXzG9LxqzLF40Tm55hH0nJHAFsin9aElfAmpeRY9UcmwIoNWfIHTM2RwFCm//S5GxYLt9uNwYPlfRg/DsREYy2qPNeXth05TbM1IND5ho0Qb6WLB3f0MM39N9YdPsv+nCDPr+Xs8uF/E0tHgowj0LdDgfaPlX+eLOQReiChorjhs3lybBN
  template:
    metadata:
      name: gitea-admin-user
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-oidc-credentials
  namespace: gitea
spec:
  encryptedData:
    key: AgA+nsX7ujf+uzZ2N653HXfGosxZT7zuvHTW/8aZAZ6Ll/wtswLCHwx6G3Sr2u6m8L6AaxhSoUE/RO9NEhQs+xCcTc9J5ODI1lsvd0Qiupw7+EZMn+4EBsT/LjypXfpDvhM9DTbyX7hbJxR04ncJW9q518CHqoT5j4pkm5PLglbmocb85j3QUpBNOaNFAe1SnhwrIoleX1q9BjdeOGP5cj8e4zbaMHTUft3NQyJyi0ghZ2CASEngTQWIVNHbElm04F6xOhnL69YY1Gh+2pO8BJLKRvCE2JA7FgBcRBwvTkGt6q11tbtEaKEoFiMqdEQKdb9AnZa8XErvBljIaZmtIRTsmoU6DZTgBXaFDaEcOKvM/NuSdLUz2j1gKmWZSJ6gMnI9blUxdaV8KHozygbBF0zj2d3eqXpmtI/atmXfbvRduuLLo1vhmaQ++P0vcHZyFlAIQTLJdCGwIt5hdK0SL6uChtYDqhYV+KebuFBmI1r1PKDaurqz+KViSIfK+kt152Mf/LVBq63foszybrDIpNyk2nqlYHJnCAeZ9HtvuZ0Zz5CtAl3KIeWFg1HlynTpNn1GJO/A8wdZRoAOr1JbX5REaoU18HS9n3X6Rrujm5SwydU26AbFT1rrJAVYAjUoufNV3e1HTulcwDjUdAAowvMwciKvpdhh48H2XlFOW3+MZOWBSo8ErVOa8/t9sSowzdqJd3Q+Mw==
    secret: AgB4vQvD4KBUOIkPCBL5+Fgb+jwbt2jvNcucCP0UFZrX3jN40bQxOqcElG1Z/6LZhoq2dh+amkpGIh3R/YKJLNeUsE1TIL9W3ThriO+Ng9AH/K6omywljIldSHsPRARjkAiuUKMlx1mEs7TRw1/YACmXKNw77KL5RO4NLtU7VgxyKcLe3JXIAexbNK6W3X8TkDqu9wx3eNduUet4Jt8jEbaMfINwexE0cH7NRVyQbLDT7i7MxJD1PisQWh6XH1ZGkLkxSxh+wYSHwIHSoMgQT3r4vxJ1rmvDCSEPMEBvB/3tLYYTGWAImffRIdnYHxWD/PDAaSqHqSMGeXtFnduv5SOYK4eSayz1p73IwcTvQc8ePUxgwS8uahN7UYRaoWem1Vs/uw63XBetm5PQX+wh/yvnK1NOdFAFIRn7Oh/iQhoy6kDIdzEndR9WZS0HW8IrMj8oFPTgJh1lRtWyXuhLiYiwsLcmMg6U2sPSJVRcnvxqdG+DqKjWL9WQjaP0EWZe3luole2CMfTkIBcPPJxMd9HJCgCcEdHl9Z5WZ3NPrOGb0DETOnTnn9hJPPlts3QJfqvyygNFqIUkSWE2zrDA4GWF2sMBE8bh8yQYGBD0COwxjvyWvfO0WlZ/63sjTwqeiV229MUGVNquD7BKvShGT71b8UFGfZG8rx5h1WiVnexCJ4Rth/u2eRaxptyD6NO0A43RAMoMJiDatL7ml/LMxfLyhDJvG368rYM=
  template:
    metadata:
      name: gitea-oidc-credentials
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  encryptedData:
    password: AgCG5I1alY74KUfK2r0gwZxaTrLD2py2Q7vBPPW+4LQg8KA+lTjYUEhN5BZLHzFRUtHW+KQVqqgxRlKOAsRZn55tgzzqxgmNxlipIuX1fGyb5Ds45azqT45nWfJlzf50r0WByMSjsdvVshkNjFjCXlPfwr2r73x+ZWqRv75Ra+6YsPHGWvkzglMQ8G04T+s2crb2PiqtOeSGsflJazLG5VAf8tddChGKsptPwate0UogTctFGHwhjMsTp/J2ivTqOh+VqPSknNuyJQfzYWHTrEHhNR4dNROy1lxFjA2/bbpUoezx4OHEmMB2iKmhVFycLPcxKG0IOykINLO11chia/sStL/3FQSJw0v99YqmgOl4g6HmkCiUSMQyULm2hdAsfbkgt6mGbSTqFkXS+uKClW5WKU0tzowXWQXZ4tCSC4ODZGDRuWbAsipClQLSknd9plh5FEzsrXu5Gujg+ALFDFef6lv6ukd6e9tOlAB68mMklXUVuas3a7JKSeZgYf4+JucmDyn1pDNpyT7KnOzP2GrQ77vTkfgay3QOZ4zh1CMw+5oxrdRbML0JcDvOTBraOKAu+nVwZCWK5nLmS81qj4fWRvqdiklw457cu9m9DZYJIgcp2OGKddyvyH/O8p5JqkmsRowd6LcmUt9F+/9ibnO3z/L9X1+pQkIfDnyBWdoGgIO1MSQzzsoqDdd+MDOxdZZTV9yYkQ==
    postgres-password: AgCfEmOIW7DTsRFmEUsqOAd1+ASfVPxQfZyIcrq1q9LVXX/FWY6dBTmLBpVRPLF1h3acLoUqqIPLGOK0wPgCbr9waEvM9HKsfhFqzTGvPIVid2r2tuga4u0pi0U6JqidcHROpyQCwE2NJrFiY2r3aGo5wstybYzHGDmTIAU8FfCTf38GzqgdneR3Gike9L9IoUmvEXzGu9edcQYinb5+yJ/wWp2+XA9936HmY8vTsjKlv2VWBiu5ZbWO3YRNYOvKesONDgvGh/3I9yagEqajjlpSEIAffDsXRnf16ihqGaxWk6KUD89yOtuiLo5BR9Z2KQmJ2jHB5T/666dZLMUGRgZBxf8G5wR64hSOaAoXI4HZ6xYYsmamGRrVExphzUNrkNu2wHmUwTW3LT8pXG1IyUAd0MZS8ZUlS3RdB6cyczYvhCuln2ER1iLXSPLjKy8HVhvDBwp8deGdvtgXK6USziDbYAaKvosYe5jZynLuEddCFJ55i+Kfk6hPotnnlltlIA5XA9ncK20L6r+tj6Y6CLYR+eShMAi4TS3d3vmmwcWdaqUevh6v+klpCCSJ8JyIP+YTBBl+0fWVNqQDqtAXHzFeU3IRl2MYCZ1pbaLQQgsORabT3tbYbZ1Ab0re56BnIDykw8zwylr2sOOq68laEYqmiXBKEc/9OZZNUFZx2BHE8cx4ewiTkXiCeH917XZRnC60Nlo2zn85v72N
  template:
    metadata:
      name: gitea-postgresql
      namespace: gitea
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-config
  namespace: immich
spec:
  encryptedData:
    immich-config.yaml: AgAA1uORMAGoH8uX44bzeylc/bxC39WkaKTzVBAsHzEem2iW+FzmGv5amYEBlgWnX8jjSGK4SS22g/n8DcL7GFsrgnrZNlJPUXzJQUD9w2DjjpVaIFrQHvjrnLgFLu9+od78RWXAWds85oiRX6LYCiqVc54Nyc5zbnQ8DHlvw+8VwfpitqWZA38cclJ+sDbB9hCLBPSEDsBlC37p8bQqjPoP2m68QhEpg4Kxrv40k7WxCvrNdNUCAiOh9hRdHvbCqEtleL0dk3aWJ3kGSf6p0rsVPl+cyxlfsFrFg74lN6gJKfgTV4j3mzv296wb1btDVN36JaOTpcdS4aVK10Kg53idBG4vX73eQZFRaPw7ZGi6u2A/c9I13etOMxrCX0y2Vxu2ZnAtnFdkAdlcsjhiX2LdjVYGzmT+ZqOlGp7lJd+gKX9CXhfrJlw3/2lQtRpqqWihIrrdkthm/4mAybayREV+Q/ojEXMmk20XkI6OLsP/ljZjMNA7vCB3tyLnbRfqHacpKbnVp/b1Jth6TOYZnvLhne5i0JmM6RCUiZXitJnxDzlQskFAUKoyBHorS8vYD8lYHqMv4iO2E4Qn38vHnSAaeQOH13Fe5tMJ+U/VPQ4ybawWCapUil9iuGTKwMQ4xbkpyN8z1KyP3y0RZf2w9NXoFeBZR/iSgGFUataEbpXWipGb66ieV0CAqqYGvLHFfk2kTbN9HMRKTcsClCaLOCFQKiMQLXNtxLsIsxPxrb1l1kI1xnW1H87upx1Y3SEzIHZdQgIeVmkVUFaVrFw5/WjqrdVZsxBxGkqvD6lKFY3X1srzcxh1A7LasSuoOJNVxVTFBr4LsQ7tNnT/rHaMZ4YSLwRJUQc6hnNK+4B4GaTX0190B3rion3vM3XsEqU8U3hpVNZupFnJe1Lwsp3Dom9nQga30WJjcD5SDGZQUtuPEjL5//A7uTMGrx4/B3ub2VAc+8LulG7dIAGSO7Ez0drVYdqcGoNQ+EPmY0WxO/CbYoD06QWbqQDgBD/f1ftERTwBGW0IQe6MEWqeTcgFdg==
  template:
    metadata:
      name: immich-config
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-oidc-credentials
  namespace: immich
spec:
  encryptedData:
    clientId: AgAvvCAj4sOLQgLcYpkSUKYld6dWCwEsTxtJ0BRjSOdKU29lh7l7XMkLcqiQJKV36PL/he0YKD1MW/jWcD5oys7ZHtaoDKAfCmTMR8eWGU3UcY+0w0ELn+G6TL0EkgKY6Gmc0JOyguwJmsilD8iyW021NknF+hj/pIak5iLxHOidPbRmNCWqS0b+iM0WLfLs+jrstLOYfs/VA/dftqZMogN5JLQ59edb9DU7Z5DRpZbaqmE7kgLuRR3t8Iiz300S2Tvhx8E2/2gKLQbyVMmZ60KjdtNln6HDhIjwxHsas3iWUXlgh1V2sIh9hDOBS4yWPoGDRYEAUdBJfGLFUYi3K2cQOC2oAAYaU0WTTDRGzQMugPDbKONO+fDVjKT0UJpiBwJ5aoNW244f7qUNDbeGJBM4iyQYit24fsHyL+4jbXjaoN/mjfZuN57h+QV6K5sH+bJOSl0OyH8gT+UNYsIZ/Er1w+AsNMdxB6Laf7rPnxlVsyy6bDLp8mRE8lPjz8f9EPGZIHTkcLTgNdsbDowG2RfvEft+UkRVuk0g8++VxQs9T9jzUgU1CsIESW0mDwl/K64rJm4y6G6wAoqCB4RyS0XHQ0+e0dqpponRKpokqzxZPgfnlay1Hc+L2DnAlD3kYsvAnQdECQMaQ93xh2/18Ym5BSQCBDB5RnElbgJBj0uE3eOJ4CMMftts3b+rFtVi+SOvpkXciAg=
    clientSecret: AgBj7B514brIE0r4g9UmqIo8gnBAHA1G/e/reQH3ATOUz2gAMGmC6VVXhfLam4Nwq3U3s+SwcU+jvVYAu8mmf/2ltUIxvBIfFnk5olM8QlE7eTVQAOdKLbhBmIWN5WhTgP4kIXawny9i1h8ft7anYVpI+PJoAQ0X8OiRbybsb+O8wWDFaJQ/lEwFAww3yNtiltCf3b7wJHI8pjvjJrtHeJRb13+T/sisSSh85QdQdbeSXBFGAo3waPHcbtaTVAmMs74ZTpDw56o9rP8hPtybSN67IcBa4lt/p3Z5OsVQgHa8Q1mHdzb/8wLDY+39UbHIZtxhH46MYDs7BL7G6ddTZUViXYBZRFKBdcFSs5tT2jeWR021stpKosRSgrEv3P5d3QDMYIdsK6FZw+lMAzaT7glqxiYLa6Ot0lYLU/fF1iDJ+76J4u1Py+BqHHGQV4qE+q4WTCXnAHlKzLtoV2MiaZN7NaPSfVl751BJFl1lgXbAievsgGPU0rYDn2q+ZS89QYi3yPqhN/3ICTbOE/iMaHhrEKkBBRmz6rllLqrLcTwJo5AWJAUqSyPXY6Jbm3akT/MJJ06Rj3IulZPN2At2ps3tz7+DVSES/MgP6GmpY9ZpIbiqR28rnNjzqRK3gZ0QNvJrScoeAy+uQTFXjRyRuW/8yNPnkYsMpqDe9cmB88lbV1XJzjtdtKKX4e26nWlJZqaT/S7AeoFdDDwLuRSHs2oZYVvFnosLRG8=
  template:
    metadata:
      name: immich-oidc-credentials
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: immich-postgresql
  namespace: immich
spec:
  encryptedData:
    password: AgBg5CBTb0W5tXw8ibklTQP/1D/NEsliDeV/pHL1Pk39RFkUg3OB0pQbSe7ZVqcGHxyLaaTDr7o8NpDsAZnsJOH5vzpGBrddtkPJNwrSFxd2GAzGTI2SQH/hoQQfp27UpJzEqXiLIHw1rFTOTrJJayeWgwDHf26zc0oIBpbbQlu0o+26Vca7sArnGixX4iU5SS0HF4S2Hhk0CkL/dW18k/xrey+GlFdAtYcww2i5Cd6lxRTC5CIX3dQSqBWWcRy3Q4wOWH3zrQpkt96bSIlW9YrDD57dX7NAwCwrCx8pI6MdNAJEZpdjF7T6WAX+t5L1YzF7ecbtGvw4l3IavFKnvJZn1CCS95cI8xFrTw+igozYwMJB+oxre7E+q4vk20LTYuv+hSCeTIzX8t/5u0XqHW3/6XrqMNVOt8ZA0LIpABqqIgR/vEDsQPWwuehPwNvOhTJNKSBQRQi/SGtHUTkYCQwfLSPRpUV/MK++MaQin1nVy/Ch9dexVuWsR/cdffTwagzs4MPO1DDgfYNZe8sY9hPp3XUDjx03ayCT1rUA8Ymqajl2rRseCMuzdQnM33pizqOKXJys2DHWthr2xi0DCYd96dky/L92eHOC/tSQlyiYYjZGOUIMmHMdJpxkHUIXHJ/F1iTIyY4UMJMP3GkavggEk8WIImOebijYSuK1U1QJ/dlPuE4Q30YbhA7Ns5vj4Mu3IsAmAfg=
    postgres-password: AgAJAAmqRjNDaheSbzBtB39VjcDeSGEFyFnSmw7a3BlBBVqSuh2PXs01AVo483Qhy5VJQCHt/pkbnyw3bNn25OPmYZVdeoiwl6gZFwquY7fxpNANhvado6xeC0PFNhHAqX0M/U9UL5xPIfMRNc6JztvQaFosJD56EPiC3Mx76LrKc7hIDN8JxXDoba8NaUfQmvPeqX0bXucbDgDFv5yJmHWv4xtuTO6a/xwnt5uYn0nmC0ut07ntFeQSA4RqT2ptauwMy5SwM9JaR6fL8RUVQKX24Imk8s70XeVXE2mwdXu3w0fVx3PKlHcL2OwLumUsxhgW68ypeoHASR+B59mnJgMajYEQY+OwXwaaQNe1VAZyvfgwL4KUv7DvnwPgcUJNFzOowqmtfYzoydwKgtRboWlRvTR2sfWjtxgOXPxFLd/fKMtv1+CVbrRCivOGsBk7bG5k2QTraTuGCq3MlwMRm0vdhFVnOqZ79/Y9H6GZiFwxN+7W8w9wRKqRb7FmfLGgUbsf5UVqUpAqbHy82Uq3SvUwQ+f+rxBFN8RJVP2zDFiM8WBsywMU4Rvcl9SFcnsLTunaHalPMJw+AiflsT5X6hvrTo1S8xYujq2U2TltFTVK5SKRBrkmNVm+MWef1TmClN/2tFTqm0qy3qNrPZXdNzUVBpIyo7gB0EPuOJ6YXKKWI42LLavb12DdmDEsrK5VRTWcK0/RiMCH5e8G
  template:
    metadata:
      creationTimestamp: null
      name: immich-postgresql
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: immich-postgresql-user
  namespace: immich
spec:
  encryptedData:
    DB_DATABASE_NAME: AgAQgjOnZOKDyXwbFsgeZg1okBja90dNsjewAwfUY9XsdKTOPsDlzBjVOz5UmtyZ8kTzkCY6SDsBVmOxsxfcnvXlBjjBW2n/XmOtm7ftKGaEH9g48Lh9BCOfKey9dWrk04JhVwXwdM3lQtwQxlTwHQ1Ie5J72Ht4bK/ti78FbriynvjNwBFNwv6zyBuJY2Zb9EYyGMkIOxawPLSSkYIvzjVSfy8aRHZ8tZdqrL9PjBSn7q2T+EGC+AfhRGkntTySVmgSjfiQfdfH9goum08m41tNfM2JShBPfzMaBI1AQEHdN3EcPkLIE5frQSJ8Pak7am/w4LN7kB9IJQzMF1UrCe5CR5waOBBEczuh5Hlw5krPgCe7u4epV9h3BOk9h1mT6kyMBBNKFnRPvbgMdEG+/Il2oYv7q+Ql5tPIVIYrwdSlPDVAYSQOasQ+oL9/Y+ESRePr3CzlAHy+dCkBflWKW6nAq16VH0XLzElfqLuBZN8ag2kI5waXhO+12gPv3MbRxwL3N51LS4xuKQ7RsRsaxh3if/wSAaLzJEekagTBYSPPUv7CtbZR6ixLFwSRmpaC2IeXd2hXAlanjof76fO1vEnebQ8k8UVBdgEjwTC2STLEAYvICLcwIgTuPWxU/Rpra6tAGAa/PKF70UIKkP+lGrcc0/LsVcWLakCkL6MV6kAeNTYKMRBwkNsn0ra8T6MKS/wMYn4Mcgg=
    DB_PASSWORD: AgBXzZBsinnsjx/rMLcfGlthCWVqQT8QejjZewRPGyeX6/ksQhuNTt+YDKDQc/0wvJuOwQ35/dWJDIFTZRVyVlKFYQKtCzQuOWdDmwCarbgj4d+OH33d/J7iyJ68/jqRdT389mEnOVh2cCclym13yHD2N3ifd7MWt6TGOHEC6DE3GyNm9bUMyOcOtyBCYn95xBEeUwCPtLzZB+3POrlCaZZxKBVOtR1zkA9dt4naVE5xwtLXK51SkbhRAnurNCzVziCCUwgqdMikXWd7T5GQN8hGXqAlgnENPUqNrxeTd9K2yhLgeEdVcg36m3kG/S3+HzGMxBeNpXOd8XkOCOP1Jm0Yr8X6qfF0MTAmnmru/kaxAhvLMIJkHP8DwxE4q2taal54PgOuAX6uk1Va2zaFk95s/1wbvLsa5b+GZHdagaZSADT1uBJ57N59PZ3kfv4s7Pe3EOSfsuzSTsEsF6I+QPhrKV0uBVviewRcNdpGRHjAe+DAACxJvr2se/Qyzby5mJnMgawe6LHXyqVd+c4qcZ48/6hucQiXhOAhBSHG6pTLnDc/WLmatB3w+Pz9GEb+Rr7zQZRHG5ZxeFm7o9Z3eHPu3ywk2L0ziFXg1NJ7uNrYQXTmIK3AdXThgI0nYN2pn88iPYrMHATcpv9vqF1+HWb5NquaeD8V4YCcD4l0Rj11GLG4Lq0ZUWDFSrRo5/79v7XrC37UKEiT/gP6b/Mqn2ENAEfS6v8K
    DB_USERNAME: AgB1+8+LCEYsybi0bHdZEA0cSqzuuj0dqDLzxbVoNSZarVmrpb5IseK+4iUCLuB6K1dHI7qXMUjpKiZgM2oNQnJySNh/Epyz658orYaxM6EZz2GNIqQTwo6DIiIM8V7l2++N4fz2M/j2arBm4oHXeTJ0NJmh2KtEQ27y+kL+m9qvXoh2gGzY/LCYaqE7EvTtp284BiKC7TAJEfMO9qgQbvjXRouZXNW0p9XMksyZp7M5feZ/LXxXjkd10QIWFXrnj+nWtw713kZD6FBuvmPDITQZ70J35AeXp7Sdfz07rXHOmMWY0S6IHOxQ56k60+umP5ng+kiLciRKZoef43yTtm3wsG+X3n9ZWadZxBimwKgQkq7xLJAwDWgASg0J5bwkICmcGCU2cxv1J9RqT5y+6ItkQiZgk1BZIcBvhwt1whzUI3qGBetA+wDqTgSJMSpJgQZ7wV49SSG+Dv0gjDzLsDq6ntmkpYWjNloUNM2pc8toQW0H8yRY5ursvJnzfRlhHDlvx8t9YYEyazHDmI3cYSKmPcoJuL3OczhhO4qiPXgomgYzPrsbTdpEgsrQEArG9vaJDqH0LnKK73YxlKvlyg3kZUV1XUF0hivynQPs4MEYee5JWgsGiaVIAH12Gz1cDVJ3nH2YxhsqvlcAcXv2HBrsZU81Fgl29rwxSWjFBmITwcd+DC38fvbu9Fp/D6s4FAO+7Ig85/8=
    password: AgAGkMOzFeqF/aRBpZUDoq/6BNWr4tkf+MuFPQHCmsn9hvcOkCTksEK7p02C0Ru+7iLrtHF2KK+49WHjo4z3/QCAAnS2BmG2iZXkugPfp/8znMOgTQTg3VT5pw/YanCNcr50BB+a+3MFNXjMSpyNjitcWd+y9SzhCWsIFDYxRhMw5xrK6G+6cQUtdYzPXBlnxghPZBWBg5wiD57aA4SaSjz9LssiPbp1QtH6fW9JPkoNKpQkK+/3i5Ko+xMWigyDpcY1Qd74o67vRVGCQoHO8vxZ8A2lRHWYjW80xAHXUUzXNZUEefUfGQ7XJ8aTqZ21OBtSbzgn4gJFDFGRXpcqLcl9cTg7P1C0pcfIgclGT1kkDX+d+kPaqhxW9Ee1u0aiA69ynof/rGu0YnbzVLXNZlp0S5iVZb2mUPyLALwXmAPXuh5lsfHJ45t9TTiTKxb5ZAggXObHdl48bcM5qn3Ypyah2uU1/K4zdUd5rFdZkqyVTevEOYnGMD3hjasX++zCs5mT+SWY3/6x8bHePf0HUkfMdJKbFFkN97iIRAqP1PkOzJiZMcclssyiQkTnS2BMbUf/owMYzzezGeF5ZIT6X1n5i5QAtNvUQSaPqxVVDNgAsCccJKpUjC+Hx3t9M2kolHhkmDJCkV+2Plq3IYkkBzdo5ubpv5TCVpxXF9S5hqmvSqb0LzNAfTzgar/RShjsjctycfSBid5kvQEWRnQq5NhIlY7wkR3o
    username: AgASpZ4/NI10U0e3pa2zB2vfxXyA/aa8pSYyNwCSLcM9Kp8Ag+UxQXC8m6bcm9sWh4nEb0oM7IX8GPUOxSyf5bf31RNMAq4LagOY5jf9WBEuoYUpdoe2m9lKRBXkFXyd1Ip54VLjkWXGLjGV/buDPAm3ebUE6rBHQr5aTGbqe1eQ8WLkFc44rk7jjxLnejAuGiV2SjLXHCocvE9jr2KbQDJbILw0tVlpB9jabP/UVJRSPGrV+yTdvdaKD+PJXnly5uHFk+PPtwn1Z1xm6am4jujWoAgb+FBjcaZdmFECtGdYoLVFPQW33EZQJnM06E+QVU4ssHULvX4s8muNamWcBGnuWxhCflWGvtQZQy6ZF9sWVsn81majrUGfzWaUCQJy8IFvetzhmQsFy8zJFoWHS/bcjo7OrJ/IYxie7Wsy4eq4ph5T9Mvy19TTdyGQUAKAGSI7AbpGK+hKb7TGozklR5KpCViKa9NYm16JJGaLdGjtet6kPF5c99+yy0JXUX1F4ufsuaTOYpO1PYMlyqMKdpZyamNuWJa3Bsr32qguJNtsCU+OVyoR74NgUHteEiIvFuWMdFPXKCsrUwcr89jwW+md/UfjzJXpGwie25nReVMwEeAzmn5obBPYyc2VSCUeXnidJEh902ajeXXwpHb36KjPI9Ak1F1b84GHePUKo75juoZG1PdTYFYidkm4v6ROgvqz7jMB6Dk=
  template:
    metadata:
      creationTimestamp: null
      name: immich-postgresql-user
      namespace: immich
    type: kubernetes.io/basic-auth
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: crossplane-keycloak-credentials
  namespace: keycloak
spec:
  encryptedData:
    credentials: AgBTjhI8iY1NIToAtBAbRe9qVikwtCVKzH52DYazQICVjjHOBOmG+XHQ0XSnopit6le+ayC9ls2pp3zRMXacMyd6a9x5qmFcO9N0hTxPa9F9O2tuwAjUBaWWegM+WgMu2DXa/9nYdKi0Kz1HSDy0gnB9Wtlmd4Xjbr/tYpDRVTRKizCgs1LLNr7/o6G/ZV0zrJH0Hxt85cZNPjSIoODu4v69ea/Xq8ioqSC6GvfThFQdwx1CZi/TS1OAHarpSWl+i3RfPEls46iLNhHi3t8tD1YumSCENqpFtQ3B1jMc5kAE4kGtsvA9RdrGp8oDLIurmWOzL7HeYSJILrB6cVo7L+Y9wnI+ovCeyqbHeGIjBnYUUYTW7kvubbhBzzdhwDDVsUk5/3TYV8eQoccvYWPfmOhzIGEmBH7jOdD7TfCvbx1TMv2cuMjTGoE2A0WBE+hHY/OgtlWN+Mgvld83KvMLW8cNp2Q4ZhY+syo5okgkAUUqUbsw17bGpBnGObsipdB2j/PqqGTHlNcqskK46TVsu8MPV2MwWcZ/UfDbiEyA9GVYFB8aZXtoNwhXsRLNVS874K3HbtiMoNNWYaZ3ddairQl8uomhEm/H/33IiRJwElFgKfQvQOr5ak6yoBC75NoFetz4fZLr0XgWgB2RvIckb3+WxjrCmc5IdrlrL221otu70T1bZcvZN1vaDA9V2AseY7/WZJuGDqHpfeARkdXzDJd99LPW1Ww6eGBru1BQQWi0obilE+dVEueJ9neebkmZX07OTU1dOQRVvHFQb7k0U39s646mseV5g62d5J+2dnxE9bwoYdy9XY2Hu/oCqPW3ezgyENy0rjwhJo3ua36P2BSkDGI694N7EEb2GjuZSZKlDCWaAmfEGhj4Fw9z
  template:
    metadata:
      labels:
        type: provider-credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-admin-password
  namespace: keycloak
spec:
  encryptedData:
    pazzword: AgBgtRvU9pbVKgL7wZQnogcTXA+I8XO3BqAzialWSeeX3YpYCBbhNELkTe4BcugoYfL+keC/0D2VqiECisYL4arvUkOmHXKBVUsl97rzSKcS8zSomy4OMx3lQWI6xBuB4vwLzDN6qcl+LfnoIJlzcycpgQhdWPPcEJ3utzp/dhO9LzocCW2xHREgaySYSkuaTBdq/Tj2HZKuaJ7hVNgbO00zjHZd7qAs0OVFoZpJlZyMYdZU0S1r0gL+XaP3v2ARZ3XQEDmmxufLOPWcxYOeJ712q8wFZn5sCJdns31sFCFYI5mN/d6iBvcEfuWLnCz1ET0DC6HgedLID1jB3sCjYnQdz92V2fkDyj6uzqb7X9CTKwn6JvY8Egd40KVbamsLuoiT2aNpFw23gkK3Ajus3H5S9aL3k8zCSgBrHBKkO2TZDER6Fzw39cLBSnRBWzPS7wy10JIfRNPKV9rmGlHWY6y+H4KWkPiIlUOqDGjlfaLPXyD9o0DcTs3rTPF/lScqPtqSlKJCEDwWqudC8UKLn5YSPfECdOnqNCDyj586h+FZk0IxU4MSG7HbLHi+B3/Aev3qibBeMIYSsUYGn+B61YY0INm+AEV+M2Co98WU46S7+zIHpxSK6LxV+ov6MIdyo1tGC4PnFd1bIZ2ZxjlUnwMfE8tFSyWO+963ECVcPx0rXbvEDnzKvPsW4f25ttk0zzjX2WLxOfs6fcOKnoeGnG+U
  template:
    metadata:
      name: keycloak-admin-password
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-db-credentials
  namespace: keycloak
spec:
  encryptedData:
    password: AgA+ujL86YIs5ecAexMZ/CgsEBN5VMSYoXV2C0+OKEbmlaNwqBDzlTbFiCMBLe+RIaLTbJ5Lj14HFrGWqx4SdKpIqwJwxtzYJqnUzsQmgHQrAg17OJHgZbMdtI01F13/LrPjaLi0dmygPzO6xw1/bw8RHd1GiAeVorUmQh0Z7zfw33x3YaSb/KC4lCyn+INYOrPG+Gl7WtwKe0g7lLQbkwIl89A37SGHABxErlmuecev6j8CDNQ/WGugHrz/WJK6Z4bNIItMSi0YUeSL60LniEF9Vdx8FrEZ/VMMvyzHz9Sy8X1ygcTKnQi2woKi8QIXi0t1yRAj9HNHmtojh6iDvFtQVldX+OGmCb52iwaS1gybmzPXEs+++vRG1btosXh2t2PGs0SeCvZul+ZVufOfB/i9pRuu6AvVOAqxGz2Z1bRinZfkbrYlweUv5GwZKFTb+xRBx5Umwv1jJ/zngCEEZ+xdAk5iC6IVhqfclW8qcESagEPgIK0ZPwD7KD2iLi9dy3Ob0nd8+jvlnIr1BmwoPc5pf+mbt50OpGAdMlBGfPaYRFFM95pFs3UEX/E0gM/RRWDzgz8c60fyvnV9ACeRLnDqQhMUp3G8z7P6ZndnvVRw9R1n0mXR7yUabUtju63/tHQpY9muJ0m7occD93W+18mUVDPoiBC/sQzppYtqBNo/CJ96L2D0wky/raUNKQQVaiYw/izlXIquW6kn
    postgres-password: AgChWk610AN5uWVgF6/DlIZZ/GJSFfUZi8s0VPluyo6/iT8yKahc1wWW2hnQ2CIfHAlziq/ksLWElPAgpFYSaQAcOLi/uOEi8K4tR994MAMYMX8zh46ZOm4e7da8E8kS7A96aez2cs5J19VPoGbEwteCqB+sZFby+4qLeRPTL6Fk0q8WppnhZDstYwvMYL2gpTeIDqQlwyg360QTHoFa7BAntdpJcO3eA11tTubhV16ImwV348LYo4x1o5fesgazepAope2KhujKuPjBjqEDfMObJUvTTQuXQswmVLG7N4eeBilo/CEAWt7JS4wDODQM1RxLjiySDqJJBjwKxISORX19Np1iPINmTuJddCiS+i4xo6qp25SbExx9AT94TW6bSZrAruJsPPrm30tc90/nGBRnM/kD0CtfOayudmrkli3NMm4fAJ+qWaNTaU4WAiy0jimO5BrPzK+OlCVR7VRh2r6nr83X4BHAYjY7cAiCNF9ptl4G3EKvYnMawM5I3pGCbyr+0XbihEXurphFso5ElvmhzDonv/rzLF+ta/frpYdN2o2I7UUamh+is6u3iYFl/Zh+k8eZuJxZkdY/h2pha5ZL9tjzniGFQxYbWqMZN5suZDBo/YtItHlhx8Jr1o3m7lwpW299P11DFoo8rPHPUfOxTet9iqCDHTTBGx8ymLqG1fnT91WbJP+JIFHY8O9+HnrY2+Wg5aGpDkg9
    username: AgAPDXqoidAyFnMjYy6AHu9w/IS/IyQF7EjGtb3kFaq86nTLvtdSPBgS2V1cEi/Ko8MI8e0B0TA6LfXfvFm85yTDzazGVCRWVDLWBdrt5FEKqg/gUbBZXvC/AcBCxMZxE+cXXLuICz5ogW0Z+nDhgo343IUTG9QgWKXVPVpp2IPqI7ka0IhwbBo9/Q7kd2UwkMDxzNIqgArr/l25fZ59YsjvASI9qFh6rsnSu4idsLGRFJwo5oKJMUVM1K4BaFz5fllBKijTvueLP1FyOMzYjA3DUdu4fr6dmqbZTZu8pzDr4AJdNsYTN54vaZj6+46DHPH29iSYHYHRMGvk4KcJsIOibTxlIxiXS+WzwuaVfPkdAYFwR0vcuBplsFG0yxhNA//bQCtMQIteF3ofGgLtSXdibaXstkuHCCvWR6Mg4/zN/xuDt5qrQW/cPepjKgxqWSObF95BKKp0Tv9fID1K4MlRLI4g+k6j/nMr8hli7ZMtoaQLUa26nnQU9V7dwK/r6GdgCPVtO1gHm5XPL8z4HB5pa7/pxQdieCHLBV9+TFSaEqxhDReje2ZyeLFehQEYdBqEarxqe8L/CNZIQPPzkDNn6X2qmpPiQFiraBOkYB1Q2yr0j5nj6Xuv1ObGlMCvo4fDVW4aWcOquu4IDL2J7nZkON6qbDnMHVoq7/cif8Pf448lA5iqzTRMIGeUlq31EigqJ2g7LNNGGHmUCQ==
  template:
    metadata:
      name: keycloak-db-credentials
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-initial-passwords
  namespace: keycloak
spec:
  encryptedData:
    jiyoung: AgBcUBmxePlb1zMfM3AuCjb5l94cq0Y/wo4aKaM8Zf64oGxJfsdV62hYasXdgAhXrzOpOwV+gS1pJxlgGZXBhRecfT8pGJ565BpN6BUW3S3tlBzKKz9KQeywdTWSNwro/H+y/ySGxvmBQcK5MrFitbkaWegyh8XiNZi6zwKshIfYYrS67h19bfjmlKREPbMtO4ML0ABm1aK1PvNIk6ootKpJrap+XcBtnKPJXyF80qrOBcQvrN8z+MXzBcP4hvYB9bnaqGdsR70DLH5H4HfBo6HkWdYYO61ymcz1x/rQ3xFATR9+cZYLEQrgvHGOzGbvLXbbNGmTEQf6oouN8p0y2zIFb7s3UtxfhbSvYbx7ynXuZXGzemB87uSH/+hdLcg2zwxM9BpAJvnyml6n5EkXYINIKiOUPc39Xx59DKy/sdhAxNat9XkUY3T+Hjy1POGrHQ7CYvwVKqF57ffpH7T4ZVmbU3PCdqJg0NSfUO8/K4gILareKM3/a5ljHyBBSnajpJDMc2ZI2OtCGPWyhWLkzB5DuiB7NOEkgKnv8fsoAmNuT85VoHrYUAafIUn06rQ+TB3Zbeyc682UQm4A8AuZbjsnbA5lEBkznhXltceUPkjsM9vC4o9NGsdgw2y0IALRDzCiY9QPoiNYDSihSwmdR2qWR+HmH5tRvd+W6y88wwT1uNSChYq5ZhV1WelDKTdmgqd5d20yw0UJgm67JipXEH0=
    olav: AgA3vrrT4IljgWy68cCPOStbZdOqo8dpAVtXvSYTyhogPOJQgW/tC1LOPEkrrUbEqMbeahYJ4NihjmKROaeVyuY7Ll3cbLtocxbGKT46xsaJmwRqj6D/cEeOUyK2F1pCBdUHsIrElknZoLTX78JqtuxTvcA4vEz0RHHcuTTqZfSeXxNteYrWbUfBaY7ZPW4RX5CU+s5cInJLYAS/N7asj9ZB5v84vy/WHZI/AQmO7IBcn2KV3WC84HAxtFNxXQee9MAQiutyLjXoC1tdTWIxZ/mI64qU4cq3Nq0AARcLmEh+Q+7eGLzC3xDxIJ3s0DzgJ11qpgcd491VG78PcyQXvAHDHUTasQoqMqgvgSsTDO3dPP2lPxaEgRjzCXIwxSLUOmQtqHQ5qEXxTtSnSuvL/JQ3jM6Q1Xcc+kzD+4ail2ROu09/pNJFUZwLMS8y140+vH6FZn3CZFLVrKkSxsL04xWCE+DCblHIg1MCW5bt0QvaCqHm0BhiJwInG113XrqNxIvJMzf596G6Qebl6AyhLOBCeoC6iehN0xc2lO6oAl8S8Q4hbmJsUzOam1s3eNlnqROuqjOqj8BY8GlwfyL1fmVpSUg6GrHwwH9Ie+3ZWy9NvNmSrBiCm9v00CGL/6RBkMkHVmDsiIBWrMyNDEmDOAMMmm0AiSWz9TKiyy63PPpE8Cfo6fu9k5+4mGjd6vR5pRF4xu8lOt5Yycg59hzOb9zS
  template:
    metadata:
      name: keycloak-initial-passwords
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: netbird
spec:
  encryptedData:
    cloudflare-dns-api-token: AgBxmCyBQd1Ih5SaHD1CqdjuUOpVu9LgQ1wQbObXappqOJvdI09gKCmNQ+8ucNmoEbA88Of1ZNFIHM4UnvmdrXGRHIjJfR4E5GRQKzcmOb/FiIJ+97rYpm7fcp/S1Qcx2uA9THvhvO61O2UW4LdR/m2mFETs/tdgSdKkv5nRfxW+lWvPsMR/UFPiAJl0q77jUQksTT9QIrEmQ2L5miMtB2XtqtdBDx5m43xmwInl4XJeB5ZMqikonqb8D36bjgNYYgtmxIQ5arJc6W33u01n0rmkg7N6NQz/3lRXAzhluaitlUmYM1S+VbJUDuFUfmkTL0Wbz1Ab1eYoygshhWeAS/bK5ttkp4idfdHWd0zsRcM0ESs4T8ReQlelnii3ggh6SZXp8njIGucPduEeaBI81ODEsh6iS9t6OA0AIv4ffO0yRwA9LUsnDlmiPDf2NZe7ecPivM8uPZHFwhnci6fDp8kxdDGiEn3qzC6kFK3wOShTivtTu8LWieXNGb6PPf2vLU2L8+84MEwEHn5L2oQ/VNIUAGzrZMYHQTlTKajt6QNPxS6SwxSTWAh+nZzIwI6snSFKGZXFbcKisZl+V39JqOxIaPhZvwtOTBx6aDD5NGT6pVy4FAxfUwNTn9iElf4tBLdm/3oSrKaDyOKh6n2QO+uVK+H/jPt/HeRs3Y+MIDHyuEj+RzC/tOcNGZ3usX7yXO8B5W4rJut91dsQYMO+OXvGgEtinR4eVag9gIkKA+w8GPbXvvs4XF56
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-agent-setup-key
  namespace: netbird
spec:
  encryptedData:
    setupKey: AgANdgUUaLKyvercS9O31avaxlyiD7zfL0ect3vj7hkE3hFbfmoarKkTPBx/C6Bdss9PmyBrkx1JsZbXynIev4XyJSqohYawCRMUUy0BLMxqbUwpYEFbQrkJ7m72NKtBJb3qClKW1+6MZCINBmJs0sziDv7Ev/hhWUYILKthjO/W+Z7qoAdbQ65bz89hVaV/Lv1P7Qjj0smYfVrBsYYM+ihhxeX0TDxFo6cyXorA35lRMEIAQJbULnB6UTlowgHkP05FSc5Ta5Ekxztxb5u4lcfa/YaI8ekjU8/DIf9WPjjNNW8tNeFWD95lkSGy2pXUD2ZgjUbhBS+guEhzzaxgayMZhl7xigJQt2uXZilKKfqBJ8c+y84uYPoYuVhiJoKlWhMoAT6vNC9En4oH8lIn4Hvuf840WDt1XFbx8F8jfqlfUnjgJfZDK3ksoiH64kb8S3AIZrgURLeMplIngC+QybVWUwr3mPbtAFTJQsV2whkSd1uelD1A3xsEDHGnBPiwhSFP2thn53/4khRzNJ2atDb6GLOW3BR/OL4HPgJL9HGAYiQqZgEjKZaxpBuOHm2S++S4z6kQmq+0ziIOwfq6XbOJwl2TFdJ/HVclTiHUQyfNFWKP4DBDmZKGyDMxOXJdyAqkMPx4YnPPbuZBdy7fViRuH7bCljluni/aDi/XfUNEAac/SxQIBgcJKTyMsRteiE9Mt92iHQfQhjBjbvNZ64G6S7k6mo/dUWtAAsM240ySkjrLKlo=
  template:
    metadata:
      name: netbird-agent-setup-key
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-backend-oidc-credentials
  namespace: netbird
spec:
  encryptedData:
    clientId: AgCEhhixK/003E5HfsqYAQbGo87FBib3OgmtyIgHbCb8EcscFh03JJMWhrWCbRN7k3kC3q7CY3KAreG0/jMP4jJi9Xb1xRJaX9W0/bxSmFPh392XQfK9H5X32bsI6KyQ0Sm1E6FgN4d1rKdLL2qt2WG/Pvl37bKpQ/n+AEyBpXUcqCxbaSsIqDGiBam5/VVBXNEh94aE3J5hYARNPUgUOOwzxjZLxcPAw6jFKy5yJcXeahZ+1gaJpjv585ENeTlElZqJAc8p+lqw7s9Mf7+EW7FrhiJIv0ZUumGwdpBaNOk+D/64G0CStvdzQ+wXR4XC+SCqDsHZNuAy03nu6EqQUhRmOJI+SP5SUJaMABc9TjxeM9DwCHuaPElURGgiHXdOU8CwWi1kNkgzlojO2CuFgeMqHGrM8Gr87hQI0iSe59NnJVjUMZp3N89UeC5nP9A5fhYLP32rTJMRhIgVJqis9yS9XiVfh/9OXhLOv2klFwnHlIR6cH7YGHOcNna43aAsiuM2YkszGz5JKs9/Q0OJc9wTu3MxKrE7pkMe4U1svNqJra+HiE0ivbq7+R/TTUNYG1E3kk2yx/uF1+YRcMX/hlSLpzJuIeZ+Z2ogx+IkHSbBEFkd8Olh5DsBnUpxsRZPa3sDXNjY49dHFTLPeLY+tsrqgWPyNs6lcOOYN6FhTapaYE/Pg0AB8g9JBa147fLJ04uZ/6bLyZgmjU2+vgYjV+A=
    clientSecret: AgBMGLAe/rx3YoeqFv81ioQC8GI/GcIg2IXiNYOb61l3bVi5VmAT2uV7CmNuU+iIL/FIslgZeLYK1WSWJ7JM9gMzrZuAYFxi23SsYjafWRTWZfTYrQVK0dlp9TCBY3s8C5gujiE08htCmQRGyRnsaFmuxJfwGCzFnPYHKPKJWdgzvrIcY8Jy/bQgvBt3YWZ1264N8rmEb8zzFmn3dPZ2YD4ryXr6lY450VmdxYg/wCx9/8UuHcRqy3FH6UA8pC1Y6w6PsAN68ENobkmAT2aPRLntqMvqnrMaNkJWDx1VUuz8O4F9KfpHdvPko+r6xaeI5cuLaMR9VzGS/7mtADe3zPniPzcjj7sFmX0Wt/QaFGx5JsaetPs4ngw3IQwttPkvRibI31ZGEioJDSwBT3JfSisz/oS7fG1IaRh60zswNBx0CaA0vHJht3bbb/XPdHdoEgoPK6qRXokW7lHIx3OxP3FmSqUKV/51y2LqEIJY+T0DWCI3xXiOAxUKWw0ThThUmtIuEsNFJ/riCgLo0levCrUtTzV/+VxOmo8SDwZlhoIpbh0OOk2YyoPT+bCBDZUDWbxnEkVxL/z9ansvJ7db9j2qzv4b9qknPVDC8eB+BjMHZE+IK43BX5tBAobAck0bGRTN3R5XrDv2oo/bG1weDjj/j0aRFVmp5i3NyIFqSYY7p+d+82p5N2XbWVnynEUogx4RIKnHd/hCV0MLeeNWiyuSXcgTaeb35VMpvIExb22kQQ==
  template:
    metadata:
      name: netbird-backend-oidc-credentials
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-relay-secret
  namespace: netbird
spec:
  encryptedData:
    NB_AUTH_SECRET: AgBwuk9UL1sF+dHVXNjTGY8/FZbHszo+c/lBR/tAcOfwJRDZMxUpDUTZCMAh8dof6Sry5RRSenZiWOM5pUYz9Vzzple+e7UjB//TJCz7LtVr6DAcZE8UHC79pxYiQoRuVPLHnzV5uy7OOrGJfAlKfYxgmYWf9YYDV4P2H86Oiflw/fm7auedQL0N/FMfe/JCYKVnSIss1m33rVYeQS66RMj36uH4mo+ctXwbeKfQFEdMmY2MpEuaLNNp4UKqOMxErHSOIR6iCDjKXNgppSwQaZ0I0Ydqr5NWYrtIpMdf4gJA/+FTCCs/nRJkPZFFUD4897XKXvueQrw3lag6dEJiZPMEl5IUaysq7vjdkBKis6INlkfPUC9K2BAAMQePqZEogYtEQUU3Ahm8JjUD57RQikYKxWv7rvcxlMlzGNGkWBpKT8idjy8gYY0/B8+RuU1XaxZ4xkNN7FrfpfNQ0qxuT1PzPYOmq8q12NmgKCNkPr0G/Tqs4Y2L6P5GFUeFqf0qrDE0XTRbPz6ly4Bic7gMq0r5Oalj8UTmk05rcDbt/1CnWqYkN3d80n23R8NMDbWb634H8gFw9E7r+1VA1qrHZz5/5VuS84yIHe5tMZ9vtok3KXXz9eE83+ucceRSu+ex5GsrnxG2w3sZB7H1ogbpDAKm8Q/BeIKWxessnfL8ZdYA96zwn22ZNgpLu1/L1bxY4NdXxzbHVSXrnQvnu7CFMU5rZ+XrBA==
    NB_EXPOSED_ADDRESS: AgAsz3POoeLHik09MEKr+e52iDi2drsmsJntQHwi0NVjkGAgrz0VnY51VDrGOS9WaXZXAD1iPU1lmIAIzH+MerB6Vcpo5F8zZjn20yRQpT5XaPBztGtMWhHE8vey/xXCssafA8OgwN/201mJBVUBOGJKdIO8NTGmuCaO4LCxbmHP0fiYE24QzkPEhTdIkHjpINQuzgAfuozrPiSWmo7br+z/Y6w5s7n/onUGHUC4scjxkMq35v+5SbLGzfOJ9nyhw6ienhUwF88KezDvxz1hfra0Il8m3PzcqluQK+Q7E2emSZBWaKBTJGI8XfjiJN/tRtp0C72V+5nPSw+AnfdJ2BUGF1lsdgHWoSj4pJhBhKnxtbmWRmWs3WYMD4hzWsAf1Su50cNoLtlyYNUqENcbkrrprLacQbPYwodA9Is92sTCbmWXaAex6GGlRU9Kn9t++IXgwl3GjTUIY59EgQyYSq2yxYhBM+ZfIEY4VDpcQLxScqz/yYiYoYVlJp7cyH1MvZ1sfNnZGkDxlyZpxH0vPuw5n4HloQSwPmbZlCMFJbNXzZTcVG4PM4VCIoUaSQIn/1em1uu1McN9bz19d0Xfx89/C0Qs/RbhDga3vlafTxXN+/efivErI9/1cXLjY3UB2xj97PVejHeRoP+L4oyh5I0zCqcS+LBUrAYhZrwaoRYwHlyBUHr9tOF1JU3+m64h6uxVVuL8BSSUeEeKrbYYt5HsipYd/5PWEMzuLxB7gHPZvWG+aaEa
  template:
    metadata:
      name: netbird-relay-secret
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: netbird
spec:
  encryptedData:
    password: AgCG/s3MyJZ4M4qNyjVMonU+TDY+ocaxyye0tMF5kh7uVSJjHoASnEfKjnPpOC/quWksWploqGTgyHsL0RCY9BOPMpAuE4Pp+53VY6jAvH/NrJZOsqQEdgtdmjchE+3d3vZOCM5tHkjZSClvTGgTRFkXdl5pIWE4+o4WuLGtcPFrB4uLYCwq6UgazoOarKTAaj3oPuqssh4HFGyds8gKNxBHaq6cO3RvcH20P7dvnw005UCDymyVoTjxiuueT5SM48aH7O8aXhRYPbFbLwHsD70Q0UaYRT+UUPWnvpHWpDheHr3dVvWM3nkXxkOwre5Ld75BjfDfIM0/CihULYIOxUi/A5e2HTdauGQCGUSJvfh0z+VwKeVsLe6mBEJ6EKCWpWPDwVtnfEYk2Yohj/Z4O5F0ie2eRhXN+QsdCu+RQAxf47gFiKe0GueN1tVn+/QxGrrv4Lp2Jb1FiMwPSmSd4kJW44cKjd36j5ESrXyifOOwvcC+7eCDFOPfTMqYmloRp4RyXTec/BFzxZZ1ONWRYq0Tni4tlw04CCNsEVy+OYDArWu48v6cfPTAiHjgZBZHyIrRpG6cPOGzcWzJLdkQXkK6kxbUOqLVjPNEW2x71kRqiyAquszYV92FBoE3n8T1Au8GsADuurnN/t1f1B8SFi0BX+dILcMkxmQgVTBk4+946/NT2jRILE4ADCWYCI2CFzK5esvn44rlCzB5TeAPZA/l
    username: AgAr8X32/nMm8HRFPn5EK8rta+TqJtiBsmNKJqRe8EANSWCj/UlVfLSv2g1NSsOYZgorVKgHsS8ROj6K0xBbN13hGIwJ0kFICi9REHTdl5j34ElrCPsbYMwr3ZHjTHqTn1h/k1LS6COpp1lzAtiE9iUHKRu2EPLDJO1E5teRFLIBXMXanWWiLzjzgbqHImmW0p8/75yqs0DK1xYNiAAge7F7KJM9wq3lpRYbJ9oh9fv/hOEAL637NXr/fKtxMYj+hYzXzKSolbVep9iBmltCI5BJJMbsGLFaSa5oSjG+pS+2UB/k/osUiO2TfRpwn04yIiiMh5hmXqsAIN5T8t41l/46hqOvMMzrWfxHHNPGHBdlMuypXMVNrjloBLxbjApD8fNYR+TlkC6QqPEdv2Thd6Fta68JUWhLPyzbVhhGD9roMvY5r1diN4QQmMEfhl1JEumNWYDA7rSi1wrMjLr1XUJe+8WfxfW3f0RzNwXJvFtMYsqnjUJJkjmuswCwpfW5jBkAE2Q5wTdaFdoGCrZZsHUbiT3Vittw2/QUWvVLNvx3DZT3T9D6T7nVuq+3I7ORNFcXYhO8fdwevMhv/1L47n/EkVo/7ZfSj2xIvaDZ4/GTQvshttkOQkGFchFmkbmH/F/NP3/Hha0hk40s8n+1GR/nOsUEnEVBlSDbL8Aolt9ZC5DSJY8ISkPwWcLv8DqdY95MpmC+qgrT
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-admin-user
  namespace: nextcloud
spec:
  encryptedData:
    password: AgA133ceVXtfCLHa+fpkKReprcm1tpKZkbpTwYf4tFydhIIO2QLXRD2CioaSDYsk0aNlQUzXZyQJyMfplLs6G7Q0t/2d8cYVgVmEBcdiXgB3CF80gY4/2newgaCQp1MbCYoc28q4scE1vZvqgRfSGWiO12sxPklMWtrAxgIoNz8myEhKEuy42GfneXDUjfN+LvHFSJugDCVBLE4rc5MkNdB40UO6R54zxL40FQf7azZEMcLDIaUfM+ki4445V00rEMA8J9k9DEdPIXwRKva+osIcbKDuWv3L43fEyP0cfj1g5lgNcm+oX/Kmo+Lp48+jvdYOMQCnWW2A19pBIHlym33bPR/ViMkl/o+ztKiH6UxxNy2FyUZkIxKjL+1h/XctiqiNjJg7zCWEn2Mtue0zN0CMSe37ApzE5DzngsHpLAkvJ0DxHq3PPd3l3TevgeVj0hc2gR0sT08TuT9TRKK/EqCcPHpmeKtnzzruV7X+Vl0nreAXS1T/x2COW4FP8RE4RkXSDxf8OPhDy+MSFX5ortwbGM29yOt/9YU1Sq9uDiCj3OeXkFq0wEHCMsaSVJB255cMTR/a8FNrQFIeHT/UkRVQuSCVKAejpeN3EQnMr6ni9BQWu0BVLTwGa3XXQ94ucnmQtye3TwhIz30Hb/4mLloU5jMYUap3NwBp9cMwOxrKyH2HU0cDARAwgsgJhRA6a+OAGPz4eBwxZpuRza5CPRXNefF3cg==
    username: AgApHO0tUVeG++vsB3O4rLN+RrgTFG4uYNvk3lI8+KMV+n3faNak6j+RwhNtwqwcDO+loVQCfvi+mpW9txEqCfTRQ/xbI7oAHGP2fg5YJw4fHFDdKyH5tAeYE27nF21cAjABFymTqkqyD00A7qlorjljc6ld+KUwS67XL0LFsssuNfZQ9bxDjl/Yk2a+4ykNXNYQnMO7hqvxcrIhq9Dbbr8rOFNYTM6ByhBwaU7R5/QgUJ/52Eyb3Og0C97SSesNQUH2UsjHFVEqdMD7dGmdiAdvlA/5wd1V8yFlaUfPkOkWnCpnkpz4frSIMKJwQcIcDF5/YIfMtb/i4AOcUAv9qW4brlS4iCB37juPfItZkUtXm/iHItEAE3E2lkl9f+t1St1f9KuLh3aNOSgfiwTSU26Iw/850ZUjQKhkSXDw4uqiZ7XJC89dT1aavQfPLugj/5U7vuGRkkC+O04x3N6lLbDOqNC7O8H+sIwqd2AnOvVFvGf9ucnyY5ziNuDgNUL8bJQPd06zGh4u15tbOaat4LJ0DbNQACp4Fln9ivipgLvDb/8+WYSmkX2gL2TPMjzoq/FLtB3jgbhGcCDb0gqDkE/UT19JVHjCoBZAHi4Zds4cSknKiqbO8X5mcK4j4dQNutsA5cAbWasjtC18tCySevkmu/6k3oZutg4OYaoiWA0lHwu8AtFlQJGVMg2m4hEp2vaZ0SRsEQ==
  template:
    metadata:
      name: nextcloud-admin-user
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-database-auth
  namespace: nextcloud
spec:
  encryptedData:
    mariadb-database: AgAJjONCV3vYfnRn9F/Swg7ZPECa3rom6sS/9LVkyJJhhDxuAekF37urjn10HhsKSWHcb3+iqWd3IGXfVnBdBa2eEDdg8SQ/ljyvLqw8NjcaXIRdokUgRQ7NZlmTK6/ANFY8JUV960jseSKsCrxaA/WGBQnGTXf6MI3DrJlfdy3TxNXOYgXVACGh5sHOW/h38lbzLLshYbU9NwJz9KvDHdlDzjV2cF7FsXW91vclK6eNpMoABgkd2daOpwy83FT6mCAxkK9cCHlrsAEVXGMoilqPMVkvcM7VxXyixGeOXN6Ll8X79lkj5ja5uzTADEjdXhQiqyRrFzB/EAJF2LHy1sxgpxbpXCi2Ru/znJj7scomE6TF51VlY81sHxLFUxAgwMgQcnLQh3i1vYru/1NCrdeC9IAWdYCe6wuiOsf9FcHq4rx6N8zlkUFgewXVAWadUEpGad9QUQm//aVjq2kFM1kxRfiL25hoB6ec4WXhDgcy4f7CKtKIgS5IC3hxm3b4XAuuekr0s4G2+XGf1endy/Fwu4gVyFzRfQQx8hU75s2jaktmamaIfYlxuHfEGJnnxSbIFOUGM3sQd88sT/7gEXuNQCDXcsNylSaQNwYjPbu/iTlscMlowlTb32W1iM3IwSvx2xeo0DOMVzFUBqE5fxpG9zyA8U7gte7QQ1RWvRXE7Ll4dsOuHziPANZdMyRml8WCqgmcvtzc2s8=
    mariadb-host: AgBXn4r+i1apjXlWD08Qqx5Jh25Un1b85dgd2gLVrZmZo+PYXyAjQA5DLn0rSTfQJtcwp+8/huL94jxm3q1wXQkh4N5TsB7OItt7LModq6ZntgsDqvN+wCK9mvubBee4SSCKvQolgoI92YkJBBM3kdqN9Qvi/AY7X3zTUk2KjwlCeURVWY+L6kNEtPetNz5zZbMz/LAo6wPBX14ZhZL/48WWTURCyJqj9OHYpoH25yQaSEIaemAHRFh+5KLe7+J2nUmuiViXu2OuxmhwKKZYANAiKfur62Phmbcp0BTJnXjXyVAX/1kqmpkspuBOd1ITYg5HpD28BMHjeVWmoWVZf35eQ3IivD9O/Eg7l0SzBoGIBla4dRXbuKRb0hZON6CVqMzv86vo5zkLtd0n6s9vo1p8qQJ0eUCxHfXzwepRo96trvq+FUwFH9/dGhGan6VZP2RUwcvdY7Tk62fzEA6sqPuR/XFjfn4BrzMjaH+bB1DmtHjz/oeOX97wlCGboj1gJfvTIBF7HjzuDLJ1rcI7UTMTA88RscYMqlXhfG1Ao2ZD32d64q8VibWDgVoJMhskUcePivURV/o1xm74rYp1TxOdNn5TqDDzhYFSjgWcjmpbeqC3A3cxlf3DnjwFnm/daDRBXW/W4261GI0Zks8CTgWs9xaV/sNX0/Nei2o5Mtr1q1k6vm4n57Xg17th9OMS2JML5JshKodn9AWqKOqgvzA2Hw==
    mariadb-password: AgA8r1aajxOhMcQbDLegI4FOUu7lU0hlnWpniUCKz/oAVHf58bxsZBXsXwnF8InhUrrtUk7tEZeWK/YjbbI19IlQRuaJ0CXfbycSOnma/VkVTx2N+U8ODpqcT6btzdQXq9qi4NQkv3c/9JLxqiSCJi7cm/T+x6taSEzoL4Aa/zzJ+yMXZU8sB+mvkIxQMwriHQeQKoxK8AHYTG2WQM9a0tIjLsmXU7GhpekryMRKCaJDdjlSpFZCKO+ym/eCGeqP3oMXV7MRpB0l9qLIzs+GwKigup/x9lqLmHCuxaEeFZlkThLDTiuVXGoIZ+fwEOct2TH1aVKZFQRfRG/EPC5+AJApub2iiFofOJb8SxLHv9DFmfbPz3108vFUN0P3u8x2nZLwCWF9BjvpFrhYxWgr4M9PVtLu8caOBP3Ja9W6jYqawh+7ARXx7wW9GOPA1P9JUbt/7mCuueiuWw1xgchfceU9aZeFkebrhcH4/sK+Favt618Lv7XB8WIT8LK4IjEFs4SNW2rG6p7rYnCvtaL+Fc4YR8J+kkXxlo0LHLN9dM6TSiueorQdkpudxG7IJc/0xIs/h4OTpdPPhkEK76GQHen9X718nFI8oXPraGmyygcdIIBgV964PEMH8EwPZIHgHfddVosO2mupMwgccl8GDNUDTf4MO6hKPUWRb4P+QlUaFqwAw7jvu1psLA65NgWHxUn6FGruI39iPcUB9P0uQd2sSKNabw==
    mariadb-replication-password: AgAu3zmyxd/YLtPdmamJh8Re6VoBPXuwp9zeQTIK4Kd36Un7Ut/nTqxxbcdOESmEg3dVy1fnxIJp4NiLTY/xmkFGK3XpAgu5UOYSQolxzChB6sRC0nVNeeGixt4/tpv2PQocbFNo4XKKWiNZ9fLsEbLDAM/tZPhVamlbqCclQelZ9+PBcHcc0c2NkHqORHF5tW5QSCvOgt7b9zG//X/JGon2fa1ittDrqOr3s6Y7KJwT9YLQXCo0tlLjQd6yPQ7ew+KIwd+ZOlq7istE39o5s4qIRDuTHR+soGaTBcAHjBdVOBaM1naGtSWMuw6pUrRP+KdGOp3Uh1YuwjelmATAIR9+4/Gvvo7hSPlI8JaAV/RwJPlMJTe9MBWDV/fnLYnCWea/vNIj3n4eRkeXHEHW3mqu4K7PXN59Uw5lU8WzHI3Zm+vU6f20tYYA6E4wfD5FP2EwHgCKOEloYm9tji4OhTkERSQGnH3IKNk+Sh+UhDMGOd8V2VZXHoPEsLKKGny6D3FqRKUFzL4+ysjN16p1J6K9wJGwMHhBOabp5f1JhKoKOgLsbXrAEV5gdIzdlo5EoipA5jI1l7k3gcFUIptuxsBTAZwrBoNx29cebj9C/FRuz2kEC7J4LKDHlBHjhlJ3S0yZ9LwQzQk5LKKnwcSCKARAImrih7lvZsAzDAtWfDCstg3lBT6slx/mpvSBzAYhow6vXAI9XA6wxOy+iQUl8++oaM+oEg==
    mariadb-root-password: AgBjgQYehZaqqgIT1yQuE+EWb7o6m1ad8PTkkfSReV5QMDF6p/1/8bTkYxiXDR6VBRNJBKeZAhHHZMw54KRdDlj2JMPPr2VwKV1AJ0Kb1zXr2I+3cMydk93K9UHWx23QzececXBIXsXnry+ah8rEfJqENsrtqJSe6dv+T7cLJX9Sso1zvRqYq5uYvjEx/HmfX0r9q5R2RXhhBnJmBAIOVoh5khSCVsqO4Ah+Xt1VVhmWQWEdJmj/c4EXjqnJq753HlDmkI21N+A3VPbGIQLswkIDzIZeShjYAcZCXkLq3DDwJuN81fPSQZN/2Sp7nUm+HTdyOjTxXhEMbszWKwCW87C2uhEh0QgJb0iyc700/KHX9jV4Y1hwyC7thmSRf4p+O3iqSBgaImADh1BVBcpZM4D1CSjz/dpfcdDIxEAGHsiQDZexMJK9sC5Nx2kXP4pWXLxLc6XidsX7BWzIl7lzcY+rneZUWVtiRUARtSqYmO7l6KLmxUz4FuwPraD5B7S+1iy9kZ26JJ4AMJ+CklHMbR2rxUjcOt8KXkMMcUDaPoHs+gMddof8AKfBZGQkBc9oPnmM4T5yWhyklX9Q6zbk11bAplgQyjEroCWUb4q0ozb1EVI7FdtROGfvYU5R616//zo/AZVbAMncnRVdSkSq6Nmu0eaDs0mUttTf69o9D9yM3oQg4RO3yOTVELUi4n1mohxr9T7heip2aDp3S5DYvGsMWL63BA==
    mariadb-username: AgCVfKxxY8KGPODGmC6S8JHqVOVIvrz+t3mW+wAROGJa2mHeRur+lpt29tQ2a2Cc9dsGDg3s4cQ8xqVcFASkpD2gw/MeMK1dt/teNjYFLuWY/50S0a24nXHMzKwrZPTRu+a+7UrhMysitU9AdJ7uFqvLEftKj6y+zVXnB1OtTDfp5kV7XUq2aF9SVelETxoqUqPBWFUbWvAF0jyVMZ2EZbYOv9UmqD60C8VTA8gd+IDUcycnPfY/5dvz0YzufAxzyAQK0fGuX6WHEsdUqNX1YKdI7GQx1L3qMPgPSw6wlkYR22whwdgenyt63DffRR7mHhUmQeL9I75UUWuozFu/V/20/48lsskmSUCdJHxoiFvGdu0v6dPpfr0XMMQR1O5Uao0XLbjU6Asp9QTMAUxYIr7mdFU2Ae7jliRSviWD/2UOhtT5885Ua3dWOBvbAdsSwi6UEZFvH7GbkJRLDKmeoigCxKA9GmWOACrdCg7vvKVvYLFrct2NBCxpbKwTUw/qkT5KRlxx684lgv60ICYTPLLfxH9WI9MtxTEX94nUh9X8bKi2YkUi1KA0j9xOWXlKN+un1k+fUy8+jszNv+SMU7elCkoS6zFBHmg4uqIe2ItjBx62PR1rQ6SC7cl0NgXYu25ANW/Js9Ougeq7awOEhjj5ndB/OHdtEU5vFVeGc3XZa2TYRBuvkvPD75fTHT7SnxATRra6daehsv4=
  template:
    metadata:
      name: nextcloud-database-auth
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: openwebui-oidc-credentials
  namespace: openwebui
spec:
  encryptedData:
    clientId: AgATZxYet2gz1Of6VtDGIDaStL66v1Z4hJKja87uAnc730z78ePf5irRcv//h5rxJkA7Ge0dB8aAdyUSKwySIRwA67yV52bTPLroyYUuqe3bIRFaUWZJKWWf4gSpwHIXDDKMxhzq67k6fuwvVKT/Y74FgkUGRqmyDF0syvXiZ5dtS/V8wqxlWdS0cng03+JQf468N5LUM2CBW4kYwhfqB9EF2txNKABnAcBC5RVKRMq07VmP2BW645CpqlVCDiRQWeh25GsWDbd6YsA/BgG1uNY+OeN/Kd1TpjVdnMJJG5Kgud376Qmgl8fo7RKB51hnRETUL1yBMc5k/BUZH/deuZJTfvr9JqpWwjr2dAw9DPJvX+fTu2LT7kXQHJbYr74908Yd0JrUCO2yOz4e9DpHvCvWCOlYqN8VDc4D4ZKw7PJgNLPdkue8lNXsifCDARebtGScekXLuWnVfojTxmBPz/fUh/12VT1OJxmt0wGtbV3sPW/XT/abjdMI3/PVU4DCeD/4strplrwoPlehk6tQWtBxIIGD8KPHXiPX6WknNtWVlQ+iiIR6hHTLckCyFZ7f4x9msL8JdFquuv4ICdtJ1RJDajHjEm+GVIomsJKgUW1eN8b1pTptNM6YDPewRi87ASN7988s0semlV8YBtBoqs4p4PA3LX4PABEwBGEurJNVsTDW9HU4W786av3nQ3E8vsXMz1i0ZDNm8dY=
    clientSecret: AgAAGMRkzGpDj+45aIwMQ+DoK1FF9Nlxq7GBYgp8+9HWSdRt5rwQqmMv636bQeGqx3PGzrrCkTTUqzlr1otF7ilrwMWV+mPPjoR+Zz1q0LvlzKmhFngcTtSJJtTT5VQyd62EyxOIuVyNItdJS5SYis4mvrRUBfVnlNIVi6deC+k1s8CiF+EJPOGKeQHV8ozG11ryqK5OqzZNiidd+PT47FEfdVbnNz2dQzPKFK84F8ob9kMgbcQrxW2q1OmTCgbtweQLVq5cKmIBeIuxtomg2PMwyeQNPxwkiCSHzbZ9DA42CzqbvmqyOw4U1Ylz9KeG4AhQZN+mNVVp0pzy4yiOeJ3g9bnUFFO3GjExTxS27IXRkWmhZbR33faOASNeBEgDaiQah7dS4XNCkAPj/cs8tfGkBCfs5vFO5Buc8I0VkZiQGecX8u4sQCX258Y0vIK5fWpc2sBUjypW9w34EaztA5QqNSwcIo1KrIuKBVThqTqE0+RPkJNe73fNpGR8e5bOxjF7truTjSNKpx2VXAizvg92+LvUcd3KmL0NfMHlIeUe2sW6EzbAwhjIK6p98pOl1mo1tk1B638KT6tnHvLyr6Ej0/+XJ8rhcp1LBkQYsAol1eEOej2Hik0G1HO1YerEoiK3TIou15HwpQ2vYLFtbuzyCiuyq422UWbYiclptN3R4B7g70mXbOeY4xY1lOVOSTHYtakHSg/3Mzfhg25wM69c0ycXqMtC
  template:
    metadata:
      name: openwebui-oidc-credentials
      namespace: openwebui
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: pingvinshare-oidc-credentials
  namespace: pingvinshare
spec:
  encryptedData:
    clientId: AgB6AVC0G8DfPEKjYNVcOdHaI/Xx20vQIq+sG5jnfSjiDpHuBOAOk+dazjJ1tmIYIybyygebVOHswKvo9uCHQiIOoYPOW+9AMYUqdZY2NeCfib5x1w7wUVA0rt4O1gFUHvmFbgUcb5GeEzabe6T0pOiLFgYU0LnUztLb+dFVTdmvhU7/pqAWVtjxAfOw62T3KJpH5y7WRosDNyCxUBEFcQttkKWGL5t9tA5Z/e9dxl7cJ4WIWrQ8IciidkwsqKXD81ysGFgVIlzglPxsQjoGYo7Y8+SuTM/m9R4n7mvQYMEtq3RYRLDmIZguGHunXPD3RmBTh719h7IL+bokf2C9eQj2w0oAHeZ1WRMJoZYKP9f2LV0TcsH93wCV5jEJYf/xVwGNIgZyfoQv7kbcc3hrncf7xeUWlHVqwLfZuH9NVoeObLPnkBAp8LnMvwMTc/h86e6gsRVML9oSrJcrtmxhuCEo/Se8O4OOJFKLIbLyy0PzSEVsgO5CZoW1Ven51+EyTI6GEOJp8pw1zwBmjM0o2MkXb2al8is6kBnkD7cqIK8DaOQqPd0Pzuq34IdWIHIx60l9leoeYG6d05Rp5r4KAJmSKuNKjKhmf4VYoYCs3icR65/RQuBBDE4LXZ+BTE9OyEvr7vKdZQteS5HdYCxMINDiBfVbHofcO3zmAFxxMkxhkyJOMo85v7pCy91ESZ/3uL+YEDKa/3Se
    clientSecret: AgAxLGU9KoY7a7pO9lQNdeEF5z/IugZGcfM8C4eqolsW+1PX0ChD4hnucuhbIW3HaDvDEcV28q4Kwa7t3sHuBIF70+L3Ct0vVSnCeBXsY8kOtYtHkSxhokNGRCjwDLBOv61iWP0MyD0R/xt1uoRjua2O7MmhknkGFgfmZlIr/m4Q0zGK/ZH4r9ARNY4wdsh5FTGiaUspFJnLVTKqryUErcJFLZalKBzatfjUvuUMckKXiCpSlCCXAsFZzrniAvZ9OiXJn9gfRnFh9yhT+C5l9IyGlI0a64tMgJlu68F0YcucO8j39qDjprF3sJhkeDFxE6zrTk0rXM9/P5a490CXxdto3JrX7Jvvuc40S8S9vl6h+bl49wmLtQ6pLZZQ5Apz+HdYP4wtLWDpXAnO+ymVpsGjbg0aCb9UobruynmuZFvFjawqVIznvJLUxQLOpo1c5vNJ3DwWFh2cIR6V35kAicDC7cnyoNe1/wcJYTY+FT7lfnVRV/moQq0nO31h37Nf3CQZQCenmHwbCxyclbGLA7OMBuAFTKEtVXKE0UO27QpA9KbdulXOVbIUMBN89Dl/6tOlrgaAEtFunwOf/rrgm9gUOhAnlFKZC4A7eYOGjtpJti2hypUSfHwsItg7qejPR+Y6daF3qFcNwZgwBi1oK+i33i3ahc5xxuQNq4A5jxSCdYWQgkeN7hIZZfcAyBojmTXmYwm08MisNJMbB7/9QsibgEFHYUMH
  template:
    metadata:
      creationTimestamp: null
      name: pingvinshare-oidc-credentials
      namespace: pingvinshare
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-setup-key
  namespace: traefik
spec:
  encryptedData:
    setupkey: AgABoc3LquXGAbkQTZyhKvRT+JTSSFemrCPZr+td5H3a/WO3OWLO8peO0K1MMe8LkJ5VjQvarFgAzxD4om7X/9OhnvjwY94sOtr7licFT2pMCQF+HkEcTaNVfWZZA9AUa17FQNM3XsSe2pJaw77fgoWDVguH7Y+inLeFt5vQ/SfnCGrXbCu3aEfjzwT+WgRMq+UNScX9Ry+wY3F4NrToGSR0zwkxHFPcd0jvsCUTFJGwYkM51+JpB7Ju/UTzOIyC3xZQTgzhpFvLwk9XbQqHKYhcP6fVKn5WUEAj93yzSowxfOcvqZTFWtFa+Nu7QgjimqyWmQqvOEzCn7SFj8iSqYEd0ocQp1g0Lv1Avt/2zaVlc7xweHAS68k0iZ5vYBinFCqla1fPmlUQdwydhsnVVnEctwuoaOEt6iztoVkN9u+W1hIn9aoacH1Ag6ieIbSfZBeDDbjV0OxPy8D1BLVQhIsHY8u8IlppYDXrYd6YPvr6Yc/n2+OIgsG8r6hVbdPeSjTfbRAXTPgqpDI3aThLugPHe1oPZW7mmoMhcWOlZkxv/8ciBTx2M0xP0Uz/hiXehyfOGC+XNgNuxm9wXyZ6xAVnCuGNVoTyahSGfNGPVkstWZpYhMtKscZupy6E8j6ObFvKkN2Nasvj0wDYhEAE+daLfovUNpdUhx0uWRg89iNyV2tZmQWBH/yv2QEyMF0qNAUftxVQU22DVHHnNfg/SyvS5BGnYtCJDdOammxEtt9lmSAkSWM=
  template:
    metadata:
      name: netbird-setup-key
      namespace: traefik
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.3-r3
    helm.sh/chart: coturn-1.0.3
  name: coturn
  namespace: coturn
spec:
  dnsNames:
    - coturn.homelab.olav.ninja
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt
  secretName: coturn
  usages:
    - digital signature
    - key encipherment
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: keycloak-jwt-signing-certificate
  namespace: keycloak
spec:
  commonName: homelab
  duration: 8760h
  isCA: false
  issuerRef:
    group: cert-manager.io
    kind: Issuer
    name: self-signed-issuer
  privateKey:
    algorithm: RSA
    size: 2048
  renewBefore: 720h
  secretName: keycloak-jwt-signing-certificate
  subject:
    organizations:
      - homelab
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
  namespace: cert-manager
spec:
  acme:
    email: olav.s.th@gmail.com
    privateKeySecretRef:
      name: letsencrypt-issuer-account-key
    server: https://acme-v02.api.letsencrypt.org/directory
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              key: cloudflare-dns-api-token
              name: cloudflare-api-credentials
            email: olav.s.th@gmail.com
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: self-signed-issuer
  namespace: keycloak
spec:
  selfSigned: {}
---
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: default-l2-announcement-policy
  namespace: kube-system
spec:
  externalIPs: true
  loadBalancerIPs: true
---
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: default-ip-pool
  namespace: kube-system
spec:
  blocks:
    - start: 192.168.0.90
      stop: 192.168.0.99
---
apiVersion: keycloak.crossplane.io/v1beta1
kind: ProviderConfig
metadata:
  name: default
  namespace: crossplane
spec:
  credentials:
    secretRef:
      key: credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    source: Secret
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://upload.wikimedia.org/wikipedia/commons/b/bb/Gitea_Logo.svg
    ingress.kubernetes.io/proxy-body-size: 10000m
    traefik.ingress.kubernetes.io/router.middlewares: gitea-login-redirect-keycloak@kubernetescrd
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea
  namespace: gitea
spec:
  ingressClassName: null
  rules:
    - host: gitea.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: gitea-http
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - gitea.homelab.olav.ninja
      secretName: gitea-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "false"
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  rules:
    - host: homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hajimari
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - homelab.olav.ninja
      secretName: hajimari-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Immich
    hajimari.io/enable: "true"
    hajimari.io/icon: https://user-images.githubusercontent.com/27055614/182044984-2ee6d1ed-c4a7-4331-8a4b-64fcde77fe1f.png
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.9.3
  name: immich-server
  namespace: immich
spec:
  rules:
    - host: immich.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: immich-server
                port:
                  number: 2283
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - immich.homelab.olav.ninja
      secretName: immich-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  name: http
            path: /
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak.homelab.olav.ninja-tls
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.middlewares: keycloak-ipallowlist@kubernetescrd
  name: keycloak-admin
  namespace: keycloak
spec:
  ingressClassName: traefik
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  name: http
            path: /admin
            pathType: Prefix
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak.homelab.olav.ninja-tls
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  labels:
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-ui
  name: hubble-ui
  namespace: kube-system
spec:
  rules:
    - host: hubble.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hubble-ui
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - hubble.homelab.olav.ninja
      secretName: hubble-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-management
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /api
            pathType: Prefix
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /management.ManagementService/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-relay
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-relay
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-relay
                port:
                  number: 80
            path: /relay
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.35.2
    helm.sh/chart: netbird-0.14.4
  name: netbird-backend-signal
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-signal
                port:
                  number: 80
            path: /signalexchange.SignalExchange/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.8.2
    helm.sh/chart: netbird-dashboard-1.1.1
  name: netbird-dashboard
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-dashboard
                port:
                  number: 80
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://nextcloud.com/wp-content/uploads/2022/08/nextcloud-logo-icon.svg
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    app.kubernetes.io/version: 30.0.10
    helm.sh/chart: nextcloud-6.6.10
  name: nextcloud
  namespace: nextcloud
spec:
  rules:
    - host: nextcloud.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: nextcloud
                port:
                  number: 8080
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - nextcloud.homelab.olav.ninja
      secretName: nextcloud-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.9.0
    helm.sh/chart: ollama-1.19.0
  name: ollama
  namespace: ollama
spec:
  rules:
    - host: ollama.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: ollama
                port:
                  number: 11434
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - ollama.homelab.olav.ninja
      secretName: ollama-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://avatars.githubusercontent.com/u/158137808
  labels:
    app.kubernetes.io/component: open-webui
    app.kubernetes.io/instance: openwebui
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 0.6.13
    helm.sh/chart: open-webui-6.19.0
  name: open-webui
  namespace: openwebui
spec:
  rules:
    - host: openwebui.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: open-webui
                port:
                  name: http
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - openwebui.homelab.olav.ninja
      secretName: openwebui-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Pingvin Share
    hajimari.io/enable: "true"
    hajimari.io/icon: https://user-images.githubusercontent.com/58886915/166198400-c2134044-1198-4647-a8b6-da9c4a204c68.svg
  labels:
    app.kubernetes.io/instance: pingvinshare
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pingvinshare
    helm.sh/chart: app-template-4.0.1
  name: pingvinshare
  namespace: pingvinshare
spec:
  rules:
    - host: pingvin.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: pingvinshare
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - pingvin.homelab.olav.ninja
      secretName: pingvinshare-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-35.4.0
  name: traefik
spec:
  controller: traefik.io/ingress-controller
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.5.0
    helm.sh/chart: postgresql-16.7.2
  name: gitea-postgresql
  namespace: gitea
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: valkey
    app.kubernetes.io/part-of: valkey
    app.kubernetes.io/version: 8.1.1
    helm.sh/chart: valkey-3.0.4
  name: gitea-valkey
  namespace: gitea
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: valkey
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.3
    helm.sh/chart: redis-20.13.2
  name: immich-redis
  namespace: immich
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.2.5
    helm.sh/chart: keycloak-24.7.3
  name: keycloak
  namespace: keycloak
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 7800
        - port: 8080
  podSelector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.4.0
    helm.sh/chart: postgresql-16.6.6
  name: keycloak-postgresql
  namespace: keycloak
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 3306
        - port: 3306
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mariadb
      app.kubernetes.io/version: 11.3.2
      helm.sh/chart: mariadb-18.2.0
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: Client
metadata:
  name: builtin-homelab-realm-management
  namespace: keycloak
spec:
  forProvider:
    clientId: realm-management
    realmIdRef:
      name: homelab
  managementPolicies:
    - Observe
  providerConfigRef:
    name: default
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: ClientScope
metadata:
  name: netbird-api
  namespace: netbird
spec:
  forProvider:
    consentScreenText: Netbird Management API
    includeInTokenScope: true
    name: netbird-api
    realmIdRef:
      name: homelab
---
apiVersion: pkg.crossplane.io/v1
kind: Provider
metadata:
  name: provider-keycloak
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/provider-keycloak:v2.1.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-auto-ready
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-auto-ready:v0.5.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-go-templating
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-go-templating:v0.10.0
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: immich-postgresql
  namespace: immich
spec:
  bootstrap:
    initdb:
      database: immich
      import:
        databases:
          - immich
        source:
          externalCluster: postgres-subchart
        type: microservice
      owner: immich
      postInitSQL:
        - CREATE EXTENSION IF NOT EXISTS "vectors";
        - CREATE EXTENSION IF NOT EXISTS "cube" CASCADE;
        - CREATE EXTENSION IF NOT EXISTS "earthdistance" CASCADE;
      secret:
        name: immich-postgresql-user
  externalClusters:
    - connectionParameters:
        dbname: immich
        host: immich-postgresql.immich.svc.cluster.local
        user: immich
      name: postgres-subchart
      password:
        key: password
        name: immich-postgresql
  imageName: ghcr.io/tensorchord/cloudnative-pgvecto.rs:16.5-v0.3.0@sha256:be3f025d79aa1b747817f478e07e71be43236e14d00d8a9eb3914146245035ba
  instances: 1
  managed:
    roles:
      - login: true
        name: immich
        superuser: true
  postgresql:
    parameters:
      timezone: Europe/Oslo
    shared_preload_libraries:
      - vectors.so
  storage:
    pvcTemplate:
      accessModes:
        - ReadWriteOnce
      storageClassName: proxmox-csi
      volumeName: immich-postgresql-cnpg
    size: 10G
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: pingvinshare-postgresql
  namespace: pingvinshare
spec:
  bootstrap:
    initdb:
      database: pingvinshare
      owner: pingvinshare
  instances: 1
  postgresql:
    parameters:
      timezone: Europe/Oslo
  storage:
    pvcTemplate:
      accessModes:
        - ReadWriteOnce
      storageClassName: proxmox-csi
      volumeName: pingvinshare-postgresql
    size: 10G
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: KeystoreRsa
metadata:
  name: jwt-signing-certificate
  namespace: keycloak
spec:
  forProvider:
    active: true
    algorithm: RS256
    certificateSecretRef:
      key: tls.crt
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    enabled: true
    name: jwt-signing-certificate
    priority: 110
    privateKeySecretRef:
      key: tls.key
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    providerId: rsa
    realmIdRef:
      name: homelab
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: Realm
metadata:
  name: homelab
  namespace: keycloak
spec:
  forProvider:
    realm: homelab
---
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: csi.proxmox.sinextra.dev
spec:
  attachRequired: true
  podInfoOnMount: true
  storageCapacity: true
  volumeLifecycleModes:
    - Persistent
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: gitea-ssh
  namespace: gitea
spec:
  entryPoints:
    - ssh
  routes:
    - match: HostSNI(`*`)
      services:
        - name: gitea-ssh
          port: 22
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: external-cluster-ingressroute
  namespace: traefik
spec:
  entryPoints:
    - webpublic
  routes:
    - match: HostSNIRegexp(`jiyoung.cloud|{subdomain:[a-z]+}.jiyoung.cloud`)
      services:
        - name: external-cluster
          port: ingress-port
  tls:
    passthrough: true
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: login-redirect-keycloak
  namespace: gitea
spec:
  redirectRegex:
    permanent: false
    regex: ^(https?://[^/]+)/user/login(\?.*)?$
    replacement: ${1}/user/oauth2/keycloak${2}
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: ipallowlist
  namespace: keycloak
spec:
  ipAllowList:
    sourceRange:
      - 192.168.0.1/24
      - 10.0.0.0/8
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: securityheaders
  namespace: traefik
spec:
  headers:
    customFrameOptionsValue: SAMEORIGIN
    forceSTSHeader: false
    referrerPolicy: same-origin
    sslRedirect: true
    stsPreload: false
    stsSeconds: 15552000
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: jiyoung
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - valueSecretRef:
          key: jiyoung
          name: keycloak-initial-passwords
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: jiyoung
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: olav
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - valueSecretRef:
          key: olav
          name: keycloak-initial-passwords
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: olav
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.23.8
    helm.sh/chart: gitea-12.0.0
    version: 1.23.8
  name: gitea-test-connection
  namespace: gitea
spec:
  containers:
    - args:
        - gitea-http:3000
      command:
        - wget
      image: busybox:latest
      name: wget
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test
  labels:
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ollama
    app.kubernetes.io/version: 0.9.0
    helm.sh/chart: ollama-1.19.0
  name: ollama-test-connection
  namespace: ollama
spec:
  containers:
    - args:
        - ollama:11434
      command:
        - wget
      image: busybox
      name: wget
  restartPolicy: Never
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /mutate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    rules:
      - apiGroups:
          - cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
        resources:
          - certificaterequests
    sideEffects: None
    timeoutSeconds: 30
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-mutating-webhook-configuration
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-backup
        port: 443
    failurePolicy: Fail
    name: mbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - backups
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-cluster
        port: 443
    failurePolicy: Fail
    name: mcluster.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - clusters
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-database
        port: 443
    failurePolicy: Fail
    name: mdatabase.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - databases
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /mutate-postgresql-cnpg-io-v1-scheduledbackup
        port: 443
    failurePolicy: Fail
    name: mscheduledbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - scheduledbackups
    sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.18.0
    helm.sh/chart: cert-manager-v1.18.0
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /validate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    namespaceSelector:
      matchExpressions:
        - key: cert-manager.io/disable-validation
          operator: NotIn
          values:
            - "true"
    rules:
      - apiGroups:
          - cert-manager.io
          - acme.cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - '*/*'
    sideEffects: None
    timeoutSeconds: 30
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/instance: cnpg
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cloudnative-pg
    app.kubernetes.io/version: 1.26.0
    helm.sh/chart: cloudnative-pg-0.24.0
  name: cnpg-validating-webhook-configuration
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-backup
        port: 443
    failurePolicy: Fail
    name: vbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - backups
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-cluster
        port: 443
    failurePolicy: Fail
    name: vcluster.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - clusters
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-scheduledbackup
        port: 443
    failurePolicy: Fail
    name: vscheduledbackup.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - scheduledbackups
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-database
        port: 443
    failurePolicy: Fail
    name: vdatabase.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - databases
    sideEffects: None
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cnpg-webhook-service
        namespace: cnpg-system
        path: /validate-postgresql-cnpg-io-v1-pooler
        port: 443
    failurePolicy: Fail
    name: vpooler.cnpg.io
    rules:
      - apiGroups:
          - postgresql.cnpg.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - poolers
    sideEffects: None
