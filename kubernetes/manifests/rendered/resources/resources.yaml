apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
---
apiVersion: v1
kind: Namespace
metadata:
  name: coturn
---
apiVersion: v1
kind: Namespace
metadata:
  name: crossplane
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: csi-proxmox
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: Namespace
metadata:
  name: hajimari
---
apiVersion: v1
kind: Namespace
metadata:
  name: immich
---
apiVersion: v1
kind: Namespace
metadata:
  name: keycloak
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: baseline
  name: netbird
---
apiVersion: v1
kind: Namespace
metadata:
  name: nextcloud
---
apiVersion: v1
kind: Namespace
metadata:
  name: sealed-secrets
---
apiVersion: v1
kind: Namespace
metadata:
  name: traefik
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: proxmox-csi
mountOptions:
  - noatime
parameters:
  cache: writethrough
  csi.storage.k8s.io/fstype: ext4
  ssd: "true"
  storage: local-zfs
provisioner: csi.proxmox.sinextra.dev
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-startupapicheck
  namespace: cert-manager
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook
  namespace: cert-manager
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.2-r8
    helm.sh/chart: coturn-1.0.0
  name: coturn
  namespace: coturn
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: rbac-manager
  namespace: crossplane
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.5.20
  name: gitea-postgresql
  namespace: gitea
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak
  namespace: keycloak
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.2.0
    helm.sh/chart: postgresql-16.2.5
  name: keycloak-postgresql
  namespace: keycloak
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-envoy
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-operator
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hubble-generate-certs
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.7.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.11
  name: proxmox-cloud-controller-manager
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-signal
  namespace: netbird
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.1.3
    helm.sh/chart: netbird-dashboard-1.0.0
  name: netbird-dashboard
  namespace: netbird
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets
  namespace: sealed-secrets
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-33.2.1
  name: traefik
  namespace: traefik
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-cainjector-leader-election
      - cert-manager-cainjector-leader-election-core
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificaterequests
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-tokenrequest
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
rules:
  - apiGroups:
      - ""
    resourceNames:
      - cert-manager-webhook-ca
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager:leaderelection
  namespace: cert-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - cert-manager-controller
    resources:
      - leases
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - watch
      - list
      - delete
      - update
      - create
  - apiGroups:
      - storage.k8s.io
    resources:
      - csistoragecapacities
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - ""
    resourceNames:
      - hubble-server-certs
      - hubble-relay-client-certs
      - hubble-relay-server-certs
      - hubble-metrics-server-certs
      - hubble-ui-client-certs
    resources:
      - secrets
    verbs:
      - update
  - apiGroups:
      - ""
    resourceNames:
      - cilium-ca
    resources:
      - secrets
    verbs:
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - sealed-secrets-key
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
rules:
  - apiGroups:
      - ""
    resourceNames:
      - 'http:sealed-secrets:'
      - http:sealed-secrets:http
      - sealed-secrets
    resources:
      - services/proxy
    verbs:
      - create
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
rules:
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
      - create
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - persistentvolumeclaims/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - csinodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattachments
    verbs:
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - volumeattachments/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - create
      - update
      - patch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiregistration.k8s.io
    resources:
      - apiservices
    verbs:
      - get
      - list
      - watch
      - update
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
  name: cert-manager-cluster-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-approve:cert-manager-io
rules:
  - apiGroups:
      - cert-manager.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - approve
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-certificates
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificates/status
      - certificaterequests
      - certificaterequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/finalizers
      - certificaterequests/finalizers
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
    verbs:
      - create
      - delete
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-certificatesigningrequests
rules:
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests
    verbs:
      - get
      - list
      - watch
      - update
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests/status
    verbs:
      - update
      - patch
  - apiGroups:
      - certificates.k8s.io
    resourceNames:
      - issuers.cert-manager.io/*
      - clusterissuers.cert-manager.io/*
    resources:
      - signers
    verbs:
      - sign
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-challenges
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - challenges/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - httproutes
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  - apiGroups:
      - route.openshift.io
    resources:
      - routes/custom-host
    verbs:
      - create
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-clusterissuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - clusterissuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-ingress-shim
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
    verbs:
      - create
      - update
      - delete
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
      - clusterissuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/finalizers
    verbs:
      - update
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways
      - httproutes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gateways/finalizers
      - httproutes/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-issuers
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
      - issuers/status
    verbs:
      - update
      - patch
  - apiGroups:
      - cert-manager.io
    resources:
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-orders
rules:
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - orders/status
    verbs:
      - update
      - patch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders
      - challenges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cert-manager.io
    resources:
      - clusterissuers
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
    verbs:
      - create
      - delete
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - orders/finalizers
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: cert-manager-edit
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates/status
    verbs:
      - update
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - create
      - delete
      - deletecollection
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-cluster-reader: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: cert-manager-view
rules:
  - apiGroups:
      - cert-manager.io
    resources:
      - certificates
      - certificaterequests
      - issuers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - acme.cert-manager.io
    resources:
      - challenges
      - orders
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook:subjectaccessreviews
rules:
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
rules:
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - services
      - pods
      - endpoints
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
      - list
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - list
      - watch
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumbgppeeringpolicies
      - ciliumbgpnodeconfigs
      - ciliumbgpadvertisements
      - ciliumbgppeerconfigs
      - ciliumclusterwideenvoyconfigs
      - ciliumclusterwidenetworkpolicies
      - ciliumegressgatewaypolicies
      - ciliumendpoints
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumidentities
      - ciliumlocalredirectpolicies
      - ciliumnetworkpolicies
      - ciliumnodes
      - ciliumnodeconfigs
      - ciliumcidrgroups
      - ciliuml2announcementpolicies
      - ciliumpodippools
    verbs:
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
      - ciliumendpoints
      - ciliumnodes
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
    verbs:
      - delete
      - get
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
      - ciliumnodes/status
    verbs:
      - get
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints/status
      - ciliumendpoints
      - ciliuml2announcementpolicies/status
      - ciliumbgpnodeconfigs/status
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - ""
    resourceNames:
      - cilium-config
    resources:
      - configmaps
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services/status
    verbs:
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies
      - ciliumclusterwidenetworkpolicies
    verbs:
      - create
      - update
      - deletecollection
      - patch
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnetworkpolicies/status
      - ciliumclusterwidenetworkpolicies/status
    verbs:
      - patch
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpoints
      - ciliumidentities
    verbs:
      - delete
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumidentities
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
  - apiGroups:
      - cilium.io
    resources:
      - ciliumnodes/status
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumendpointslices
      - ciliumenvoyconfigs
      - ciliumbgppeerconfigs
      - ciliumbgpadvertisements
      - ciliumbgpnodeconfigs
    verbs:
      - create
      - update
      - get
      - list
      - watch
      - delete
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - create
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resourceNames:
      - ciliumloadbalancerippools.cilium.io
      - ciliumbgppeeringpolicies.cilium.io
      - ciliumbgpclusterconfigs.cilium.io
      - ciliumbgppeerconfigs.cilium.io
      - ciliumbgpadvertisements.cilium.io
      - ciliumbgpnodeconfigs.cilium.io
      - ciliumbgpnodeconfigoverrides.cilium.io
      - ciliumclusterwideenvoyconfigs.cilium.io
      - ciliumclusterwidenetworkpolicies.cilium.io
      - ciliumegressgatewaypolicies.cilium.io
      - ciliumendpoints.cilium.io
      - ciliumendpointslices.cilium.io
      - ciliumenvoyconfigs.cilium.io
      - ciliumexternalworkloads.cilium.io
      - ciliumidentities.cilium.io
      - ciliumlocalredirectpolicies.cilium.io
      - ciliumnetworkpolicies.cilium.io
      - ciliumnodes.cilium.io
      - ciliumnodeconfigs.cilium.io
      - ciliumcidrgroups.cilium.io
      - ciliuml2announcementpolicies.cilium.io
      - ciliumpodippools.cilium.io
    resources:
      - customresourcedefinitions
    verbs:
      - update
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools
      - ciliumpodippools
      - ciliumbgppeeringpolicies
      - ciliumbgpclusterconfigs
      - ciliumbgpnodeconfigoverrides
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - cilium.io
    resources:
      - ciliumpodippools
    verbs:
      - create
  - apiGroups:
      - cilium.io
    resources:
      - ciliumloadbalancerippools/status
    verbs:
      - patch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - update
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-crossplane: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-admin
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-browse: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-browse
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-edit: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-edit
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-rbac-manager
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - compositeresourcedefinitions/finalizers
    verbs:
      - update
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - providerrevisions/finalizers
    verbs:
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - escalate
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
    verbs:
      - bind
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
    verbs:
      - '*'
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-view: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-view
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    rbac.crossplane.io/aggregate-to-admin: "true"
  name: crossplane:aggregate-to-admin
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
      - namespaces
    verbs:
      - '*'
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterroles
      - roles
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - rolebindings
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    rbac.crossplane.io/aggregate-to-browse: "true"
  name: crossplane:aggregate-to-browse
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    rbac.crossplane.io/aggregate-to-edit: "true"
  name: crossplane:aggregate-to-edit
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    rbac.crossplane.io/aggregate-to-view: "true"
  name: crossplane:aggregate-to-view
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - pkg.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.crossplane.io/aggregate-to-allowed-provider-permissions: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane:allowed-provider-permissions
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    crossplane.io/scope: system
    helm.sh/chart: crossplane-1.18.2
    rbac.crossplane.io/aggregate-to-crossplane: "true"
  name: crossplane:system:aggregate-to-crossplane
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
      - customresourcedefinitions/status
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
      - services
    verbs:
      - '*'
  - apiGroups:
      - apiextensions.crossplane.io
      - pkg.crossplane.io
      - secrets.crossplane.io
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - extensions
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - delete
      - watch
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - watch
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
rules:
  - apiGroups:
      - ""
      - extensions
      - networking.k8s.io
      - discovery.k8s.io
    resources:
      - ingresses
      - namespaces
      - endpointslices
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: keycloak-xbuiltinobjects-create
rules:
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - keycloak.crossplane.io
    resources:
      - xbuiltinobjects
    verbs:
      - get
      - list
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets-unsealer
rules:
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - bitnami.com
    resources:
      - sealedsecrets/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - update
      - delete
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.7.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.11
  name: system:proxmox-cloud-controller-manager
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - create
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - create
      - get
  - apiGroups:
      - ""
    resources:
      - serviceaccounts/token
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-33.2.1
  name: traefik-traefik
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingressclasses
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.io
    resources:
      - ingressroutes
      - ingressroutetcps
      - ingressrouteudps
      - middlewares
      - middlewaretcps
      - serverstransports
      - serverstransporttcps
      - tlsoptions
      - tlsstores
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-cainjector:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cert-manager-tokenrequest
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-tokenrequest
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-5"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-startupapicheck:create-cert
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-startupapicheck:create-cert
subjects:
  - kind: ServiceAccount
    name: cert-manager-startupapicheck
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook:dynamic-serving
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager-webhook:dynamic-serving
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager:leaderelection
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cert-manager:leaderelection
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxmox-csi-plugin-controller
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-controller
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-config-agent
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cilium-config-agent
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: hubble-generate-certs
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hubble-generate-certs
subjects:
  - kind: ServiceAccount
    name: hubble-generate-certs
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: system:proxmox-cloud-controller-manager:extension-apiserver-authentication-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
  - kind: ServiceAccount
    name: proxmox-cloud-controller-manager
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets-key-admin
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-key-admin
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets-service-proxier
  namespace: sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: sealed-secrets-service-proxier
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:authenticated
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-cainjector
subjects:
  - kind: ServiceAccount
    name: cert-manager-cainjector
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-approve:cert-manager-io
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-approve:cert-manager-io
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-certificates
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificates
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: cert-manager
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-certificatesigningrequests
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-certificatesigningrequests
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-challenges
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-challenges
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-clusterissuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-clusterissuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-ingress-shim
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-ingress-shim
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-issuers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-issuers
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-controller-orders
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-controller-orders
subjects:
  - kind: ServiceAccount
    name: cert-manager
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook:subjectaccessreviews
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cert-manager-webhook:subjectaccessreviews
subjects:
  - kind: ServiceAccount
    name: cert-manager-webhook
    namespace: cert-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium
subjects:
  - kind: ServiceAccount
    name: cilium
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: cilium
  name: cilium-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium-operator
subjects:
  - kind: ServiceAccount
    name: cilium-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane
subjects:
  - kind: ServiceAccount
    name: crossplane
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: crossplane:masters
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
  name: crossplane-rbac-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: crossplane-rbac-manager
subjects:
  - kind: ServiceAccount
    name: rbac-manager
    namespace: crossplane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hajimari
subjects:
  - kind: ServiceAccount
    name: hajimari
    namespace: hajimari
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: keycloak-xbuiltinobjects-create
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: keycloak-xbuiltinobjects-create
subjects:
  - kind: ServiceAccount
    name: keycloak
    namespace: keycloak
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oidc-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: oidc:olav
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxmox-csi-plugin-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxmox-csi-plugin-controller
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-controller
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxmox-csi-plugin-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxmox-csi-plugin-node
subjects:
  - kind: ServiceAccount
    name: proxmox-csi-plugin-node
    namespace: csi-proxmox
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets-sealed-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sealed-secrets-unsealer
subjects:
  - kind: ServiceAccount
    name: sealed-secrets
    namespace: sealed-secrets
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:proxmox-cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:proxmox-cloud-controller-manager
subjects:
  - kind: ServiceAccount
    name: proxmox-cloud-controller-manager
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-33.2.1
  name: traefik-traefik
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-traefik
subjects:
  - kind: ServiceAccount
    name: traefik
    namespace: traefik
---
apiVersion: v1
data:
  turnserver.conf: "# Coturn TURN SERVER configuration file\n#\n# Boolean values note: where a boolean value is supposed to be used,\n# you can use '0', 'off', 'no', 'false', or 'f' as 'false',\n# and you can use '1', 'on', 'yes', 'true', or 't' as 'true'\n# If the value is missing, then it means 'true' by default.\n#\n\n# Listener interface device (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#listening-device=eth0\n\n# TURN listener port for UDP and TCP (Default: 3478).\n# Note: actually, TLS & DTLS sessions can connect to the\n# \"plain\" TCP & UDP port(s), too - if allowed by configuration.\n#\n#listening-port=3478\n\n# TURN listener port for TLS (Default: 5349).\n# Note: actually, \"plain\" TCP & UDP sessions can connect to the TLS & DTLS\n# port(s), too - if allowed by configuration. The TURN server\n# \"automatically\" recognizes the type of traffic. Actually, two listening\n# endpoints (the \"plain\" one and the \"tls\" one) are equivalent in terms of\n# functionality; but Coturn keeps both endpoints to satisfy the RFC 5766 specs.\n# For secure TCP connections, Coturn currently supports SSL version 3 and\n# TLS version 1.0, 1.1 and 1.2.\n# For secure UDP connections, Coturn supports DTLS version 1.\n#\n#tls-listening-port=5349\n\n# Alternative listening port for UDP and TCP listeners;\n# default (or zero) value means \"listening port plus one\".\n# This is needed for RFC 5780 support\n# (STUN extension specs, NAT behavior discovery). The TURN Server\n# supports RFC 5780 only if it is started with more than one\n# listening IP address of the same family (IPv4 or IPv6).\n# RFC 5780 is supported only by UDP protocol, other protocols\n# are listening to that endpoint only for \"symmetry\".\n#\n#alt-listening-port=0\n\n# Alternative listening port for TLS and DTLS protocols.\n# Default (or zero) value means \"TLS listening port plus one\".\n#\n#alt-tls-listening-port=0\n\n# Some network setups will require using a TCP reverse proxy in front\n# of the STUN server. If the proxy port option is set a single listener\n# is started on the given port that accepts connections using the\n# haproxy proxy protocol v2.\n# (https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)\n#\n#tcp-proxy-port=5555\n\n# Listener IP address of relay server. Multiple listeners can be specified.\n# If no IP(s) specified in the config file or in the command line options,\n# then all IPv4 and IPv6 system IPs will be used for listening.\n#\n#listening-ip=172.17.19.101\n#listening-ip=10.207.21.238\n#listening-ip=2607:f0d0:1002:51::4\n\n# Auxiliary STUN/TURN server listening endpoint.\n# Aux servers have almost full TURN and STUN functionality.\n# The (minor) limitations are:\n#\n# 1) Auxiliary servers do not have alternative ports and\n# they do not support STUN RFC 5780 functionality (CHANGE REQUEST).\n#\n# 2) Auxiliary servers also are never returning ALTERNATIVE-SERVER reply.\n#\n# Valid formats are 1.2.3.4:5555 for IPv4 and [1:2::3:4]:5555 for IPv6.\n#\n# There may be multiple aux-server options, each will be used for listening\n# to client requests.\n#\n#aux-server=172.17.19.110:33478\n#aux-server=[2607:f0d0:1002:51::4]:33478\n\n# (recommended for older Linuxes only)\n# Automatically balance UDP traffic over auxiliary servers (if configured).\n# The load balancing is using the ALTERNATE-SERVER mechanism.\n# The TURN client must support 300 ALTERNATE-SERVER response for this\n# functionality.\n#\n#udp-self-balance\n\n# Relay interface device for relay sockets (optional, Linux only).\n# NOT RECOMMENDED.\n#\n#relay-device=eth1\n\n# Relay address (the local IP address that will be used to relay the\n# packets to the peer).\n# Multiple relay addresses may be used.\n# The same IP(s) can be used as both listening IP(s) and relay IP(s).\n#\n# If no relay IP(s) specified, then the turnserver will apply the default\n# policy: it will decide itself which relay addresses to be used, and it\n# will always be using the client socket IP address as the relay IP address\n# of the TURN session (if the requested relay address family is the same\n# as the family of the client socket).\n#\n#relay-ip=172.17.19.105\n#relay-ip=2607:f0d0:1002:51::5\n\n# For Amazon EC2 users:\n#\n# TURN Server public/private address mapping, if the server is behind NAT.\n# In that situation, if a -X is used in form \"-X <ip>\" then that ip will be reported\n# as relay IP address of all allocations. This scenario works only in a simple case\n# when one single relay address is be used, and no RFC5780 functionality is required.\n# That single relay address must be mapped by NAT to the 'external' IP.\n# The \"external-ip\" value, if not empty, is returned in XOR-RELAYED-ADDRESS field.\n# For that 'external' IP, NAT must forward ports directly (relayed port 12345\n# must be always mapped to the same 'external' port 12345).\n#\n# In more complex case when more than one IP address is involved,\n# that option must be used several times, each entry must\n# have form \"-X <public-ip/private-ip>\", to map all involved addresses.\n# RFC5780 NAT discovery STUN functionality will work correctly,\n# if the addresses are mapped properly, even when the TURN server itself\n# is behind A NAT.\n#\n# By default, this value is empty, and no address mapping is used.\n#\n#external-ip=60.70.80.91\n#\n#OR:\n#\n#external-ip=60.70.80.91/172.17.19.101\n#external-ip=60.70.80.92/172.17.19.102\n\n\n# Number of the relay threads to handle the established connections\n# (in addition to authentication thread and the listener thread).\n# If explicitly set to 0 then application runs relay process in a\n# single thread, in the same thread with the listener process\n# (the authentication thread will still be a separate thread).\n#\n# If this parameter is not set, then the default OS-dependent\n# thread pattern algorithm will be employed. Usually the default\n# algorithm is optimal, so you have to change this option\n# if you want to make some fine tweaks.\n#\n# In the older systems (Linux kernel before 3.9),\n# the number of UDP threads is always one thread per network listening\n# endpoint - including the auxiliary endpoints - unless 0 (zero) or\n# 1 (one) value is set.\n#\n#relay-threads=0\n\n# Lower and upper bounds of the UDP relay endpoints:\n# (default values are 49152 and 65535)\n#\n#min-port=49152\n#max-port=65535\n\n# Uncomment to run TURN server in 'normal' 'moderate' verbose mode.\n# By default the verbose mode is off.\n#verbose\n\n# Uncomment to run TURN server in 'extra' verbose mode.\n# This mode is very annoying and produces lots of output.\n# Not recommended under normal circumstances.\n#\n#Verbose\n\n# Uncomment to use fingerprints in the TURN messages.\n# By default the fingerprints are off.\n#\n#fingerprint\n\n# Uncomment to use long-term credential mechanism.\n# By default no credentials mechanism is used (any user allowed).\n#\n#lt-cred-mech\n\n# This option is the opposite of lt-cred-mech.\n# (TURN Server with no-auth option allows anonymous access).\n# If neither option is defined, and no users are defined,\n# then no-auth is default. If at least one user is defined,\n# in this file, in command line or in usersdb file, then\n# lt-cred-mech is default.\n#\n#no-auth\n\n# Enable prometheus exporter\n# If enabled the turnserver will expose an endpoint with stats on a prometheus format\n# this endpoint is listening on a different port to not conflict with other configurations.\n#\n# You can simply run the turnserver and access the port 9641 and path /metrics\n#\n# For more info on the prometheus exporter and metrics\n# https://prometheus.io/docs/introduction/overview/\n# https://prometheus.io/docs/concepts/data_model/\n#\n#prometheus\n\n# TURN REST API flag.\n# (Time Limited Long Term Credential)\n# Flag that sets a special authorization option that is based upon authentication secret.\n#\n# This feature's purpose is to support \"TURN Server REST API\", see\n# \"TURN REST API\" link in the project's page\n# https://github.com/coturn/coturn/\n#\n# This option is used with timestamp:\n#\n# usercombo -> \"timestamp:userid\"\n# turn user -> usercombo\n# turn password -> base64(hmac(secret key, usercombo))\n#\n# This allows TURN credentials to be accounted for a specific user id.\n# If you don't have a suitable id, then the timestamp alone can be used.\n# This option is enabled by turning on secret-based authentication.\n# The actual value of the secret is defined either by the option static-auth-secret,\n# or can be found in the turn_secret table in the database (see below).\n#\n# Read more about it:\n#  - https://tools.ietf.org/html/draft-uberti-behave-turn-rest-00\n#  - https://www.ietf.org/proceedings/87/slides/slides-87-behave-10.pdf\n#\n# Be aware that use-auth-secret overrides some parts of lt-cred-mech.\n# The use-auth-secret feature depends internally on lt-cred-mech, so if you set\n# this option then it automatically enables lt-cred-mech internally\n# as if you had enabled both.\n#\n# Note that you can use only one auth mechanism at the same time! This is because,\n# both mechanisms conduct username and password validation in different ways.\n#\n# Use either lt-cred-mech or use-auth-secret in the conf\n# to avoid any confusion.\n#\n#use-auth-secret\n\n# 'Static' authentication secret value (a string) for TURN REST API only.\n# If not set, then the turn server\n# will try to use the 'dynamic' value in the turn_secret table\n# in the user database (if present). The database-stored  value can be changed on-the-fly\n# by a separate program, so this is why that mode is considered 'dynamic'.\n#\n#static-auth-secret=north\n\n# Server name used for\n# the oAuth authentication purposes.\n# The default value is the realm name.\n#\n#server-name=blackdow.carleon.gov\n\n# Flag that allows oAuth authentication.\n#\n#oauth\n\n# 'Static' user accounts for the long term credentials mechanism, only.\n# This option cannot be used with TURN REST API.\n# 'Static' user accounts are NOT dynamically checked by the turnserver process,\n# so they can NOT be changed while the turnserver is running.\n#\n#user=username1:key1\n#user=username2:key2\n# OR:\n#user=username1:password1\n#user=username2:password2\n#\n# Keys must be generated by turnadmin utility. The key value depends\n# on user name, realm, and password:\n#\n# Example:\n# $ turnadmin -k -u ninefingers -r north.gov -p youhavetoberealistic\n# Output: 0xbc807ee29df3c9ffa736523fb2c4e8ee\n# ('0x' in the beginning of the key is what differentiates the key from\n# password. If it has 0x then it is a key, otherwise it is a password).\n#\n# The corresponding user account entry in the config file will be:\n#\n#user=ninefingers:0xbc807ee29df3c9ffa736523fb2c4e8ee\n# Or, equivalently, with open clear password (less secure):\n#user=ninefingers:youhavetoberealistic\n#\n\n# SQLite database file name.\n#\n# The default file name is /var/db/turndb or /usr/local/var/db/turndb or\n# /var/lib/turn/turndb.\n#\n#userdb=/var/db/turndb\n\n# PostgreSQL database connection string in the case that you are using PostgreSQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# See http://www.postgresql.org/docs/8.4/static/libpq-connect.html for 8.x PostgreSQL\n# versions connection string format, see\n# http://www.postgresql.org/docs/9.2/static/libpq-connect.html#LIBPQ-CONNSTRING\n# for 9.x and newer connection string formats.\n#\n#psql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> connect_timeout=30\"\n\n# MySQL database connection string in the case that you are using MySQL\n# as the user database.\n# This database can be used for the long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n#\n# Optional connection string parameters for the secure communications (SSL):\n# ca, capath, cert, key, cipher\n# (see http://dev.mysql.com/doc/refman/5.1/en/ssl-options.html for the\n# command options description).\n#\n# Use the string format below (space separated parameters, all optional):\n#\n#mysql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> port=<port> connect_timeout=<seconds> read_timeout=<seconds>\"\n\n# If you want to use an encrypted password in the MySQL connection string,\n# then set the MySQL password encryption secret key file with this option.\n#\n# Warning: If this option is set, then the mysql password must be set in \"mysql-userdb\" in an encrypted format!\n# If you want to use a cleartext password then do not set this option!\n#\n# This is the file path for the aes encrypted secret key used for password encryption.\n#\n#secret-key-file=/path/\n\n# MongoDB database connection string in the case that you are using MongoDB\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format described at http://hergert.me/docs/mongo-c-driver/mongoc_uri.html\n#\n#mongo-userdb=\"mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\"\n\n# Redis database connection string in the case that you are using Redis\n# as the user database.\n# This database can be used for long-term credential mechanism\n# and it can store the secret value for secret-based timed authentication in TURN REST API.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-userdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Redis status and statistics database connection string, if used (default - empty, no Redis stats DB used).\n# This database keeps allocations status information, and it can be also used for publishing\n# and delivering traffic and allocation event notifications.\n# The connection string has the same parameters as redis-userdb connection string.\n# Use the string format below (space separated parameters, all optional):\n#\n#redis-statsdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# The default realm to be used for the users when no explicit\n# origin/realm relationship is found in the database, or if the TURN\n# server is not using any database (just the commands-line settings\n# and the userdb file). Must be used with long-term credentials\n# mechanism or with TURN REST API.\n#\n# Note: If the default realm is not specified, then realm falls back to the host domain name.\n#       If the domain name string is empty, or set to '(None)', then it is initialized as an empty string.\n#\n#realm=mycompany.org\n\n# This flag sets the origin consistency\n# check. Across the session, all requests must have the same\n# main ORIGIN attribute value (if the ORIGIN was\n# initially used by the session).\n#\n#check-origin-consistency\n\n# Per-user allocation quota.\n# default value is 0 (no quota, unlimited number of sessions per user).\n# This option can also be set through the database, for a particular realm.\n#\n#user-quota=0\n\n# Total allocation quota.\n# default value is 0 (no quota).\n# This option can also be set through the database, for a particular realm.\n#\n#total-quota=0\n\n# Max bytes-per-second bandwidth a TURN session is allowed to handle\n# (input and output network streams are treated separately). Anything above\n# that limit will be dropped or temporarily suppressed (within\n# the available buffer limits).\n# This option can also be set through the database, for a particular realm.\n#\n#max-bps=0\n\n#\n# Maximum server capacity.\n# Total bytes-per-second bandwidth the TURN server is allowed to allocate\n# for the sessions, combined (input and output network streams are treated separately).\n#\n#bps-capacity=0\n\n# Uncomment if no UDP client listener is desired.\n# By default UDP client listener is always started.\n#\n#no-udp\n\n# Uncomment if no TCP client listener is desired.\n# By default TCP client listener is always started.\n#\n#no-tcp\n\n# Uncomment if no TLS client listener is desired.\n# By default TLS client listener is always started.\n#\n#no-tls\n\n# Uncomment if no DTLS client listener is desired.\n# By default DTLS client listener is always started.\n#\n#no-dtls\n\n# Uncomment if no UDP relay endpoints are allowed.\n# By default UDP relay endpoints are enabled (like in RFC 5766).\n#\n#no-udp-relay\n\n# Uncomment if no TCP relay endpoints are allowed.\n# By default TCP relay endpoints are enabled (like in RFC 6062).\n#\n#no-tcp-relay\n\n# Uncomment if extra security is desired,\n# with nonce value having a limited lifetime.\n# The nonce value is unique for a session.\n# Set this option to limit the nonce lifetime.\n# Set it to 0 for unlimited lifetime.\n# It defaults to 600 secs (10 min) if no value is provided. After that delay,\n# the client will get 438 error and will have to re-authenticate itself.\n#\n#stale-nonce=600\n\n# Uncomment if you want to set the maximum allocation\n# time before it has to be refreshed.\n# Default is 3600s.\n#\n#max-allocate-lifetime=3600\n\n\n# Uncomment to set the lifetime for the channel.\n# Default value is 600 secs (10 minutes).\n# This value MUST not be changed for production purposes.\n#\n#channel-lifetime=600\n\n# Uncomment to set the permission lifetime.\n# Default to 300 secs (5 minutes).\n# In production this value MUST not be changed,\n# however it can be useful for test purposes.\n#\n#permission-lifetime=300\n\n# Certificate file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#cert=/usr/local/etc/turn_server_cert.pem\n\n# Private key file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n#pkey=/usr/local/etc/turn_server_pkey.pem\n\n# Private key file password, if it is in encoded format.\n# This option has no default value.\n#\n#pkey-pwd=...\n\n# Allowed OpenSSL cipher list for TLS/DTLS connections.\n# Default value is \"DEFAULT\".\n#\n#cipher-list=\"DEFAULT\"\n\n# CA file in OpenSSL format.\n# Forces TURN server to verify the client SSL certificates.\n# By default this is not set: there is no default value and the client\n# certificate is not checked.\n#\n# Example:\n#CA-file=/etc/ssh/id_rsa.cert\n\n# Curve name for EC ciphers, if supported by OpenSSL\n# library (TLS and DTLS). The default value is prime256v1,\n# if pre-OpenSSL 1.0.2 is used. With OpenSSL 1.0.2+,\n# an optimal curve will be automatically calculated, if not defined\n# by this option.\n#\n#ec-curve-name=prime256v1\n\n# Use 566 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh566\n\n# Use 1066 bits predefined DH TLS key. Default size of the key is 2066.\n#\n#dh1066\n\n# Use custom DH TLS key, stored in PEM format in the file.\n# Flags --dh566 and --dh1066 are ignored when the DH key is taken from a file.\n#\n#dh-file=<DH-PEM-file-name>\n\n# Flag to prevent stdout log messages.\n# By default, all log messages go to both stdout and to\n# the configured log file. With this option everything will\n# go to the configured log only (unless the log file itself is stdout).\n#\n#no-stdout-log\n\n# Option to set the log file name.\n# By default, the turnserver tries to open a log file in\n# /var/log, /var/tmp, /tmp and the current directory\n# (Whichever file open operation succeeds first will be used).\n# With this option you can set the definite log file name.\n# The special names are \"stdout\" and \"-\" - they will force everything\n# to the stdout. Also, the \"syslog\" name will force everything to\n# the system log (syslog).\n# In the runtime, the logfile can be reset with the SIGHUP signal\n# to the turnserver process.\n#\n#log-file=/var/tmp/turn.log\n\n# Option to redirect all log output into system log (syslog).\n#\n#syslog\n\n# Set syslog facility for syslog messages\n# Default values is ''.\n#\n#syslog-facility=\"LOG_LOCAL1\"\n\n# This flag means that no log file rollover will be used, and the log file\n# name will be constructed as-is, without PID and date appendage.\n# This option can be used, for example, together with the logrotate tool.\n#\n#simple-log\n\n# Enable full ISO-8601 timestamp in all logs.\n#new-log-timestamp\n\n# Set timestamp format (in strftime(1) format). Depends on new-log-timestamp to be enabled.\n#new-log-timestamp-format \"%FT%T%z\"\n\n# Disabled by default binding logging in verbose log mode to avoid DoS attacks.\n# Enable binding logging and UDP endpoint logs in verbose log mode.\n#log-binding\n\n# Option to set the \"redirection\" mode. The value of this option\n# will be the address of the alternate server for UDP & TCP service in the form of\n# <ip>[:<port>]. The server will send this value in the attribute\n# ALTERNATE-SERVER, with error 300, on ALLOCATE request, to the client.\n# Client will receive only values with the same address family\n# as the client network endpoint address family.\n# See RFC 5389 and RFC 5766 for the description of ALTERNATE-SERVER functionality.\n# The client must use the obtained value for subsequent TURN communications.\n# If more than one --alternate-server option is provided, then the functionality\n# can be more accurately described as \"load-balancing\" than a mere \"redirection\".\n# If the port number is omitted, then the default port\n# number 3478 for the UDP/TCP protocols will be used.\n# Colon (:) characters in IPv6 addresses may conflict with the syntax of\n# the option. To alleviate this conflict, literal IPv6 addresses are enclosed\n# in square brackets in such resource identifiers, for example:\n# [2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478 .\n# Multiple alternate servers can be set. They will be used in the\n# round-robin manner. All servers in the pool are considered of equal weight and\n# the load will be distributed equally. For example, if you have 4 alternate servers,\n# then each server will receive 25% of ALLOCATE requests. A alternate TURN server\n# address can be used more than one time with the alternate-server option, so this\n# can emulate \"weighting\" of the servers.\n#\n# Examples:\n#alternate-server=1.2.3.4:5678\n#alternate-server=11.22.33.44:56789\n#alternate-server=5.6.7.8\n#alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to set alternative server for TLS & DTLS services in form of\n# <ip>:<port>. If the port number is omitted, then the default port\n# number 5349 for the TLS/DTLS protocols will be used. See the previous\n# option for the functionality description.\n#\n# Examples:\n#tls-alternate-server=1.2.3.4:5678\n#tls-alternate-server=11.22.33.44:56789\n#tls-alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to suppress TURN functionality, only STUN requests will be processed.\n# Run as STUN server only, all TURN requests will be ignored.\n# By default, this option is NOT set.\n#\n#stun-only\n\n# Option to hide software version. Enhance security when used in production.\n# Revealing the specific software version of the agent through the\n# SOFTWARE attribute might allow them to become more vulnerable to\n# attacks against software that is known to contain security holes.\n# Implementers SHOULD make usage of the SOFTWARE attribute a\n# configurable option (https://tools.ietf.org/html/rfc5389#section-16.1.2)\n#\n#no-software-attribute\n\n# Option to suppress STUN functionality, only TURN requests will be processed.\n# Run as TURN server only, all STUN requests will be ignored.\n# By default, this option is NOT set.\n#\n#no-stun\n\n# This is the timestamp/username separator symbol (character) in TURN REST API.\n# The default value is ':'.\n#\n#rest-api-separator=:\n\n# Flag that can be used to allow peers on the loopback addresses (127.x.x.x and ::1).\n# This is an extra security measure.\n#\n# (To avoid any security issue that allowing loopback access may raise,\n# the no-loopback-peers option is replaced by allow-loopback-peers.)\n#\n# Allow it only for testing in a development environment!\n# In production it adds a possible security vulnerability, so for security reasons\n# it is not allowed using it together with empty cli-password.\n#\n#allow-loopback-peers\n\n# Flag that can be used to disallow peers on well-known broadcast addresses (224.0.0.0 and above, and FFXX:*).\n# This is an extra security measure.\n#\n#no-multicast-peers\n\n# Option to set the max time, in seconds, allowed for full allocation establishment.\n# Default is 60 seconds.\n#\n#max-allocate-timeout=60\n\n# Option to allow or ban specific ip addresses or ranges of ip addresses.\n# If an ip address is specified as both allowed and denied, then the ip address is\n# considered to be allowed. This is useful when you wish to ban a range of ip\n# addresses, except for a few specific ips within that range.\n#\n# This can be used when you do not want users of the turn server to be able to access\n# machines reachable by the turn server, but would otherwise be unreachable from the\n# internet (e.g. when the turn server is sitting behind a NAT)\n#\n# Examples:\n# denied-peer-ip=83.166.64.0-83.166.95.255\n# allowed-peer-ip=83.166.68.45\n\n# File name to store the pid of the process.\n# Default is /var/run/turnserver.pid (if superuser account is used) or\n# /var/tmp/turnserver.pid .\n#\n#pidfile=\"/var/run/turnserver.pid\"\n\n# Require authentication of the STUN Binding request.\n# By default, the clients are allowed anonymous access to the STUN Binding functionality.\n#\n#secure-stun\n\n# Mobility with ICE (MICE) specs support.\n#\n#mobility\n\n# Allocate Address Family according (DEPRECATED and will be removed in favor of allocation-default-address-family)\n# If enabled then TURN server allocates address family according  the TURN\n# Client <=> Server communication address family.\n# (By default Coturn works according RFC 6156.)\n# !!Warning: Enabling this option breaks RFC6156 section-4.2 (violates use default IPv4)!!\n#\n#keep-address-family\n\n# TURN server allocates address family according TURN client requested address family.\n# If address family not requested explicitly by the client, then it falls back to this default.\n# The standard RFC explicitly define that this default must be IPv4, \n# so use other option values with care! \n# Possible values: \"ipv4\" or \"ipv6\" or \"keep\" \n# \"keep\" sets the allocation default address family according to \n# the TURN client allocation request connection address family.\n#\n#allocation-default-address-family=\"ipv4\"\n#allocation-default-address-family=\"ipv4\"\n\n# User name to run the process. After the initialization, the turnserver process\n# will attempt to change the current user ID to that user.\n#\n#proc-user=<user-name>\n\n# Group name to run the process. After the initialization, the turnserver process\n# will attempt to change the current group ID to that group.\n#\n#proc-group=<group-name>\n\n# Turn OFF the CLI support.\n# By default it is always ON.\n# See also options cli-ip and cli-port.\n#\n#no-cli\n\n#Local system IP address to be used for CLI server endpoint. Default value\n# is 127.0.0.1.\n#\n#cli-ip=127.0.0.1\n\n# CLI server port. Default is 5766.\n#\n#cli-port=5766\n\n# CLI access password. Default is empty (no password).\n# For the security reasons, it is recommended that you use the encrypted\n# form of the password (see the -P command in the turnadmin utility).\n#\n# Secure form for password 'qwerty':\n#\n#cli-password=$5$79a316b350311570$81df9cfb9af7f5e5a76eada31e7097b663a0670f99a3c07ded3f1c8e59c5658a\n#\n# Or unsecure form for the same password:\n#\n#cli-password=qwerty\n\n# Enable Web-admin support on https. By default it is Disabled.\n# If it is enabled it also enables a http a simple static banner page\n# with a small reminder that the admin page is available only on https.\n# Not supported if no-tls option used\n#\n#web-admin\n\n# Local system IP address to be used for Web-admin server endpoint. Default value is 127.0.0.1.\n#\n#web-admin-ip=127.0.0.1\n\n# Web-admin server port. Default is 8080.\n#\n#web-admin-port=8080\n\n# Web-admin server listen on STUN/TURN worker threads\n# By default it is disabled for security reasons! (Not recommended in any production environment!)\n#\n#web-admin-listen-on-workers\n\n# Redirect ACME, i.e. HTTP GET requests matching '^/.well-known/acme-challenge/(.*)' to '<URL>$1'.\n# Default is '', i.e. no special handling for such requests.\n#\n#acme-redirect=http://redirectserver/.well-known/acme-challenge/\n\n# Server relay. NON-STANDARD AND DANGEROUS OPTION.\n# Only for those applications when you want to run\n# server applications on the relay endpoints.\n# This option eliminates the IP permissions check on\n# the packets incoming to the relay endpoints.\n#\n#server-relay\n\n# Maximum number of output sessions in ps CLI command.\n# This value can be changed on-the-fly in CLI. The default value is 256.\n#\n#cli-max-output-sessions\n\n# Set network engine type for the process (for internal purposes).\n#\n#ne=[1|2|3]\n\n# Do not allow an TLS/DTLS version of protocol\n#\n#no-tlsv1\n#no-tlsv1_1\n#no-tlsv1_2\n\n# Disable RFC5780 (NAT behavior discovery).\n#\n# Originally, if there are more than one listener address from the same\n# address family, then by default the NAT behavior discovery feature enabled.\n# This option disables the original behavior, because the NAT behavior\n# discovery adds extra attributes to response, and this increase the\n# possibility of an amplification attack.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-rfc5780\n\n# Disable handling old STUN Binding requests and disable MAPPED-ADDRESS\n# attribute in binding response (use only the XOR-MAPPED-ADDRESS).\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nno-stun-backward-compatibility\n\n# Only send RESPONSE-ORIGIN attribute in binding response if RFC5780 is enabled.\n#\n# Strongly encouraged to use this option to decrease gain factor in STUN\n# binding responses.\n#\nresponse-origin-only-with-rfc5780"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.2-r8
    helm.sh/chart: coturn-1.0.0
  name: coturn
  namespace: coturn
---
apiVersion: v1
data:
  config.yaml: |-
    customApps:
    - apps:
      - icon: https://brands.home-assistant.io/homeassistant/icon.png
        name: Home Assistant
        url: http://homeassistant.homelab.olav.ninja:8123
      group: Applications
    darkTheme: tron
    defaultEnable: false
    globalBookmarks:
    - bookmarks:
      - name: Keycloak
        url: https://keycloak.homelab.olav.ninja
      - name: Netbird
        url: https://netbird.homelab.olav.ninja
      group: Utilities
    - bookmarks:
      - name: OpenWrt
        url: http://192.168.0.1
      - name: Proxmox
        url: https://192.168.0.190:8006
      group: Infrastructure
    - bookmarks:
      - name: Cloudflare Dashboard
        url: https://dash.cloudflare.com
      - name: Github Repo
        url: https://github.com/olav-st/homelab
      group: External
    instanceName: null
    lightTheme: gazette
    name: Olav
    namespaceSelector:
      any: true
      matchNames:
      - media
    title: Homelab
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari-settings
  namespace: hajimari
---
apiVersion: v1
data:
  immich-config.yaml: |
    placeholder: foo
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: immich
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-immich-config
  namespace: immich
---
apiVersion: v1
data:
  create-extensions.sql: |
    CREATE EXTENSION cube;
    CREATE EXTENSION earthdistance;
    CREATE EXTENSION vectors;
  immich-superuser.sql: "ALTER USER immich WITH SUPERUSER; \n"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql-init-scripts
  namespace: immich
---
apiVersion: v1
data:
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-configuration
  namespace: immich
---
apiVersion: v1
data:
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-health
  namespace: immich
---
apiVersion: v1
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-scripts
  namespace: immich
---
apiVersion: v1
data:
  JAVA_OPTS_APPEND: -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
  KC_BOOTSTRAP_ADMIN_USERNAME: admin
  KEYCLOAK_CACHE_STACK: kubernetes
  KEYCLOAK_CACHE_TYPE: ispn
  KEYCLOAK_DATABASE_HOST: keycloak-postgresql
  KEYCLOAK_DATABASE_NAME: bitnami_keycloak
  KEYCLOAK_DATABASE_PORT: "5432"
  KEYCLOAK_DATABASE_USER: bn_keycloak
  KEYCLOAK_ENABLE_HTTPS: "false"
  KEYCLOAK_ENABLE_STATISTICS: "false"
  KEYCLOAK_HOSTNAME: https://keycloak.homelab.olav.ninja/
  KEYCLOAK_HOSTNAME_STRICT: "false"
  KEYCLOAK_HTTP_PORT: "8080"
  KEYCLOAK_LOG_LEVEL: INFO
  KEYCLOAK_LOG_OUTPUT: default
  KEYCLOAK_PRODUCTION: "true"
  KEYCLOAK_PROXY_HEADERS: xforwarded
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak-env-vars
  namespace: keycloak
---
apiVersion: v1
data:
  agent-not-ready-taint-key: node.cilium.io/agent-not-ready
  arping-refresh-period: 30s
  auto-direct-node-routes: "false"
  bpf-events-drop-enabled: "true"
  bpf-events-policy-verdict-enabled: "true"
  bpf-events-trace-enabled: "true"
  bpf-lb-acceleration: disabled
  bpf-lb-external-clusterip: "false"
  bpf-lb-map-max: "65536"
  bpf-lb-sock: "false"
  bpf-lb-sock-terminate-pod-connections: "false"
  bpf-map-dynamic-size-ratio: "0.0025"
  bpf-policy-map-max: "16384"
  bpf-root: /sys/fs/bpf
  cgroup-root: /sys/fs/cgroup
  cilium-endpoint-gc-interval: 5m0s
  cluster-id: "0"
  cluster-name: default
  clustermesh-enable-endpoint-sync: "false"
  clustermesh-enable-mcs-api: "false"
  cni-exclusive: "true"
  cni-log-file: /var/run/cilium/cilium-cni.log
  custom-cni-conf: "false"
  datapath-mode: veth
  debug: "false"
  debug-verbose: ""
  direct-routing-skip-unreachable: "false"
  dnsproxy-enable-transparent-mode: "true"
  dnsproxy-socket-linger-timeout: "10"
  egress-gateway-reconciliation-trigger-interval: 1s
  enable-auto-protect-node-port-range: "true"
  enable-bpf-clock-probe: "false"
  enable-endpoint-health-checking: "true"
  enable-health-check-loadbalancer-ip: "false"
  enable-health-check-nodeport: "true"
  enable-health-checking: "true"
  enable-hubble: "true"
  enable-ipv4: "true"
  enable-ipv4-big-tcp: "false"
  enable-ipv4-masquerade: "true"
  enable-ipv6: "false"
  enable-ipv6-big-tcp: "false"
  enable-ipv6-masquerade: "true"
  enable-k8s-networkpolicy: "true"
  enable-k8s-terminating-endpoint: "true"
  enable-l2-announcements: "true"
  enable-l2-neigh-discovery: "true"
  enable-l7-proxy: "true"
  enable-local-redirect-policy: "false"
  enable-masquerade-to-route-source: "false"
  enable-metrics: "true"
  enable-node-selector-labels: "false"
  enable-policy: default
  enable-runtime-device-detection: "true"
  enable-sctp: "false"
  enable-svc-source-range-check: "true"
  enable-tcx: "true"
  enable-vtep: "false"
  enable-well-known-identities: "false"
  enable-xt-socket-fallback: "true"
  envoy-base-id: "0"
  envoy-keep-cap-netbindservice: "false"
  external-envoy-proxy: "true"
  hubble-disable-tls: "false"
  hubble-export-file-max-backups: "5"
  hubble-export-file-max-size-mb: "10"
  hubble-listen-address: :4244
  hubble-socket-path: /var/run/cilium/hubble.sock
  hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt
  hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt
  hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key
  identity-allocation-mode: crd
  identity-gc-interval: 15m0s
  identity-heartbeat-timeout: 30m0s
  install-no-conntrack-iptables-rules: "false"
  ipam: kubernetes
  ipam-cilium-node-update-rate: 15s
  k8s-client-burst: "100"
  k8s-client-qps: "50"
  k8s-require-ipv4-pod-cidr: "false"
  k8s-require-ipv6-pod-cidr: "false"
  kube-proxy-replacement: "true"
  kube-proxy-replacement-healthz-bind-address: ""
  max-connected-clusters: "255"
  mesh-auth-enabled: "true"
  mesh-auth-gc-interval: 5m0s
  mesh-auth-queue-size: "1024"
  mesh-auth-rotated-identities-queue-size: "1024"
  monitor-aggregation: medium
  monitor-aggregation-flags: all
  monitor-aggregation-interval: 5s
  nat-map-stats-entries: "32"
  nat-map-stats-interval: 30s
  node-port-bind-protection: "true"
  nodeport-addresses: ""
  nodes-gc-interval: 5m0s
  operator-api-serve-addr: 127.0.0.1:9234
  operator-prometheus-serve-addr: :9963
  policy-cidr-match-mode: ""
  preallocate-bpf-maps: "false"
  procfs: /host/proc
  proxy-connect-timeout: "2"
  proxy-idle-timeout-seconds: "60"
  proxy-initial-fetch-timeout: "30"
  proxy-max-connection-duration-seconds: "0"
  proxy-max-requests-per-connection: "0"
  proxy-xff-num-trusted-hops-egress: "0"
  proxy-xff-num-trusted-hops-ingress: "0"
  remove-cilium-node-taints: "true"
  routing-mode: tunnel
  service-no-backend-response: reject
  set-cilium-is-up-condition: "true"
  set-cilium-node-taints: "true"
  synchronize-k8s-nodes: "true"
  tofqdns-dns-reject-response-code: refused
  tofqdns-enable-dns-compression: "true"
  tofqdns-endpoint-max-ip-per-hostname: "50"
  tofqdns-idle-connection-grace-period: 0s
  tofqdns-max-deferred-connection-deletes: "10000"
  tofqdns-proxy-response-max-delay: 100ms
  tunnel-protocol: vxlan
  unmanaged-pod-watcher-interval: "15"
  vtep-cidr: ""
  vtep-endpoint: ""
  vtep-mac: ""
  vtep-mask: ""
  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
---
apiVersion: v1
data:
  bootstrap-config.json: |
    {
      "node": {
        "id": "host~127.0.0.1~no-id~localdomain",
        "cluster": "ingress-cluster"
      },
      "staticResources": {
        "listeners": [
          {
            "name": "envoy-prometheus-metrics-listener",
            "address": {
              "socket_address": {
                "address": "0.0.0.0",
                "port_value": 9964
              }
            },
            "filter_chains": [
              {
                "filters": [
                  {
                    "name": "envoy.filters.network.http_connection_manager",
                    "typed_config": {
                      "@type": "type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager",
                      "stat_prefix": "envoy-prometheus-metrics-listener",
                      "route_config": {
                        "virtual_hosts": [
                          {
                            "name": "prometheus_metrics_route",
                            "domains": [
                              "*"
                            ],
                            "routes": [
                              {
                                "name": "prometheus_metrics_route",
                                "match": {
                                  "prefix": "/metrics"
                                },
                                "route": {
                                  "cluster": "/envoy-admin",
                                  "prefix_rewrite": "/stats/prometheus"
                                }
                              }
                            ]
                          }
                        ]
                      },
                      "http_filters": [
                        {
                          "name": "envoy.filters.http.router",
                          "typed_config": {
                            "@type": "type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"
                          }
                        }
                      ],
                      "internal_address_config": {
                        "cidr_ranges": [
                          {
                            "address_prefix": "10.0.0.0",
                            "prefix_len": 8
                          },
                          {
                            "address_prefix": "172.16.0.0",
                            "prefix_len": 12
                          },
                          {
                            "address_prefix": "192.168.0.0",
                            "prefix_len": 16
                          },
                          {
                            "address_prefix": "127.0.0.1",
                            "prefix_len": 32
                          },
                          {
                            "address_prefix": "::1",
                            "prefix_len": 128
                          }
                        ]
                      },
                      "stream_idle_timeout": "0s"
                    }
                  }
                ]
              }
            ]
          },
          {
            "name": "envoy-health-listener",
            "address": {
              "socket_address": {
                "address": "127.0.0.1",
                "port_value": 9878
              }
            },
            "filter_chains": [
              {
                "filters": [
                  {
                    "name": "envoy.filters.network.http_connection_manager",
                    "typed_config": {
                      "@type": "type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager",
                      "stat_prefix": "envoy-health-listener",
                      "route_config": {
                        "virtual_hosts": [
                          {
                            "name": "health",
                            "domains": [
                              "*"
                            ],
                            "routes": [
                              {
                                "name": "health",
                                "match": {
                                  "prefix": "/healthz"
                                },
                                "route": {
                                  "cluster": "/envoy-admin",
                                  "prefix_rewrite": "/ready"
                                }
                              }
                            ]
                          }
                        ]
                      },
                      "http_filters": [
                        {
                          "name": "envoy.filters.http.router",
                          "typed_config": {
                            "@type": "type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"
                          }
                        }
                      ],
                      "internal_address_config": {
                        "cidr_ranges": [
                          {
                            "address_prefix": "10.0.0.0",
                            "prefix_len": 8
                          },
                          {
                            "address_prefix": "172.16.0.0",
                            "prefix_len": 12
                          },
                          {
                            "address_prefix": "192.168.0.0",
                            "prefix_len": 16
                          },
                          {
                            "address_prefix": "127.0.0.1",
                            "prefix_len": 32
                          },
                          {
                            "address_prefix": "::1",
                            "prefix_len": 128
                          }
                        ]
                      },
                      "stream_idle_timeout": "0s"
                    }
                  }
                ]
              }
            ]
          }
        ],
        "clusters": [
          {
            "name": "ingress-cluster",
            "type": "ORIGINAL_DST",
            "connectTimeout": "2s",
            "lbPolicy": "CLUSTER_PROVIDED",
            "typedExtensionProtocolOptions": {
              "envoy.extensions.upstreams.http.v3.HttpProtocolOptions": {
                "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions",
                "commonHttpProtocolOptions": {
                  "idleTimeout": "60s",
                  "maxConnectionDuration": "0s",
                  "maxRequestsPerConnection": 0
                },
                "useDownstreamProtocolConfig": {}
              }
            },
            "cleanupInterval": "2.500s"
          },
          {
            "name": "egress-cluster-tls",
            "type": "ORIGINAL_DST",
            "connectTimeout": "2s",
            "lbPolicy": "CLUSTER_PROVIDED",
            "typedExtensionProtocolOptions": {
              "envoy.extensions.upstreams.http.v3.HttpProtocolOptions": {
                "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions",
                "commonHttpProtocolOptions": {
                  "idleTimeout": "60s",
                  "maxConnectionDuration": "0s",
                  "maxRequestsPerConnection": 0
                },
                "upstreamHttpProtocolOptions": {},
                "useDownstreamProtocolConfig": {}
              }
            },
            "cleanupInterval": "2.500s",
            "transportSocket": {
              "name": "cilium.tls_wrapper",
              "typedConfig": {
                "@type": "type.googleapis.com/cilium.UpstreamTlsWrapperContext"
              }
            }
          },
          {
            "name": "egress-cluster",
            "type": "ORIGINAL_DST",
            "connectTimeout": "2s",
            "lbPolicy": "CLUSTER_PROVIDED",
            "typedExtensionProtocolOptions": {
              "envoy.extensions.upstreams.http.v3.HttpProtocolOptions": {
                "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions",
                "commonHttpProtocolOptions": {
                  "idleTimeout": "60s",
                  "maxConnectionDuration": "0s",
                  "maxRequestsPerConnection": 0
                },
                "useDownstreamProtocolConfig": {}
              }
            },
            "cleanupInterval": "2.500s"
          },
          {
            "name": "ingress-cluster-tls",
            "type": "ORIGINAL_DST",
            "connectTimeout": "2s",
            "lbPolicy": "CLUSTER_PROVIDED",
            "typedExtensionProtocolOptions": {
              "envoy.extensions.upstreams.http.v3.HttpProtocolOptions": {
                "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions",
                "commonHttpProtocolOptions": {
                  "idleTimeout": "60s",
                  "maxConnectionDuration": "0s",
                  "maxRequestsPerConnection": 0
                },
                "upstreamHttpProtocolOptions": {},
                "useDownstreamProtocolConfig": {}
              }
            },
            "cleanupInterval": "2.500s",
            "transportSocket": {
              "name": "cilium.tls_wrapper",
              "typedConfig": {
                "@type": "type.googleapis.com/cilium.UpstreamTlsWrapperContext"
              }
            }
          },
          {
            "name": "xds-grpc-cilium",
            "type": "STATIC",
            "connectTimeout": "2s",
            "loadAssignment": {
              "clusterName": "xds-grpc-cilium",
              "endpoints": [
                {
                  "lbEndpoints": [
                    {
                      "endpoint": {
                        "address": {
                          "pipe": {
                            "path": "/var/run/cilium/envoy/sockets/xds.sock"
                          }
                        }
                      }
                    }
                  ]
                }
              ]
            },
            "typedExtensionProtocolOptions": {
              "envoy.extensions.upstreams.http.v3.HttpProtocolOptions": {
                "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions",
                "explicitHttpConfig": {
                  "http2ProtocolOptions": {}
                }
              }
            }
          },
          {
            "name": "/envoy-admin",
            "type": "STATIC",
            "connectTimeout": "2s",
            "loadAssignment": {
              "clusterName": "/envoy-admin",
              "endpoints": [
                {
                  "lbEndpoints": [
                    {
                      "endpoint": {
                        "address": {
                          "pipe": {
                            "path": "/var/run/cilium/envoy/sockets/admin.sock"
                          }
                        }
                      }
                    }
                  ]
                }
              ]
            }
          }
        ]
      },
      "dynamicResources": {
        "ldsConfig": {
          "initialFetchTimeout": "30s",
          "apiConfigSource": {
            "apiType": "GRPC",
            "transportApiVersion": "V3",
            "grpcServices": [
              {
                "envoyGrpc": {
                  "clusterName": "xds-grpc-cilium"
                }
              }
            ],
            "setNodeOnFirstMessageOnly": true
          },
          "resourceApiVersion": "V3"
        },
        "cdsConfig": {
          "initialFetchTimeout": "30s",
          "apiConfigSource": {
            "apiType": "GRPC",
            "transportApiVersion": "V3",
            "grpcServices": [
              {
                "envoyGrpc": {
                  "clusterName": "xds-grpc-cilium"
                }
              }
            ],
            "setNodeOnFirstMessageOnly": true
          },
          "resourceApiVersion": "V3"
        }
      },
      "bootstrapExtensions": [
        {
          "name": "envoy.bootstrap.internal_listener",
          "typed_config": {
            "@type": "type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener"
          }
        }
      ],
      "overload_manager": {
        "resource_monitors": [
          {
            "name": "envoy.resource_monitors.global_downstream_max_connections",
            "typed_config": {
              "@type": "type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig",
              "max_active_downstream_connections": "50000"
            }
          }
        ]
      },
      "admin": {
        "address": {
          "pipe": {
            "path": "/var/run/cilium/envoy/sockets/admin.sock"
          }
        }
      }
    }
kind: ConfigMap
metadata:
  name: cilium-envoy-config
  namespace: kube-system
---
apiVersion: v1
data:
  management.tmpl.json: |-
    {
        "Stuns": [
            {
                "Proto": "udp",
                "URI": "${NETBIRD_STUN_URI}",
                "Username": "",
                "Password": null
            }
        ],
        "TURNConfig": {
            "Turns": [
                {
                    "Proto": "udp",
                    "URI": "${NETBIRD_TURN_URI}",
                    "Username": "${NETBIRD_TURN_USER}",
                    "Password": "${NETBIRD_TURN_PASSWORD}"
                }
            ],
            "CredentialsTTL": "12h",
            "Secret": "secret",
            "TimeBasedCredentials": false
        },
        "Signal": {
            "Proto": "${NETBIRD_SIGNAL_PROTOCOL}",
            "URI": "${NETBIRD_SIGNAL_URI}",
            "Username": "",
            "Password": null
        },
        "Datadir": "",
        "HttpConfig": {
            "Address": "0.0.0.0:80",
            "AuthAudience": "${NETBIRD_AUTH_AUDIENCE}",
            "AuthUserIDClaim": "${NETBIRD_AUTH_USER_ID_CLAIM:-sub}",
            "CertFile": "${NETBIRD_MGMT_API_CERT_FILE}",
            "CertKey": "${NETBIRD_MGMT_API_CERT_KEY_FILE}",
            "OIDCConfigEndpoint": "${NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT}"
        },
        "IdpManagerConfig": {
            "ManagerType": "${NETBIRD_IDP_MANAGER_TYPE}",
            "${NETBIRD_IDP_MANAGER_TYPE^}ClientCredentials": {
                "ClientID": "${NETBIRD_IDP_CLIENT_ID}",
                "ClientSecret": "${NETBIRD_IDP_CLIENT_SECRET}",
                "GrantType": "${NETBIRD_IDP_GRANT_TYPE}",
                "Audience": "${NETBIRD_IDP_AUTH0_AUDIENCE}",
                "AuthIssuer": "${NETBIRD_IDP_AUTH0_AUTH_ISSUER}",
                "AdminEndpoint": "${NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT}",
                "TokenEndpoint": "${NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT}"
            }
        },
        "DeviceAuthorizationFlow": {
            "Provider": "${NETBIRD_AUTH_DEVICE_AUTH_PROVIDER}",
            "ProviderConfig": {
                "Audience": "${NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE}",
                "ClientID": "${NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID}",
                "DeviceAuthEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT}",
                "Domain": "${NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY}",
                "TokenEndpoint": "${NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT}",
                "Scope": "${NETBIRD_AUTH_DEVICE_AUTH_SCOPE}",
                "UseIDToken": ${NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN:-false}
            }
        },
        "Relay": {
            "Addresses": ["${NETBIRD_RELAY_URI}"],
            "CredentialsTTL": "24h",
            "Secret": "${NETBIRD_RELAY_SECRET}"
        }
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-management
  namespace: netbird
---
apiVersion: v1
data:
  .htaccess: |-
    # line below if for Apache 2.4
    <ifModule mod_authz_core.c>
    Require all denied
    </ifModule>
    # line below if for Apache 2.2
    <ifModule !mod_authz_core.c>
    deny from all
    </ifModule>
    # section for Apache 2.2 and 2.4
    <ifModule mod_autoindex.c>
    IndexIgnore *
    </ifModule>
  apache-pretty-urls.config.php: |-
    <?php
    $CONFIG = array (
      'htaccess.RewriteBase' => '/',
    );
  apcu.config.php: |-
    <?php
    $CONFIG = array (
      'memcache.local' => '\OC\Memcache\APCu',
    );
  apps.config.php: |-
    <?php
    $CONFIG = array (
      'apps_paths' => array (
          0 => array (
                  'path'     => OC::$SERVERROOT.'/apps',
                  'url'      => '/apps',
                  'writable' => false,
          ),
          1 => array (
                  'path'     => OC::$SERVERROOT.'/custom_apps',
                  'url'      => '/custom_apps',
                  'writable' => true,
          ),
      ),
    );
  autoconfig.php: |-
    <?php
    $autoconfig_enabled = false;
    if (getenv('SQLITE_DATABASE')) {
        $AUTOCONFIG["dbtype"] = "sqlite";
        $AUTOCONFIG["dbname"] = getenv('SQLITE_DATABASE');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE_FILE') && getenv('MYSQL_USER_FILE') && getenv('MYSQL_PASSWORD_FILE') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG['dbtype'] = 'mysql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('MYSQL_DATABASE_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('MYSQL_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('MYSQL_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('MYSQL_DATABASE') && getenv('MYSQL_USER') && getenv('MYSQL_PASSWORD') && getenv('MYSQL_HOST')) {
        $AUTOCONFIG["dbtype"] = "mysql";
        $AUTOCONFIG["dbname"] = getenv('MYSQL_DATABASE');
        $AUTOCONFIG["dbuser"] = getenv('MYSQL_USER');
        $AUTOCONFIG["dbpass"] = getenv('MYSQL_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('MYSQL_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB_FILE') && getenv('POSTGRES_USER_FILE') && getenv('POSTGRES_PASSWORD_FILE') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG['dbtype'] = 'pgsql';
        $AUTOCONFIG['dbname'] = trim(file_get_contents(getenv('POSTGRES_DB_FILE')));
        $AUTOCONFIG['dbuser'] = trim(file_get_contents(getenv('POSTGRES_USER_FILE')));
        $AUTOCONFIG['dbpass'] = trim(file_get_contents(getenv('POSTGRES_PASSWORD_FILE')));
        $AUTOCONFIG['dbhost'] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    } elseif (getenv('POSTGRES_DB') && getenv('POSTGRES_USER') && getenv('POSTGRES_PASSWORD') && getenv('POSTGRES_HOST')) {
        $AUTOCONFIG["dbtype"] = "pgsql";
        $AUTOCONFIG["dbname"] = getenv('POSTGRES_DB');
        $AUTOCONFIG["dbuser"] = getenv('POSTGRES_USER');
        $AUTOCONFIG["dbpass"] = getenv('POSTGRES_PASSWORD');
        $AUTOCONFIG["dbhost"] = getenv('POSTGRES_HOST');
        $autoconfig_enabled = true;
    }
    if ($autoconfig_enabled) {
        $AUTOCONFIG["directory"] = getenv('NEXTCLOUD_DATA_DIR') ?: "/var/www/html/data";
    }
  mycustom.config.php: |-
    <?php
    $CONFIG = array(
      'trusted_proxies' => array('10.0.0.0/8'),
      'default_phone_region' => 'NO',
      'maintenance_window_start' => 1,
      );
  redis.config.php: |-
    <?php
    if (getenv('REDIS_HOST')) {
      $CONFIG = array(
        'memcache.distributed' => '\OC\Memcache\Redis',
        'memcache.locking' => '\OC\Memcache\Redis',
        'redis' => array(
          'host' => getenv('REDIS_HOST'),
          'password' => getenv('REDIS_HOST_PASSWORD_FILE') ? trim(file_get_contents(getenv('REDIS_HOST_PASSWORD_FILE'))) : (string) getenv('REDIS_HOST_PASSWORD'),
        ),
      );

      if (getenv('REDIS_HOST_PORT') !== false) {
        $CONFIG['redis']['port'] = (int) getenv('REDIS_HOST_PORT');
      } elseif (getenv('REDIS_HOST')[0] != '/') {
        $CONFIG['redis']['port'] = 6379;
      }
    }
  reverse-proxy.config.php: |-
    <?php
    $overwriteHost = getenv('OVERWRITEHOST');
    if ($overwriteHost) {
      $CONFIG['overwritehost'] = $overwriteHost;
    }

    $overwriteProtocol = getenv('OVERWRITEPROTOCOL');
    if ($overwriteProtocol) {
      $CONFIG['overwriteprotocol'] = $overwriteProtocol;
    }

    $overwriteCliUrl = getenv('OVERWRITECLIURL');
    if ($overwriteCliUrl) {
      $CONFIG['overwrite.cli.url'] = $overwriteCliUrl;
    }

    $overwriteWebRoot = getenv('OVERWRITEWEBROOT');
    if ($overwriteWebRoot) {
      $CONFIG['overwritewebroot'] = $overwriteWebRoot;
    }

    $overwriteCondAddr = getenv('OVERWRITECONDADDR');
    if ($overwriteCondAddr) {
      $CONFIG['overwritecondaddr'] = $overwriteCondAddr;
    }

    $trustedProxies = getenv('TRUSTED_PROXIES');
    if ($trustedProxies) {
      $CONFIG['trusted_proxies'] = array_filter(array_map('trim', explode(' ', $trustedProxies)));
    }

    $forwardedForHeaders = getenv('FORWARDED_FOR_HEADERS');
    if ($forwardedForHeaders) {
      $CONFIG['forwarded_for_headers'] = array_filter(array_map('trim', explode(' ', $forwardedForHeaders)));
    }
  s3.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_S3_BUCKET')) {
      $use_ssl = getenv('OBJECTSTORE_S3_SSL');
      $use_path = getenv('OBJECTSTORE_S3_USEPATH_STYLE');
      $use_legacyauth = getenv('OBJECTSTORE_S3_LEGACYAUTH');
      $autocreate = getenv('OBJECTSTORE_S3_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => array(
          'class' => '\OC\Files\ObjectStore\S3',
          'arguments' => array(
            'bucket' => getenv('OBJECTSTORE_S3_BUCKET'),
            'region' => getenv('OBJECTSTORE_S3_REGION') ?: '',
            'hostname' => getenv('OBJECTSTORE_S3_HOST') ?: '',
            'port' => getenv('OBJECTSTORE_S3_PORT') ?: '',
            'storageClass' => getenv('OBJECTSTORE_S3_STORAGE_CLASS') ?: '',
            'objectPrefix' => getenv("OBJECTSTORE_S3_OBJECT_PREFIX") ? getenv("OBJECTSTORE_S3_OBJECT_PREFIX") : "urn:oid:",
            'autocreate' => strtolower($autocreate) !== 'false',
            'use_ssl' => strtolower($use_ssl) !== 'false',
            // required for some non Amazon S3 implementations
            'use_path_style' => $use_path == true && strtolower($use_path) !== 'false',
            // required for older protocol versions
            'legacy_auth' => $use_legacyauth == true && strtolower($use_legacyauth) !== 'false'
          )
        )
      );

      if (getenv('OBJECTSTORE_S3_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_KEY')) {
        $CONFIG['objectstore']['arguments']['key'] = getenv('OBJECTSTORE_S3_KEY');
      } else {
        $CONFIG['objectstore']['arguments']['key'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SECRET_FILE')) {
        $CONFIG['objectstore']['arguments']['secret'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SECRET_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SECRET')) {
        $CONFIG['objectstore']['arguments']['secret'] = getenv('OBJECTSTORE_S3_SECRET');
      } else {
        $CONFIG['objectstore']['arguments']['secret'] = '';
      }

      if (getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = trim(file_get_contents(getenv('OBJECTSTORE_S3_SSE_C_KEY_FILE')));
      } elseif (getenv('OBJECTSTORE_S3_SSE_C_KEY')) {
        $CONFIG['objectstore']['arguments']['sse_c_key'] = getenv('OBJECTSTORE_S3_SSE_C_KEY');
      }
    }
  smtp.config.php: |-
    <?php
    if (getenv('SMTP_HOST') && getenv('MAIL_FROM_ADDRESS') && getenv('MAIL_DOMAIN')) {
      $CONFIG = array (
        'mail_smtpmode' => 'smtp',
        'mail_smtphost' => getenv('SMTP_HOST'),
        'mail_smtpport' => getenv('SMTP_PORT') ?: (getenv('SMTP_SECURE') ? 465 : 25),
        'mail_smtpsecure' => getenv('SMTP_SECURE') ?: '',
        'mail_smtpauth' => getenv('SMTP_NAME') && (getenv('SMTP_PASSWORD') || getenv('SMTP_PASSWORD_FILE')),
        'mail_smtpauthtype' => getenv('SMTP_AUTHTYPE') ?: 'LOGIN',
        'mail_smtpname' => getenv('SMTP_NAME') ?: '',
        'mail_from_address' => getenv('MAIL_FROM_ADDRESS'),
        'mail_domain' => getenv('MAIL_DOMAIN'),
      );

      if (getenv('SMTP_PASSWORD_FILE')) {
          $CONFIG['mail_smtppassword'] = trim(file_get_contents(getenv('SMTP_PASSWORD_FILE')));
      } elseif (getenv('SMTP_PASSWORD')) {
          $CONFIG['mail_smtppassword'] = getenv('SMTP_PASSWORD');
      } else {
          $CONFIG['mail_smtppassword'] = '';
      }
    }
  swift.config.php: |-
    <?php
    if (getenv('OBJECTSTORE_SWIFT_URL')) {
        $autocreate = getenv('OBJECTSTORE_SWIFT_AUTOCREATE');
      $CONFIG = array(
        'objectstore' => [
          'class' => 'OC\\Files\\ObjectStore\\Swift',
          'arguments' => [
            'autocreate' => $autocreate == true && strtolower($autocreate) !== 'false',
            'user' => [
              'name' => getenv('OBJECTSTORE_SWIFT_USER_NAME'),
              'password' => getenv('OBJECTSTORE_SWIFT_USER_PASSWORD'),
              'domain' => [
                'name' => (getenv('OBJECTSTORE_SWIFT_USER_DOMAIN')) ?: 'Default',
              ],
            ],
            'scope' => [
              'project' => [
                'name' => getenv('OBJECTSTORE_SWIFT_PROJECT_NAME'),
                'domain' => [
                  'name' => (getenv('OBJECTSTORE_SWIFT_PROJECT_DOMAIN')) ?: 'Default',
                ],
              ],
            ],
            'serviceName' => (getenv('OBJECTSTORE_SWIFT_SERVICE_NAME')) ?: 'swift',
            'region' => getenv('OBJECTSTORE_SWIFT_REGION'),
            'url' => getenv('OBJECTSTORE_SWIFT_URL'),
            'bucket' => getenv('OBJECTSTORE_SWIFT_CONTAINER_NAME'),
          ]
        ]
      );
    }
  upgrade-disable-web.config.php: |-
    <?php
    $CONFIG = array (
      'upgrade.disable-web' => true,
    );
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    helm.sh/chart: nextcloud-6.6.2
  name: nextcloud-config
  namespace: nextcloud
---
apiVersion: v1
data:
  my.cnf: |-
    [mysqld]
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mariadb
    datadir=/bitnami/mariadb/data
    plugin_dir=/opt/bitnami/mariadb/plugin
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    tmpdir=/opt/bitnami/mariadb/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
    log-error=/opt/bitnami/mariadb/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    long_query_time=10.0

    [client]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mariadb/plugin

    [manager]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
---
apiVersion: v1
data:
  uploadLimit.ini: |-
    upload_max_filesize = 1G
    post_max_size = 1G
    max_input_time = 5400
    max_execution_time = 5400
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    helm.sh/chart: nextcloud-6.6.2
  name: nextcloud-phpconfig
  namespace: nextcloud
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-root-ca
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-client
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: crossplane-tls-server
  namespace: crossplane
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea
  namespace: gitea
stringData:
  assertions: ""
  config_environment.sh: "#!/usr/bin/env bash\nset -euo pipefail\n\nfunction env2ini::log() {\n  printf \"${1}\\n\"\n}\n\nfunction env2ini::read_config_to_env() {\n  local section=\"${1}\"\n  local line=\"${2}\"\n\n  if [[ -z \"${line}\" ]]; then\n    # skip empty line\n    return\n  fi\n  \n  # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line\n  local setting=\"$(awk -F '=' '{print $1}' <<< \"${line}\" | xargs echo -n)\"\n\n  if [[ -z \"${setting}\" ]]; then\n    env2ini::log '  ! invalid setting'\n    exit 1\n  fi\n\n  local value=''\n  local regex=\"^${setting}(\\s*)=(\\s*)(.*)\"\n  if [[ $line =~ $regex ]]; then\n    value=\"${BASH_REMATCH[3]}\"\n  else\n    env2ini::log '  ! invalid setting'\n    exit 1\n  fi\n\n  env2ini::log \"    + '${setting}'\"\n\n  if [[ -z \"${section}\" ]]; then\n    export \"GITEA____${setting^^}=${value}\"                           # '^^' makes the variable content uppercase\n    return\n  fi\n\n  local masked_section=\"${section//./_0X2E_}\"                            # '//' instructs to replace all matches\n  masked_section=\"${masked_section//-/_0X2D_}\"\n\n  export \"GITEA__${masked_section^^}__${setting^^}=${value}\"        # '^^' makes the variable content uppercase\n}\n\nfunction env2ini::reload_preset_envs() {\n  env2ini::log \"Reloading preset envs...\"\n\n  while read -r line; do\n    if [[ -z \"${line}\" ]]; then\n      # skip empty line\n      return\n    fi\n\n    # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing new line\n    local setting=\"$(awk -F '=' '{print $1}' <<< \"${line}\" | xargs echo -n)\"\n\n    if [[ -z \"${setting}\" ]]; then\n      env2ini::log '  ! invalid setting'\n      exit 1\n    fi\n\n    local value=''\n    local regex=\"^${setting}(\\s*)=(\\s*)(.*)\"\n    if [[ $line =~ $regex ]]; then\n      value=\"${BASH_REMATCH[3]}\"\n    else\n      env2ini::log '  ! invalid setting'\n      exit 1\n    fi\n\n    env2ini::log \"  + '${setting}'\"\n\n    export \"${setting^^}=${value}\"                           # '^^' makes the variable content uppercase\n  done < \"/tmp/existing-envs\"\n\n  rm /tmp/existing-envs\n}\n\n\nfunction env2ini::process_config_file() {\n  local config_file=\"${1}\"\n  local section=\"$(basename \"${config_file}\")\"\n\n  if [[ $section == '_generals_' ]]; then\n    env2ini::log \"  [ini root]\"\n    section=''\n  else\n    env2ini::log \"  ${section}\"\n  fi\n\n  while read -r line; do\n    env2ini::read_config_to_env \"${section}\" \"${line}\"\n  done < <(awk 1 \"${config_file}\")                             # Helm .toYaml trims the trailing new line which breaks line processing; awk 1 ... adds it back while reading\n}\n\nfunction env2ini::load_config_sources() {\n  local path=\"${1}\"\n\n  if [[ -d \"${path}\" ]]; then\n    env2ini::log \"Processing $(basename \"${path}\")...\"\n\n    while read -d '' configFile; do\n      env2ini::process_config_file \"${configFile}\"\n    done < <(find \"${path}\" -type l -not -name '..data' -print0)\n\n    env2ini::log \"\\n\"\n  fi\n}\n\nfunction env2ini::generate_initial_secrets() {\n  # These environment variables will either be\n  #   - overwritten with user defined values,\n  #   - initially used to set up Gitea\n  # Anyway, they won't harm existing app.ini files\n\n  export GITEA__SECURITY__INTERNAL_TOKEN=$(gitea generate secret INTERNAL_TOKEN)\n  export GITEA__SECURITY__SECRET_KEY=$(gitea generate secret SECRET_KEY)\n  export GITEA__OAUTH2__JWT_SECRET=$(gitea generate secret JWT_SECRET)\n  export GITEA__SERVER__LFS_JWT_SECRET=$(gitea generate secret LFS_JWT_SECRET)\n\n  env2ini::log \"...Initial secrets generated\\n\"\n}\n\n# save existing envs prior to script execution. Necessary to keep order of preexisting and custom envs\nenv | (grep -e '^GITEA__' || [[ $? == 1 ]]) > /tmp/existing-envs\n\n# MUST BE CALLED BEFORE OTHER CONFIGURATION\nenv2ini::generate_initial_secrets\n\nenv2ini::load_config_sources '/env-to-ini-mounts/inlines/'\nenv2ini::load_config_sources '/env-to-ini-mounts/additionals/'\n\n# load existing envs to override auto generated envs\nenv2ini::reload_preset_envs\n\nenv2ini::log \"=== All configuration sources loaded ===\\n\"\n\n# safety to prevent rewrite of secret keys if an app.ini already exists\nif [ -f ${GITEA_APP_INI} ]; then\n  env2ini::log 'An app.ini file already exists. To prevent overwriting secret keys, these settings are dropped and remain unchanged:'\n  env2ini::log '  - security.INTERNAL_TOKEN'\n  env2ini::log '  - security.SECRET_KEY'\n  env2ini::log '  - oauth2.JWT_SECRET'\n  env2ini::log '  - server.LFS_JWT_SECRET'\n\n  unset GITEA__SECURITY__INTERNAL_TOKEN\n  unset GITEA__SECURITY__SECRET_KEY\n  unset GITEA__OAUTH2__JWT_SECRET\n  unset GITEA__SERVER__LFS_JWT_SECRET\nfi\n\nenvironment-to-ini -o $GITEA_APP_INI"
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea-init
  namespace: gitea
stringData:
  configure_gitea.sh: "#!/usr/bin/env bash\n\nset -euo pipefail\n\necho '==== BEGIN GITEA CONFIGURATION ===='\n\n{ # try\n  gitea migrate\n} || { # catch\n  echo \"Gitea migrate might fail due to database connection...This init-container will try again in a few seconds\"\n  exit 1\n}\nfunction configure_admin_user() {\n  local full_admin_list=$(gitea admin user list --admin)\n  local actual_user_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+Username\\s+Email\\s+IsActive.*)\"\n  if [[ \"${full_admin_list}\" =~ $regex ]]; then\n    actual_user_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_admin_user' was not able to determine the current list of admin users.\"\n    echo \"       Please review the output of 'gitea admin user list --admin' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-chart/issues.\"\n    echo \"DEBUG: Output of 'gitea admin user list --admin'\"\n    echo \"--\"\n    echo \"${full_admin_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local ACCOUNT_ID=$(echo \"${actual_user_table}\" | grep -E \"\\s+${GITEA_ADMIN_USERNAME}\\s+\" | awk -F \" \" \"{printf \\$1}\")\n  if [[ -z \"${ACCOUNT_ID}\" ]]; then\n    local -a create_args\n    create_args=(--admin --username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\" --email \"gitea@local.domain\")\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = initialOnlyRequireReset ]]; then\n      create_args+=(--must-change-password=true)\n    else\n      create_args+=(--must-change-password=false)\n    fi\n    echo \"No admin user '${GITEA_ADMIN_USERNAME}' found. Creating now...\"\n    gitea admin user create \"${create_args[@]}\"\n    echo '...created.'\n  else\n    if [[ \"${GITEA_ADMIN_PASSWORD_MODE}\" = keepUpdated ]]; then\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist. Running update to sync password...\"\n      # See https://gitea.com/gitea/helm-chart/issues/673\n      # --must-change-password argument was added to change-password, defaulting to true, counter to the previous behavior\n      #   which acted as if it were provided with =false. If the argument is present in this version of gitea, then we\n      #   should add it to prevent requiring frequent admin password resets.\n      local -a change_args\n      change_args=(--username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\")\n      if gitea admin user change-password --help | grep -qF -- '--must-change-password'; then\n        change_args+=(--must-change-password=false)\n      fi\n      gitea admin user change-password \"${change_args[@]}\"\n      echo '...password sync done.'\n    else\n      echo \"Admin account '${GITEA_ADMIN_USERNAME}' already exist, but update mode is set to '${GITEA_ADMIN_PASSWORD_MODE}'. Skipping.\"\n    fi\n  fi\n}\n\nconfigure_admin_user\n\nfunction configure_ldap() {\n    echo 'no ldap configuration... skipping.'\n}\n\nconfigure_ldap\n\nfunction configure_oauth() {\n  local OAUTH_NAME='keycloak'\n  local full_auth_list=$(gitea admin auth list --vertical-bars)\n  local actual_auth_table=''\n\n  # We might have distorted output due to warning logs, so we have to detect the actual user table by its headline and trim output above that line\n  local regex=\"(.*)(ID\\s+\\|Name\\s+\\|Type\\s+\\|Enabled.*)\"\n  if [[ \"${full_auth_list}\" =~ $regex ]]; then\n    actual_auth_table=$(echo \"${BASH_REMATCH[2]}\" | tail -n+2) # tail'ing to drop the table headline\n  else\n    # This code block should never be reached, as long as the output table header remains the same.\n    # If this code block is reached, the regex doesn't match anymore and we probably have to adjust this script.\n\n    echo \"ERROR: 'configure_oauth' was not able to determine the current list of authentication sources.\"\n    echo \"       Please review the output of 'gitea admin auth list --vertical-bars' shown below.\"\n    echo \"       If you think it is an issue with the Helm Chart provisioning, file an issue at https://gitea.com/gitea/helm-chart/issues.\"\n    echo \"DEBUG: Output of 'gitea admin auth list --vertical-bars'\"\n    echo \"--\"\n    echo \"${full_auth_list}\"\n    echo \"--\"\n    exit 1\n  fi\n\n  local AUTH_ID=$(echo \"${actual_auth_table}\" | grep -E \"\\|${OAUTH_NAME}\\s+\\|\" | grep -iE '\\|OAuth2\\s+\\|' | awk -F \" \"  \"{print \\$1}\")\n\n  if [[ -z \"${AUTH_ID}\" ]]; then\n    echo \"No oauth configuration found with name '${OAUTH_NAME}'. Installing it now...\"\n    gitea admin auth add-oauth --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...installed.'\n  else\n    echo \"Existing oauth configuration with name '${OAUTH_NAME}': '${AUTH_ID}'. Running update to sync settings...\"\n    gitea admin auth update-oauth --id \"${AUTH_ID}\" --auto-discover-url \"https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration\" --key \"${GITEA_OAUTH_KEY_0}\" --name \"keycloak\" --provider \"openidConnect\" --secret \"${GITEA_OAUTH_SECRET_0}\" \n    echo '...sync settings done.'\n  fi\n}\n\nconfigure_oauth\n\necho '==== END GITEA CONFIGURATION ===='"
  configure_gpg_environment.sh: |-
    #!/usr/bin/env bash
    set -eu

    gpg --batch --import /raw/private.asc
  init_directory_structure.sh: |-
    #!/usr/bin/env bash

    set -euo pipefail
    mkdir -pv /data/git/.ssh
    chmod -Rv 700 /data/git/.ssh
    [ ! -d /data/gitea/conf ] && mkdir -pv /data/gitea/conf

    # prepare temp directory structure
    mkdir -pv "${GITEA_TEMP}"
    chmod -v ug+rwx "${GITEA_TEMP}"
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea-inline-config
  namespace: gitea
stringData:
  _generals_: ""
  actions: ENABLED=false
  cache: |-
    ADAPTER=memory
    HOST=
  database: |-
    DB_TYPE=postgres
    HOST=gitea-postgresql.gitea.svc.cluster.local:5432
    NAME=gitea
    PASSWD=gitea
    USER=gitea
  indexer: ISSUE_INDEXER_TYPE=db
  metrics: ENABLED=false
  queue: |-
    CONN_STR=
    TYPE=level
  repository: ROOT=/data/git/gitea-repositories
  security: INSTALL_LOCK=true
  server: |-
    APP_DATA_PATH=/data
    DOMAIN=gitea.homelab.olav.ninja
    ENABLE_PPROF=false
    HTTP_PORT=3000
    LFS_START_SERVER=true
    OFFLINE_MODE=false
    PROTOCOL=http
    ROOT_URL=https://gitea.homelab.olav.ninja
    SSH_DOMAIN=gitea.homelab.olav.ninja
    SSH_LISTEN_PORT=2222
    SSH_PORT=22
    START_SSH_SERVER=true
  service: DISABLE_REGISTRATION=true
  session: |-
    PROVIDER=memory
    PROVIDER_CONFIG=
type: Opaque
---
apiVersion: v1
kind: Endpoints
metadata:
  name: external-cluster
  namespace: traefik
subsets:
  - addresses:
      - ip: 192.168.0.81
    ports:
      - name: ingress-port
        port: 443
        protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager
  namespace: cert-manager
spec:
  ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cert-manager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
  selector:
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: cainjector
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
  selector:
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/name: webhook
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    io.cilium/lb-ipam-ips: 192.168.0.91
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.2-r8
    helm.sh/chart: coturn-1.0.0
  name: coturn
  namespace: coturn
spec:
  ports:
    - name: tcp
      port: 3478
      protocol: TCP
      targetPort: tcp
    - name: udp
      port: 3478
      protocol: UDP
      targetPort: udp
    - name: tcp-tls
      port: 5349
      protocol: TCP
      targetPort: tcp-tls
    - name: udp-tls
      port: 5349
      protocol: UDP
      targetPort: udp-tls
  selector:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/name: coturn
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    release: crossplane
  name: crossplane-webhooks
  namespace: crossplane
spec:
  ports:
    - port: 9443
      protocol: TCP
      targetPort: 9443
  selector:
    app: crossplane
    release: crossplane
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea-http
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: http
      port: 3000
      targetPort: null
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.5.20
  name: gitea-postgresql
  namespace: gitea
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.5.20
  name: gitea-postgresql-hl
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea-ssh
  namespace: gitea
spec:
  clusterIP: None
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: 2222
  selector:
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/name: gitea
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/service: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/name: hajimari
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/service: immich-machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-machine-learning
  namespace: immich
spec:
  ports:
    - name: http
      port: 3003
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: machine-learning
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql-hl
  namespace: immich
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-headless
  namespace: immich
spec:
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
spec:
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      nodePort: null
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: redis
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/service: immich-server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-server
  namespace: immich
spec:
  ports:
    - name: http
      port: 2283
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/name: server
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak
  namespace: keycloak
spec:
  ports:
    - name: http
      nodePort: null
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak-headless
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.2.0
    helm.sh/chart: postgresql-16.2.5
  name: keycloak-postgresql
  namespace: keycloak
spec:
  ports:
    - name: tcp-postgresql
      nodePort: null
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: postgresql
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.2.0
    helm.sh/chart: postgresql-16.2.5
  name: keycloak-postgresql-hl
  namespace: keycloak
spec:
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "9964"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    io.cilium/app: proxy
    k8s-app: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: envoy-metrics
      port: 9964
      protocol: TCP
      targetPort: envoy-metrics
  selector:
    k8s-app: cilium-envoy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: hubble-peer
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: hubble-peer
  namespace: kube-system
spec:
  internalTrafficPolicy: Local
  ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
  selector:
    k8s-app: cilium
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-management
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-management
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-signal
  namespace: netbird
spec:
  ports:
    - name: https
      port: 80
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/name: netbird-signal
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.1.3
    helm.sh/chart: netbird-dashboard-1.0.0
  name: netbird-dashboard
  namespace: netbird
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/name: netbird-dashboard
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: netbird-relay
  name: netbird-relay
  namespace: netbird
spec:
  ports:
    - name: relay
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/name: netbird-relay
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    helm.sh/chart: nextcloud-6.6.2
  name: nextcloud
  namespace: nextcloud
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: nextcloud
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  ports:
    - name: mysql
      nodePort: null
      port: 3306
      protocol: TCP
      targetPort: mysql
  selector:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/name: mariadb
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  ports:
    - name: http
      nodePort: null
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/name: sealed-secrets
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: external-cluster
  namespace: traefik
spec:
  clusterIP: None
  ports:
    - name: ingress-port
      port: 443
      protocol: TCP
      targetPort: 443
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-33.2.1
  name: traefik
  namespace: traefik
spec:
  externalTrafficPolicy: Local
  internalTrafficPolicy: Local
  loadBalancerIP: 192.168.0.90
  ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: ssh
    - name: webpublic
      port: 9443
      protocol: TCP
      targetPort: webpublic
    - name: websecure
      port: 443
      protocol: TCP
      targetPort: websecure
  selector:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/name: traefik
  type: LoadBalancer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: gitea
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: gitea-postgresql
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: immich-library
  namespace: immich
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 150Gi
  storageClassName: proxmox-csi
  volumeName: immich-library
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: immich-postgresql
  namespace: immich
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: immich-postgresql
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-management
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: proxmox-csi
  volumeName: netbird-management
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-signal
  namespace: netbird
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: proxmox-csi
  volumeName: netbird-signal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-data
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 300Gi
  storageClassName: proxmox-csi
  volumeName: nextcloud-data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: proxmox-csi
  volumeName: nextcloud-mariadb
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cert-manager
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cert-manager
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cert-manager
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cert-manager
        app.kubernetes.io/version: v1.16.2
        helm.sh/chart: cert-manager-v1.16.2
    spec:
      containers:
        - args:
            - --v=2
            - --cluster-resource-namespace=$(POD_NAMESPACE)
            - --leader-election-namespace=cert-manager
            - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.16.2
            - --max-concurrent-challenges=60
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
            - containerPort: 9403
              name: http-healthz
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cainjector
    app.kubernetes.io/component: cainjector
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cainjector
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-cainjector
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: cainjector
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cainjector
        app.kubernetes.io/version: v1.16.2
        helm.sh/chart: cert-manager-v1.16.2
    spec:
      containers:
        - args:
            - --v=2
            - --leader-election-namespace=cert-manager
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-cainjector
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook
  namespace: cert-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9402"
        prometheus.io/scrape: "true"
      labels:
        app: webhook
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: webhook
        app.kubernetes.io/version: v1.16.2
        helm.sh/chart: cert-manager-v1.16.2
    spec:
      containers:
        - args:
            - --v=2
            - --secure-port=10250
            - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
            - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
            - --dynamic-serving-dns-names=cert-manager-webhook
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
            - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
            - containerPort: 10250
              name: https
              protocol: TCP
            - containerPort: 6080
              name: healthcheck
              protocol: TCP
            - containerPort: 9402
              name: http-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.2-r8
    helm.sh/chart: coturn-1.0.0
  name: coturn
  namespace: coturn
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: coturn
      app.kubernetes.io/name: coturn
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: coturn
        app.kubernetes.io/name: coturn
    spec:
      containers:
        - args:
            - --listening-port=3478
            - --tls-listening-port=5349
            - --user=netbird:$$COTURN_USER_netbird_KEY
            - --cert=/usr/local/etc/tls.crt
            - --pkey=/usr/local/etc/tls.key
            - --realm=coturn.homelab.olav.ninja
            - --log-file=stdout
            - --no-software-attribute
            - --no-cli
            - --listening-ip=0.0.0.0
          env:
            - name: COTURN_USER_netbird_KEY
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
          image: coturn/coturn:4.6.2-r8
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            tcpSocket:
              port: tcp
          name: coturn
          ports:
            - containerPort: 3478
              name: tcp
              protocol: TCP
            - containerPort: 3478
              name: udp
              protocol: UDP
            - containerPort: 5349
              name: tcp-tls
              protocol: TCP
            - containerPort: 5349
              name: udp-tls
              protocol: UDP
          readinessProbe:
            tcpSocket:
              port: tcp
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/turnserver.conf
              name: config
              subPath: turnserver.conf
            - mountPath: /usr/local/etc
              name: certs
      securityContext: {}
      serviceAccountName: coturn
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      volumes:
        - configMap:
            name: coturn
          name: config
        - name: certs
          secret:
            secretName: coturn
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    release: crossplane
  name: crossplane
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 1.18.2
        helm.sh/chart: crossplane-1.18.2
        release: crossplane
    spec:
      containers:
        - args:
            - core
            - start
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: LEADER_ELECTION
              value: "true"
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_SERVER_CERTS_DIR
              value: /tls/server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
            - name: TLS_CLIENT_CERTS_DIR
              value: /tls/client
          image: xpkg.upbound.io/crossplane/crossplane:v1.18.2
          imagePullPolicy: IfNotPresent
          name: crossplane
          ports:
            - containerPort: 8081
              name: readyz
            - containerPort: 9443
              name: webhooks
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
          startupProbe:
            failureThreshold: 30
            periodSeconds: 2
            tcpSocket:
              port: readyz
          volumeMounts:
            - mountPath: /cache
              name: package-cache
            - mountPath: /tls/server
              name: tls-server-certs
            - mountPath: /tls/client
              name: tls-client-certs
      hostNetwork: false
      initContainers:
        - args:
            - core
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: WEBHOOK_SERVICE_NAME
              value: crossplane-webhooks
            - name: WEBHOOK_SERVICE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: WEBHOOK_SERVICE_PORT
              value: "9443"
            - name: TLS_CA_SECRET_NAME
              value: crossplane-root-ca
            - name: TLS_SERVER_SECRET_NAME
              value: crossplane-tls-server
            - name: TLS_CLIENT_SECRET_NAME
              value: crossplane-tls-client
          image: xpkg.upbound.io/crossplane/crossplane:v1.18.2
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 500m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: crossplane
      volumes:
        - emptyDir:
            medium: null
            sizeLimit: 20Mi
          name: package-cache
        - name: tls-server-certs
          secret:
            secretName: crossplane-tls-server
        - name: tls-client-certs
          secret:
            secretName: crossplane-tls-client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: crossplane-rbac-manager
    app.kubernetes.io/component: cloud-infrastructure-controller
    app.kubernetes.io/instance: crossplane
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crossplane
    app.kubernetes.io/part-of: crossplane
    app.kubernetes.io/version: 1.18.2
    helm.sh/chart: crossplane-1.18.2
    release: crossplane
  name: crossplane-rbac-manager
  namespace: crossplane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crossplane-rbac-manager
      release: crossplane
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: crossplane-rbac-manager
        app.kubernetes.io/component: cloud-infrastructure-controller
        app.kubernetes.io/instance: crossplane
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: crossplane
        app.kubernetes.io/part-of: crossplane
        app.kubernetes.io/version: 1.18.2
        helm.sh/chart: crossplane-1.18.2
        release: crossplane
    spec:
      containers:
        - args:
            - rbac
            - start
            - --provider-clusterrole=crossplane:allowed-provider-permissions
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane
                  divisor: "1"
                  resource: limits.memory
            - name: LEADER_ELECTION
              value: "true"
          image: xpkg.upbound.io/crossplane/crossplane:v1.18.2
          imagePullPolicy: IfNotPresent
          name: crossplane
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      initContainers:
        - args:
            - rbac
            - init
          env:
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.cpu
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: crossplane-init
                  divisor: "1"
                  resource: limits.memory
          image: xpkg.upbound.io/crossplane/crossplane:v1.18.2
          imagePullPolicy: IfNotPresent
          name: crossplane-init
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsUser: 65532
      serviceAccountName: rbac-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-controller
  namespace: csi-proxmox
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: proxmox-csi-plugin
      app.kubernetes.io/name: proxmox-csi-plugin
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: c69436cb1e16c36ff708b1003d3ca4c6ee6484d2524e2ba7d9b68f473acaa1ca
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: proxmox-csi-plugin
        app.kubernetes.io/name: proxmox-csi-plugin
    spec:
      containers:
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --cloud-config=/etc/proxmox/config.yaml
          image: ghcr.io/sergelogvinov/proxmox-csi-controller:v0.9.0
          imagePullPolicy: IfNotPresent
          name: proxmox-csi-plugin-controller
          ports: null
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
            - mountPath: /etc/proxmox/
              name: cloud-config
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --leader-election
            - --default-fstype=ext4
          image: registry.k8s.io/sig-storage/csi-attacher:v4.7.0
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --leader-election
            - --default-fstype=ext4
            - --feature-gates=Topology=True
            - --enable-capacity
            - --capacity-ownerref-level=2
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          image: registry.k8s.io/sig-storage/csi-provisioner:v5.1.0
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --timeout=3m
            - --handle-volume-inuse-error=false
            - --leader-election
          image: registry.k8s.io/sig-storage/csi-resizer:v1.12.0
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
          image: registry.k8s.io/sig-storage/livenessprobe:v2.14.0
          imagePullPolicy: IfNotPresent
          name: liveness-probe
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
      enableServiceLinks: false
      hostAliases: []
      initContainers: []
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroup: 65532
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: proxmox-csi-plugin-controller
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: controller
              app.kubernetes.io/instance: proxmox-csi-plugin
              app.kubernetes.io/name: proxmox-csi-plugin
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
      volumes:
        - emptyDir: {}
          name: socket-dir
        - name: cloud-config
          secret:
            secretName: proxmox-csi-plugin
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: gitea
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: 42dc3391d15270c36d253486815bc03d3aee96fb09e1345ae528e84f43c5eb11
        checksum/oauth_0: 01764dcd9b49e15763176a4daa8f8e10f4902756616d3e834ef1341e80062ad5
      labels:
        app: gitea
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: gitea
        app.kubernetes.io/version: 1.22.3
        helm.sh/chart: gitea-10.6.0
        version: 1.22.3
    spec:
      containers:
        - env:
            - name: SSH_LISTEN_PORT
              value: "2222"
            - name: SSH_PORT
              value: "22"
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: TMPDIR
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
          image: gitea/gitea:1.22.3-rootless
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 200
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: gitea
          ports:
            - containerPort: 2222
              name: ssh
            - containerPort: 3000
              name: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      initContainers:
        - command:
            - /usr/sbin/init_directory_structure.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
          image: gitea/gitea:1.22.3-rootless
          imagePullPolicy: IfNotPresent
          name: init-directories
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbin
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
        - command:
            - /usr/sbin/config_environment.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
          image: gitea/gitea:1.22.3-rootless
          imagePullPolicy: IfNotPresent
          name: init-app-ini
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          volumeMounts:
            - mountPath: /usr/sbin
              name: config
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
            - mountPath: /env-to-ini-mounts/inlines/
              name: inline-config-sources
        - command:
            - /usr/sbin/configure_gitea.sh
          env:
            - name: GITEA_APP_INI
              value: /data/gitea/conf/app.ini
            - name: GITEA_CUSTOM
              value: /data/gitea
            - name: GITEA_WORK_DIR
              value: /data
            - name: GITEA_TEMP
              value: /tmp/gitea
            - name: HOME
              value: /data/gitea/git
            - name: GITEA_OAUTH_KEY_0
              valueFrom:
                secretKeyRef:
                  key: key
                  name: gitea-oidc-credentials
            - name: GITEA_OAUTH_SECRET_0
              valueFrom:
                secretKeyRef:
                  key: secret
                  name: gitea-oidc-credentials
            - name: GITEA_ADMIN_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-admin-user
            - name: GITEA_ADMIN_PASSWORD_MODE
              value: keepUpdated
          image: gitea/gitea:1.22.3-rootless
          imagePullPolicy: IfNotPresent
          name: configure-gitea
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/sbin
              name: init
            - mountPath: /tmp
              name: temp
            - mountPath: /data
              name: data
      securityContext:
        fsGroup: 1000
      terminationGracePeriodSeconds: 60
      volumes:
        - name: init
          secret:
            defaultMode: 110
            secretName: gitea-init
        - name: config
          secret:
            defaultMode: 110
            secretName: gitea
        - name: inline-config-sources
          secret:
            secretName: gitea-inline-config
        - emptyDir: {}
          name: temp
        - name: data
          persistentVolumeClaim:
            claimName: gitea
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: hajimari
      app.kubernetes.io/name: hajimari
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: hajimari
        app.kubernetes.io/name: hajimari
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: TZ
              value: UTC
          image: ghcr.io/toboshii/hajimari:v0.3.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          name: hajimari
          ports:
            - containerPort: 3000
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 0
            periodSeconds: 10
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 0
            periodSeconds: 5
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config/config.yaml
              name: hajimari-settings
              subPath: config.yaml
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: hajimari
      volumes:
        - configMap:
            name: hajimari-settings
          name: hajimari-settings
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: machine-learning
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-machine-learning
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: machine-learning
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: machine-learning
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql
            - name: DB_PASSWORD
              value: immich
            - name: DB_USERNAME
              value: immich
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
            - name: TRANSFORMERS_CACHE
              value: /cache
          image: ghcr.io/immich-app/immich-machine-learning:v1.123.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-machine-learning
          ports:
            - containerPort: 3003
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 3
              ephemeral-storage: 10Gi
            requests:
              cpu: 10m
              memory: 2Gi
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /cache
              name: cache
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - emptyDir: {}
          name: cache
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-server
  namespace: immich
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: server
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: 88c813a390a4e10d83e7da3a7df99e9aa98f2f6ce28cc391662822301a54b78e
      labels:
        app.kubernetes.io/instance: immich
        app.kubernetes.io/name: server
    spec:
      automountServiceAccountToken: true
      containers:
        - env:
            - name: DB_DATABASE_NAME
              value: immich
            - name: DB_HOSTNAME
              value: immich-postgresql
            - name: DB_PASSWORD
              value: immich
            - name: DB_USERNAME
              value: immich
            - name: IMMICH_CONFIG_FILE
              value: /config/immich-config.yaml
            - name: IMMICH_MACHINE_LEARNING_URL
              value: http://immich-machine-learning:3003
            - name: REDIS_HOSTNAME
              value: immich-redis-master
          image: ghcr.io/immich-app/immich-server:v1.123.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          name: immich-server
          ports:
            - containerPort: 2283
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /api/server/ping
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /config
              name: config
            - mountPath: /usr/src/app/upload
              name: library
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      serviceAccountName: default
      volumes:
        - name: config
          secret:
            secretName: immich-config
        - name: library
          persistentVolumeClaim:
            claimName: immich-library
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: cilium-operator
    app.kubernetes.io/part-of: cilium
    io.cilium/app: operator
    name: cilium-operator
  name: cilium-operator
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      io.cilium/app: operator
      name: cilium-operator
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 100%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/port: "9963"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: cilium-operator
        app.kubernetes.io/part-of: cilium
        io.cilium/app: operator
        name: cilium-operator
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  io.cilium/app: operator
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
            - --debug=$(CILIUM_DEBUG)
          command:
            - cilium-operator-generic
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_DEBUG
              valueFrom:
                configMapKeyRef:
                  key: debug
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/operator-generic:v1.16.5@sha256:f7884848483bbcd7b1e0ccfd34ba4546f258b460cb4b7e2f06a1bcc96ef88039
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
          name: cilium-operator
          ports:
            - containerPort: 9963
              hostPort: 9963
              name: prometheus
              protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp/cilium/config-map
              name: cilium-config-path
              readOnly: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      serviceAccountName: cilium-operator
      tolerations:
        - operator: Exists
      volumes:
        - configMap:
            name: cilium-config
          name: cilium-config-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-cloud-controller-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-cloud-controller-manager
    app.kubernetes.io/version: v0.7.0
    helm.sh/chart: proxmox-cloud-controller-manager-0.2.11
  name: proxmox-cloud-controller-manager
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: proxmox-cloud-controller-manager
      app.kubernetes.io/name: proxmox-cloud-controller-manager
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: ce080eff0c26b50fe73bf9fcda017c8ad47c1000729fd0c555cfe3535c6d6222
      labels:
        app.kubernetes.io/instance: proxmox-cloud-controller-manager
        app.kubernetes.io/name: proxmox-cloud-controller-manager
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      containers:
        - args:
            - --v=4
            - --cloud-provider=proxmox
            - --cloud-config=/etc/proxmox/config.yaml
            - --controllers=cloud-node,cloud-node-lifecycle
            - --leader-elect-resource-name=cloud-controller-manager-proxmox
            - --use-service-account-credentials
            - --secure-port=10258
            - --authorization-always-allow-paths=/healthz,/livez,/readyz,/metrics
          image: ghcr.io/sergelogvinov/proxmox-cloud-controller-manager:v0.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 30
            timeoutSeconds: 5
          name: proxmox-cloud-controller-manager
          ports:
            - containerPort: 10258
              name: metrics
              protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /etc/proxmox
              name: cloud-config
              readOnly: true
      enableServiceLinks: false
      initContainers: []
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroup: 10258
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 10258
        runAsNonRoot: true
        runAsUser: 10258
      serviceAccountName: proxmox-cloud-controller-manager
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          operator: Exists
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: proxmox-cloud-controller-manager
              app.kubernetes.io/name: proxmox-cloud-controller-manager
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
      volumes:
        - name: cloud-config
          secret:
            defaultMode: 416
            items:
              - key: config.yaml
                path: config.yaml
            secretName: proxmox-ccm-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netbird-agent
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app: netbird-agent
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: netbird-agent
      labels:
        app: netbird-agent
    spec:
      containers:
        - args:
            - TCP-LISTEN:443,fork
            - TCP:traefik.traefik.svc.cluster.local:443
          image: alpine/socat:1.8.0.0
          name: traefik-proxy
          ports:
            - containerPort: 443
              name: http
              protocol: TCP
        - args:
            - TCP-LISTEN:22,fork
            - TCP:traefik.traefik.svc.cluster.local:22
          image: alpine/socat:1.8.0.0
          name: gitea-ssh-proxy
          ports:
            - containerPort: 22
              name: ssh
              protocol: TCP
        - env:
            - name: CLOUDFLARE_API_TOKEN
              valueFrom:
                secretKeyRef:
                  key: cloudflare-dns-api-token
                  name: cloudflare-api-credentials
            - name: DOMAINS
              value: homelab.olav.ninja,*.homelab.olav.ninja
            - name: IP4_PROVIDER
              value: local.iface:wt0
            - name: IP6_PROVIDER
              value: none
            - name: UPDATE_CRON
              value: '@every 15m'
          image: favonia/cloudflare-ddns:1.15.1
          name: cloudflare-dns-updater
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
        - env:
            - name: NB_SETUP_KEY
              valueFrom:
                secretKeyRef:
                  key: setupKey
                  name: netbird-agent-setup-key
            - name: NB_HOSTNAME
              value: k8s-agent
            - name: NB_LOG_LEVEL
              value: info
            - name: NB_MANAGEMENT_URL
              value: https://netbird.homelab.olav.ninja
            - name: NB_ADMIN_URL
              value: https://netbird.homelab.olav.ninja
            - name: NB_CONFIG
              value: /config/config.json
          image: netbirdio/netbird:0.35.2
          name: netbird-agent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_ADMIN
                - PERFMON
                - BPF
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /config
              name: config
      volumes:
        - emptyDir: {}
          name: config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-management
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-management
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config: 3590c1c9714164bad8fcd0ddc8dbcd152c75e34751ef052c0eb0e918dd6b5f4f
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-management
    spec:
      containers:
        - args:
            - --log-level
            - info
            - --log-file
            - console
            - --dns-domain
            - netbird
          image: netbirdio/management:0.35.2
          imagePullPolicy: IfNotPresent
          name: netbird-management
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          resources: {}
          securityContext: {}
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /var/lib/netbird
              name: management
      initContainers:
        - args:
            - |
              go install github.com/drone/envsubst/cmd/envsubst@latest && envsubst < /tmp/netbird/management.tmpl.json > /etc/netbird/management.json && cat /etc/netbird/management.json
          command:
            - /bin/sh
            - -c
          env:
            - name: NETBIRD_RELAY_URI
              value: rels://netbird.homelab.olav.ninja:443
            - name: NETBIRD_RELAY_SECRET
              valueFrom:
                secretKeyRef:
                  key: authSecret
                  name: netbird-relay-credentials
            - name: NETBIRD_SIGNAL_URI
              value: netbird.homelab.olav.ninja:443
            - name: NETBIRD_SIGNAL_PROTOCOL
              value: https
            - name: NETBIRD_STUN_URI
              value: stun:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_URI
              value: turn:coturn.homelab.olav.ninja:3478
            - name: NETBIRD_TURN_USER
              value: netbird
            - name: NETBIRD_TURN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: netbird-turn-credentials
            - name: NETBIRD_AUTH_OIDC_CONFIGURATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/.well-known/openid-configuration
            - name: NETBIRD_MGMT_API_CERT_FILE
              value: ""
            - name: NETBIRD_MGMT_API_CERT_KEY_FILE
              value: ""
            - name: NETBIRD_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_USER_ID_CLAIM
            - name: NETBIRD_AUTH_DEVICE_AUTH_PROVIDER
              value: hosted
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUDIENCE
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: NETBIRD_AUTH_DEVICE_AUTH_CLIENT_ID
              value: netbird
            - name: NETBIRD_AUTH_DEVICE_AUTH_DEVICE_AUTHORIZATION_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/auth
            - name: NETBIRD_AUTH_DEVICE_AUTH_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
            - name: NETBIRD_AUTH_DEVICE_AUTH_SCOPE
              value: openid
            - name: NETBIRD_AUTH_DEVICE_AUTH_USE_ID_TOKEN
              value: "false"
            - name: NETBIRD_IDP_MANAGER_TYPE
              value: keycloak
            - name: NETBIRD_IDP_CLIENT_ID
              value: netbird-backend
            - name: NETBIRD_IDP_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: clientSecret
                  name: netbird-backend-oidc-credentials
            - name: NETBIRD_IDP_GRANT_TYPE
              value: client_credentials
            - name: NETBIRD_IDP_KEYCLOAK_ADMIN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/admin/realms/homelab
            - name: NETBIRD_IDP_KEYCLOAK_TOKEN_ENDPOINT
              value: https://keycloak.homelab.olav.ninja/realms/homelab/protocol/openid-connect/token
          image: golang:latest
          imagePullPolicy: IfNotPresent
          name: configure
          volumeMounts:
            - mountPath: /etc/netbird
              name: config
            - mountPath: /tmp/netbird
              name: config-template
      securityContext: {}
      serviceAccountName: netbird-backend-management
      volumes:
        - emptyDir:
            medium: Memory
          name: config
        - configMap:
            name: netbird-backend-management
          name: config-template
        - name: management
          persistentVolumeClaim:
            claimName: netbird-backend-management
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-signal
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-backend
      app.kubernetes.io/name: netbird-signal
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-backend
        app.kubernetes.io/name: netbird-signal
    spec:
      containers:
        - args:
            - --port
            - "80"
            - --log-level
            - info
            - --log-file
            - console
          image: netbirdio/signal:0.35.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            tcpSocket:
              port: https
          name: netbird-signal
          ports:
            - containerPort: 80
              name: https
              protocol: TCP
          readinessProbe:
            tcpSocket:
              port: https
          resources: {}
          securityContext: null
          volumeMounts:
            - mountPath: /var/lib/netbird
              name: signal
      securityContext: null
      serviceAccountName: netbird-backend-signal
      volumes:
        - name: signal
          persistentVolumeClaim:
            claimName: netbird-backend-signal
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.1.3
    helm.sh/chart: netbird-dashboard-1.0.0
  name: netbird-dashboard
  namespace: netbird
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: netbird-dashboard
      app.kubernetes.io/name: netbird-dashboard
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-dashboard
        app.kubernetes.io/name: netbird-dashboard
    spec:
      containers:
        - args:
            - |
              sed -i 's/listen \[\:\:\]\:80 default_server\;//g' /etc/nginx/http.d/default.conf && /usr/bin/supervisord -c /etc/supervisord.conf
          command:
            - /bin/sh
            - -c
          env:
            - name: AUTH_AUDIENCE
              value: netbird
            - name: AUTH_AUTHORITY
              value: https://keycloak.homelab.olav.ninja/realms/homelab
            - name: AUTH_CLIENT_ID
              value: netbird
            - name: AUTH_SUPPORTED_SCOPES
              value: openid profile email offline_access netbird-api
            - name: USE_AUTH0
              value: "false"
            - name: NETBIRD_MGMT_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
            - name: NETBIRD_MGMT_GRPC_API_ENDPOINT
              value: https://netbird.homelab.olav.ninja
          image: netbirdio/dashboard:v2.8.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /
              port: http
          name: netbird-dashboard
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources: {}
          securityContext: {}
      securityContext: {}
      serviceAccountName: netbird-dashboard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: netbird-relay
  name: netbird-relay
  namespace: netbird
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: netbird-relay
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: netbird-relay
        app.kubernetes.io/name: netbird-relay
    spec:
      containers:
        - env:
            - name: NB_LOG_LEVEL
              value: info
            - name: NB_LISTEN_ADDRESS
              value: :80
            - name: NB_AUTH_SECRET
              valueFrom:
                secretKeyRef:
                  key: authSecret
                  name: netbird-relay-credentials
            - name: NB_EXPOSED_ADDRESS
              value: rels://netbird.homelab.olav.ninja:443
          image: netbirdio/relay:0.35.2
          imagePullPolicy: IfNotPresent
          name: netbird-relay
          ports:
            - containerPort: 80
              name: relay
              protocol: TCP
      replicas: 1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    helm.sh/chart: nextcloud-6.6.2
  name: nextcloud
  namespace: nextcloud
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: app
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: nextcloud
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204
        nextcloud-config-hash: 93a9742eed1608d05f159d9fa1830f38ea212bcc9440dceccdf0528d6ed6aefc
        php-config-hash: ef6725a34482d427b584b8ddff54804f14e7c98827ae18ff7642f7cf0dd254af
      labels:
        app.kubernetes.io/component: app
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/name: nextcloud
    spec:
      containers:
        - env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: MYSQL_HOST
              value: nextcloud-mariadb
            - name: MYSQL_DATABASE
              value: nextcloud
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
          image: nextcloud:30.0.4-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nextcloud
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
                - name: Host
                  value: nextcloud.homelab.olav.ninja
              path: /status.php
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
        - command:
            - /cron.sh
          env:
            - name: OVERWRITEPROTOCOL
              value: https
            - name: MYSQL_HOST
              value: nextcloud-mariadb
            - name: MYSQL_DATABASE
              value: nextcloud
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: NEXTCLOUD_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  key: username
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: nextcloud-admin-user
            - name: NEXTCLOUD_TRUSTED_DOMAINS
              value: nextcloud.homelab.olav.ninja
            - name: NEXTCLOUD_DATA_DIR
              value: /var/www/html/data
          image: nextcloud:30.0.4-apache
          imagePullPolicy: IfNotPresent
          name: nextcloud-cron
          resources: {}
          volumeMounts:
            - mountPath: /var/www/
              name: nextcloud-main
              subPath: root
            - mountPath: /var/www/html
              name: nextcloud-main
              subPath: html
            - mountPath: /var/www/html/data
              name: nextcloud-main
              subPath: data
            - mountPath: /var/www/html/config
              name: nextcloud-main
              subPath: config
            - mountPath: /var/www/html/custom_apps
              name: nextcloud-main
              subPath: custom_apps
            - mountPath: /var/www/tmp
              name: nextcloud-main
              subPath: tmp
            - mountPath: /var/www/html/themes
              name: nextcloud-main
              subPath: themes
            - mountPath: /var/www/html/config/mycustom.config.php
              name: nextcloud-config
              subPath: mycustom.config.php
            - mountPath: /var/www/html/config/.htaccess
              name: nextcloud-config
              subPath: .htaccess
            - mountPath: /var/www/html/config/apache-pretty-urls.config.php
              name: nextcloud-config
              subPath: apache-pretty-urls.config.php
            - mountPath: /var/www/html/config/apcu.config.php
              name: nextcloud-config
              subPath: apcu.config.php
            - mountPath: /var/www/html/config/apps.config.php
              name: nextcloud-config
              subPath: apps.config.php
            - mountPath: /var/www/html/config/autoconfig.php
              name: nextcloud-config
              subPath: autoconfig.php
            - mountPath: /var/www/html/config/redis.config.php
              name: nextcloud-config
              subPath: redis.config.php
            - mountPath: /var/www/html/config/reverse-proxy.config.php
              name: nextcloud-config
              subPath: reverse-proxy.config.php
            - mountPath: /var/www/html/config/s3.config.php
              name: nextcloud-config
              subPath: s3.config.php
            - mountPath: /var/www/html/config/smtp.config.php
              name: nextcloud-config
              subPath: smtp.config.php
            - mountPath: /var/www/html/config/swift.config.php
              name: nextcloud-config
              subPath: swift.config.php
            - mountPath: /var/www/html/config/upgrade-disable-web.config.php
              name: nextcloud-config
              subPath: upgrade-disable-web.config.php
            - mountPath: /usr/local/etc/php/conf.d/uploadLimit.ini
              name: nextcloud-phpconfig
              subPath: uploadLimit.ini
      initContainers:
        - command:
            - sh
            - -c
            - until mysql --host=nextcloud-mariadb --user=${MYSQL_USER} --password=${MYSQL_PASSWORD} --execute="SELECT 1;"; do echo waiting for mysql; sleep 2; done;
          env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: mariadb-username
                  name: nextcloud-database-auth
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          name: mariadb-isalive
          resources: {}
          securityContext: {}
      securityContext:
        fsGroup: 33
      volumes:
        - name: nextcloud-main
          persistentVolumeClaim:
            claimName: nextcloud-data
        - configMap:
            name: nextcloud-config
          name: nextcloud-config
        - configMap:
            name: nextcloud-phpconfig
          name: nextcloud-phpconfig
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: sealed-secrets
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: sealed-secrets
        app.kubernetes.io/version: 0.27.3
        helm.sh/chart: sealed-secrets-2.5.0
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: sealed-secrets
                    app.kubernetes.io/name: sealed-secrets
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - args:
            - --key-prefix
            - sealed-secrets-key
            - --update-status
            - --key-renew-period
            - "0"
          command:
            - /controller
          image: docker.io/bitnami/sealed-secrets-controller:0.27.3-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: sealed-secrets
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: sealed-secrets
      volumes:
        - emptyDir: {}
          name: empty-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-33.2.1
  name: traefik
  namespace: traefik
spec:
  minReadySeconds: 0
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: traefik-traefik
      app.kubernetes.io/name: traefik
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9100"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: traefik-traefik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: traefik
        helm.sh/chart: traefik-33.2.1
    spec:
      automountServiceAccountToken: true
      containers:
        - args:
            - --global.checknewversion
            - --global.sendanonymoususage
            - --entryPoints.metrics.address=:9100/tcp
            - --entryPoints.ssh.address=:2222/tcp
            - --entryPoints.traefik.address=:8080/tcp
            - --entryPoints.web.address=:8000/tcp
            - --entryPoints.webpublic.address=:9443/tcp
            - --entryPoints.websecure.address=:8443/tcp
            - --entryPoints.websecure.asDefault=true
            - --api.dashboard=true
            - --ping=true
            - --metrics.prometheus=true
            - --metrics.prometheus.entrypoint=metrics
            - --providers.kubernetescrd
            - --providers.kubernetescrd.allowEmptyServices=true
            - --providers.kubernetesingress
            - --providers.kubernetesingress.allowEmptyServices=true
            - --providers.kubernetesingress.ingressendpoint.publishedservice=traefik/traefik
            - --entryPoints.webpublic.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.webpublic.http.tls=true
            - --entryPoints.webpublic.transport.respondingTimeouts.readTimeout=0
            - --entryPoints.websecure.http.middlewares=traefik-securityheaders@kubernetescrd
            - --entryPoints.websecure.http.tls=true
            - --entryPoints.websecure.transport.respondingTimeouts.readTimeout=0
            - --log.level=INFO
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: docker.io/traefik:v3.2.2
          imagePullPolicy: IfNotPresent
          lifecycle: null
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
            - containerPort: 9100
              name: metrics
              protocol: TCP
            - containerPort: 2222
              name: ssh
              protocol: TCP
            - containerPort: 8080
              name: traefik
              protocol: TCP
            - containerPort: 8000
              name: web
              protocol: TCP
            - containerPort: 9443
              name: webpublic
              protocol: TCP
            - containerPort: 8443
              name: websecure
              protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: null
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /tmp
              name: tmp
      hostNetwork: false
      securityContext:
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      serviceAccountName: traefik
      terminationGracePeriodSeconds: 60
      volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.5.20
  name: gitea-postgresql
  namespace: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
  serviceName: gitea-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: gitea
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 16.3.0
        helm.sh/chart: postgresql-15.5.20
      name: gitea-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: gitea
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: gitea
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-postgresql
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: postgres-password
                  name: gitea-postgresql
            - name: POSTGRES_DATABASE
              value: gitea
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/bitnami/postgresql:15-debian-11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: gitea-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - emptyDir:
            medium: Memory
          name: dshm
        - name: data
          persistentVolumeClaim:
            claimName: gitea-postgresql
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: postgresql
  serviceName: immich-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: immich
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.0.0
        helm.sh/chart: postgresql-16.0.0
      name: immich-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: immich
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: immich-postgresql
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: postgres-password
                  name: immich-postgresql
            - name: POSTGRES_DATABASE
              value: immich
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "immich" -d "dbname=immich" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "immich" -d "dbname=immich" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 1536Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 1024Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 999
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /docker-entrypoint-initdb.d/
              name: custom-init-scripts
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      initContainers:
        - command:
            - /bin/sh
            - -ec
            - |
              chown 999:999 /bitnami/postgresql
              mkdir -p /bitnami/postgresql/data
              chmod 700 /bitnami/postgresql/data
              find /bitnami/postgresql -mindepth 1 -maxdepth 1 -not -name "conf" -not -name ".snapshot" -not -name "lost+found" | \
                xargs -r chown -R 999:999
              chmod -R 777 /dev/shm
          image: docker.io/bitnami/os-shell:12-debian-12-r30
          imagePullPolicy: IfNotPresent
          name: init-chmod-data
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /bitnami/postgresql
              name: data
            - mountPath: /dev/shm
              name: dshm
      securityContext:
        fsGroup: 999
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: immich-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            name: immich-postgresql-init-scripts
          name: custom-init-scripts
        - emptyDir:
            medium: Memory
          name: dshm
        - name: data
          persistentVolumeClaim:
            claimName: immich-postgresql
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  serviceName: immich-redis-headless
  template:
    metadata:
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 43cdf68c28f3abe25ce017a82f74dbf2437d1900fd69df51a55a3edf6193d141
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: immich
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.5
        helm.sh/chart: redis-19.5.3
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: immich
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
            - /bin/bash
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          image: docker.io/bitnami/redis:7.2.5-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
            - containerPort: 6379
              name: redis
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /opt/bitnami/scripts/start-scripts
              name: start-scripts
            - mountPath: /health
              name: health
            - mountPath: /data
              name: redis-data
            - mountPath: /opt/bitnami/redis/mounted-etc
              name: config
            - mountPath: /opt/bitnami/redis/etc/
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
      enableServiceLinks: true
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: immich-redis-master
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 493
            name: immich-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: immich-redis-health
          name: health
        - configMap:
            name: immich-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
        - emptyDir: {}
          name: redis-data
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak
  namespace: keycloak
spec:
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
  serviceName: keycloak-headless
  template:
    metadata:
      annotations:
        checksum/configmap-env-vars: 303a8a83b7a2cb4a1baf82976b2c64eb1b87b0e80ed6f226ebc11d92cd7645c2
      labels:
        app.kubernetes.io/component: keycloak
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: keycloak
        app.kubernetes.io/version: 26.0.7
        helm.sh/chart: keycloak-24.3.2
    spec:
      affinity:
        nodeAffinity: null
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: keycloak
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: true
      containers:
        - env:
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KC_BOOTSTRAP_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: keycloak-admin-password
            - name: KEYCLOAK_DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: keycloak-db-credentials
            - name: KEYCLOAK_HTTP_RELATIVE_PATH
              value: /
            - name: KC_SPI_ADMIN_REALM
              value: master
          envFrom:
            - configMapRef:
                name: keycloak-env-vars
          image: docker.io/bitnami/keycloak:26.0.7-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 300
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 5
          name: keycloak
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 7800
              name: discovery
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /realms/master
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /bitnami/keycloak
              name: empty-dir
              subPath: app-volume-dir
            - mountPath: /opt/bitnami/keycloak/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/keycloak/lib/quarkus
              name: empty-dir
              subPath: app-quarkus-dir
            - mountPath: /opt/bitnami/keycloak/data
              name: empty-dir
              subPath: app-data-dir
            - mountPath: /opt/bitnami/keycloak/providers
              name: empty-dir
              subPath: app-providers-dir
      enableServiceLinks: true
      initContainers:
        - args:
            - -ec
            - |
              . /opt/bitnami/scripts/liblog.sh

              info "Copying writable dirs to empty dir"
              # In order to not break the application functionality we need to make some
              # directories writable, so we need to copy it to an empty dir volume
              cp -r --preserve=mode /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir
              cp -r --preserve=mode /opt/bitnami/keycloak/data /emptydir/app-data-dir
              cp -r --preserve=mode /opt/bitnami/keycloak/providers /emptydir/app-providers-dir
              info "Copy operation completed"
          command:
            - /bin/bash
          image: docker.io/bitnami/keycloak:26.0.7-debian-12-r0
          imagePullPolicy: IfNotPresent
          name: prepare-write-dirs
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /emptydir
              name: empty-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: keycloak
      volumes:
        - emptyDir: {}
          name: empty-dir
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.2.0
    helm.sh/chart: postgresql-16.2.5
  name: keycloak-postgresql
  namespace: keycloak
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
  serviceName: keycloak-postgresql-hl
  template:
    metadata:
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 17.2.0
        helm.sh/chart: postgresql-16.2.5
      name: keycloak-postgresql
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: /bitnami/postgresql
            - name: PGDATA
              value: /bitnami/postgresql/data
            - name: POSTGRES_USER
              value: bn_keycloak
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: keycloak-db-credentials
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: postgres-password
                  name: keycloak-db-credentials
            - name: POSTGRES_DATABASE
              value: bitnami_keycloak
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: error
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: pgaudit
          image: docker.io/bitnami/postgresql:17.2.0-debian-12-r2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "bn_keycloak" -d "dbname=bitnami_keycloak" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
            - containerPort: 5432
              name: tcp-postgresql
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "bn_keycloak" -d "dbname=bitnami_keycloak" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/postgresql/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/postgresql/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /bitnami/postgresql
              name: data
      hostIPC: false
      hostNetwork: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: keycloak-postgresql
      volumes:
        - emptyDir: {}
          name: empty-dir
        - emptyDir:
            medium: Memory
          name: dshm
        - emptyDir: {}
          name: data
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: mariadb
  serviceName: nextcloud-mariadb
  template:
    metadata:
      annotations:
        checksum/configuration: fb262c2f871ddf4c9fa925e29eb7198153cfc48305559085fcf1ef74cf716807
      labels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: nextcloud
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mariadb
        app.kubernetes.io/version: 11.3.2
        helm.sh/chart: mariadb-18.2.0
    spec:
      affinity:
        nodeAffinity: null
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: mariadb
                topologyKey: kubernetes.io/hostname
              weight: 1
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: nextcloud
                    app.kubernetes.io/name: mariadb
                topologyKey: kubernetes.io/hostname
              weight: 1
      automountServiceAccountToken: false
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MARIADB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-root-password
                  name: nextcloud-database-auth
            - name: MARIADB_USER
              value: nextcloud
            - name: MARIADB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: mariadb-password
                  name: nextcloud-database-auth
            - name: MARIADB_DATABASE
              value: nextcloud
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mariadb
          ports:
            - containerPort: 3306
              name: mysql
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin ping -uroot -p"${password_aux}"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /bitnami/mariadb
              name: data
            - mountPath: /opt/bitnami/mariadb/conf/my.cnf
              name: config
              subPath: my.cnf
            - mountPath: /tmp
              name: empty-dir
              subPath: tmp-dir
            - mountPath: /opt/bitnami/mariadb/conf
              name: empty-dir
              subPath: app-conf-dir
            - mountPath: /opt/bitnami/mariadb/tmp
              name: empty-dir
              subPath: app-tmp-dir
            - mountPath: /opt/bitnami/mariadb/logs
              name: empty-dir
              subPath: app-logs-dir
      initContainers:
        - args:
            - -ec
            - |
              #!/bin/bash

              . /opt/bitnami/scripts/libfs.sh
              # We copy the logs folder because it has symlinks to stdout and stderr
              if ! is_dir_empty /opt/bitnami/mariadb/logs; then
                cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir
              fi
          command:
            - /bin/bash
          image: docker.io/bitnami/mariadb:11.3.2-debian-12-r5
          imagePullPolicy: IfNotPresent
          name: preserve-logs-symlinks
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /emptydir
              name: empty-dir
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: nextcloud-mariadb
      volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            name: nextcloud-mariadb
          name: config
        - name: data
          persistentVolumeClaim:
            claimName: nextcloud-mariadb
  updateStrategy:
    type: RollingUpdate
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: energisk-cache-prime
  namespace: default
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - args:
                - |
                  URLS=" https://api.energisk.app/price?area=BE&currency=EUR https://api.energisk.app/price?area=BG&currency=BGN https://api.energisk.app/price?area=CH&currency=CHF https://api.energisk.app/price?area=CZ&currency=CZK https://api.energisk.app/price?area=DE&currency=EUR https://api.energisk.app/price?area=DK1&currency=DKK https://api.energisk.app/price?area=DK2&currency=DKK https://api.energisk.app/price?area=EE&currency=EUR https://api.energisk.app/price?area=ES&currency=EUR https://api.energisk.app/price?area=FI&currency=EUR https://api.energisk.app/price?area=FR&currency=EUR https://api.energisk.app/price?area=GR&currency=EUR https://api.energisk.app/price?area=HR&currency=EUR https://api.energisk.app/price?area=HU&currency=HUF https://api.energisk.app/price?area=IE&currency=EUR https://api.energisk.app/price?area=LT&currency=EUR https://api.energisk.app/price?area=LU&currency=EUR https://api.energisk.app/price?area=LV&currency=EUR https://api.energisk.app/price?area=NL&currency=EUR https://api.energisk.app/price?area=NO1&currency=NOK https://api.energisk.app/price?area=NO2&currency=NOK https://api.energisk.app/price?area=NO3&currency=NOK https://api.energisk.app/price?area=NO4&currency=NOK https://api.energisk.app/price?area=NO5&currency=NOK https://api.energisk.app/price?area=NORD&currency=EUR https://api.energisk.app/price?area=PL&currency=PLN https://api.energisk.app/price?area=PT&currency=EUR https://api.energisk.app/price?area=RO&currency=RON https://api.energisk.app/price?area=RS&currency=RSD https://api.energisk.app/price?area=SARD&currency=EUR https://api.energisk.app/price?area=SE1&currency=SEK https://api.energisk.app/price?area=SE2&currency=SEK https://api.energisk.app/price?area=SE3&currency=SEK https://api.energisk.app/price?area=SE4&currency=SEK https://api.energisk.app/price?area=SI&currency=EUR https://api.energisk.app/price?area=SK&currency=EUR https://api.energisk.app/price?area=SICI&currency=EUR https://api.energisk.app/price?area=SUD&currency=EUR "
                  for URL in $URLS; do

                    curl -s -o /dev/null -w '%{url_effective}\t%{http_code}\t%{time_total}s\n' $URL\&date=$(date -d tomorrow +'%Y-%m-%d');
                  done
              command:
                - /bin/sh
                - -c
              image: rockylinux:9-minimal
              name: curl-container
          restartPolicy: Never
  schedule: 0 13 * * *
---
apiVersion: batch/v1
kind: CronJob
metadata:
  labels:
    app.kubernetes.io/name: hubble-generate-certs
    app.kubernetes.io/part-of: cilium
    k8s-app: hubble-generate-certs
  name: hubble-generate-certs
  namespace: kube-system
spec:
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            k8s-app: hubble-generate-certs
        spec:
          affinity: null
          automountServiceAccountToken: true
          containers:
            - args:
                - --ca-generate
                - --ca-reuse-secret
                - --ca-secret-namespace=kube-system
                - --ca-secret-name=cilium-ca
                - --ca-common-name=Cilium CA
              command:
                - /usr/bin/cilium-certgen
              env:
                - name: CILIUM_CERTGEN_CONFIG
                  value: |
                    certs:
                    - name: hubble-server-certs
                      namespace: kube-system
                      commonName: "*.default.hubble-grpc.cilium.io"
                      hosts:
                      - "*.default.hubble-grpc.cilium.io"
                      usage:
                      - signing
                      - key encipherment
                      - server auth
                      - client auth
                      validity: 8760h
              image: quay.io/cilium/certgen:v0.2.0@sha256:169d93fd8f2f9009db3b9d5ccd37c2b753d0989e1e7cd8fe79f9160c459eef4f
              imagePullPolicy: IfNotPresent
              name: certgen
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
          hostNetwork: false
          restartPolicy: OnFailure
          securityContext:
            seccompProfile:
              type: RuntimeDefault
          serviceAccount: hubble-generate-certs
          serviceAccountName: hubble-generate-certs
      ttlSecondsAfterFinished: 1800
  schedule: 0 0 1 */4 *
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.5.20
  name: gitea-postgresql
  namespace: gitea
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: master
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis-master
  namespace: immich
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak
  namespace: keycloak
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.2.0
    helm.sh/chart: postgresql-16.2.5
  name: keycloak-postgresql
  namespace: keycloak
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/name: mariadb
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: sealed-secrets
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sealed-secrets
    app.kubernetes.io/version: 0.27.3
    helm.sh/chart: sealed-secrets-2.5.0
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: sealed-secrets
      app.kubernetes.io/name: sealed-secrets
---
apiVersion: apiextensions.crossplane.io/v1
kind: CompositeResourceDefinition
metadata:
  name: xbuiltinobjects.keycloak.crossplane.io
  namespace: crossplane
spec:
  group: keycloak.crossplane.io
  names:
    kind: XBuiltinObjects
    plural: xbuiltinobjects
  versions:
    - name: v1alpha1
      referenceable: true
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                builtinAuthenticationFlows:
                  description: List of authentication flows to import from the realm
                  items:
                    type: string
                  type: array
                builtinClients:
                  description: List of clients to import from the realm
                  items:
                    type: string
                  type: array
                builtinRealmRoles:
                  description: List of realm roles to import from the realm
                  items:
                    enum:
                      - offline_access
                      - uma_authorization
                      - admin
                      - create-realm
                      - default-roles-master
                    type: string
                  type: array
                providerConfigName:
                  description: Name of the provider config to attach to the imported clients/roles
                  type: string
                providerSecretName:
                  description: Name of the secret containing the provider credentials (Secret must have a label with key=type and value=provider-credentials to be found)
                  type: string
                realm:
                  description: Realm to import the builtin clients/roles from
                  type: string
              required:
                - providerConfigName
                - providerSecretName
                - realm
              type: object
          required:
            - spec
          type: object
      served: true
---
apiVersion: apiextensions.crossplane.io/v1
kind: CompositeResourceDefinition
metadata:
  name: xoidcclients.oidc.homelab.olav.ninja
  namespace: crossplane
spec:
  group: oidc.homelab.olav.ninja
  names:
    kind: XOidcClient
    plural: xoidcclients
  versions:
    - name: v1alpha1
      referenceable: true
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                baseUrl:
                  description: Default URL to use when the auth server needs to redirect or link back to the client.
                  type: string
                clientId:
                  description: The Client ID for this client, referenced in the URI during authentication and in issued tokens
                  type: string
                clientSecretSecretRef:
                  description: |-
                    The client or client secret registered within the identity provider. This field is able to obtain its value from vault, use $${vault.ID} format.
                    Client Secret.
                  properties:
                    key:
                      description: The key to select.
                      type: string
                    name:
                      description: Name of the secret.
                      type: string
                    namespace:
                      description: Namespace of the secret.
                      type: string
                  required:
                    - key
                    - name
                    - namespace
                  type: object
                defaultScopes:
                  description: The default scopes to be requested when asking for authorization
                  items:
                    type: string
                  type: array
                description:
                  description: The description of this client in the GUI
                  type: string
                displayName:
                  description: The display name of this client in the GUI
                  type: string
                grantTypes:
                  description: A list of grant types that should be enabled for the client
                  items:
                    type: string
                  type: array
                postLogoutRedirectUris:
                  description: A list of valid URIs a browser is permitted to redirect to after a successful logout.
                  items:
                    type: string
                  type: array
                realm:
                  description: The realm this client is attached to
                  type: string
                redirectUris:
                  description: |-
                    A list of valid URIs a browser is permitted to redirect to after a successful login or logout. Simple
                    wildcards in the form of an asterisk can be used here. This attribute must be set if either standard_flow_enabled or implicit_flow_enabled
                    is set to true.
                  items:
                    type: string
                  type: array
                serviceAccountRoles:
                  description: A list of roles to assign to the clients service account
                  items:
                    properties:
                      client:
                        type: string
                      realm:
                        type: string
                      role:
                        type: string
                    type: object
                  type: array
                type:
                  description: Specifies the type of client
                  type: string
                webOrigins:
                  description: |-
                    A list of allowed CORS origins. To permit all valid
                    redirect URIs, add +. Note that this will not include the *
                    wildcard. To permit all origins, explicitly add *.
                  items:
                    type: string
                  type: array
              required:
                - clientId
                - realm
              type: object
          required:
            - spec
          type: object
      served: true
---
apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: keycloak-builtin-objects
  namespace: crossplane
spec:
  compositeTypeRef:
    apiVersion: keycloak.crossplane.io/v1alpha1
    kind: XBuiltinObjects
  mode: Pipeline
  pipeline:
    - functionRef:
        name: function-extra-resources
      input:
        apiVersion: extra-resources.fn.crossplane.io/v1beta1
        kind: Input
        spec:
          extraResources:
            - apiVersion: v1
              into: secrets
              kind: Secret
              selector:
                matchLabels:
                  - key: type
                    type: Value
                    value: provider-credentials
                maxMatch: 100
                minMatch: 1
              type: Selector
      step: pull-provider-configs
    - functionRef:
        name: function-keycloak-builtin-objects
      step: keycloak-builtin-objects
    - functionRef:
        name: function-auto-ready
      step: automatically-detect-ready-composed-resources
---
apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: keycloak-oidc-client
  namespace: crossplane
spec:
  compositeTypeRef:
    apiVersion: oidc.homelab.olav.ninja/v1alpha1
    kind: XOidcClient
  mode: Pipeline
  pipeline:
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: Client
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}
            spec:
              forProvider:
                name: {{ .observed.composite.resource.spec.displayName }}
                accessType: {{ .observed.composite.resource.spec.type }}
                clientId: {{ .observed.composite.resource.spec.clientId }}
                {{ with .observed.composite.resource.spec.clientSecretSecretRef }}
                clientSecretSecretRef: {{ toYaml . | nindent 6 }}
                {{ end }}
                description: {{ .observed.composite.resource.spec.description }}
                {{ with .observed.composite.resource.spec.baseUrl }}
                baseUrl: {{ . }}
                {{ end }}
                {{ with .observed.composite.resource.spec.redirectUris }}
                validRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.postLogoutRedirectUris }}
                validPostLogoutRedirectUris: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ with .observed.composite.resource.spec.webOrigins }}
                webOrigins: {{ toYaml . | nindent 6 }}
                {{ end }}
                {{ if has "client_credentials" .observed.composite.resource.spec.grantTypes }}
                serviceAccountsEnabled: true
                {{ end }}
                {{ if has "code" .observed.composite.resource.spec.grantTypes }}
                standardFlowEnabled: true
                {{ end }}
                {{ if has "device_code" .observed.composite.resource.spec.grantTypes }}
                oauth2DeviceAuthorizationGrantEnabled: true
                {{ end }}
                {{ if has "password" .observed.composite.resource.spec.grantTypes }}
                directAccessGrantsEnabled: true
                {{- end }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
        kind: GoTemplate
        source: Inline
      step: create-client
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-audience-mapper
            spec:
              forProvider:
                name: Set token audience
                protocol: openid-connect
                protocolMapper: oidc-audience-mapper
                config:
                  included.client.audience: "{{ .observed.composite.resource.spec.clientId }}"
                  id.token.claim: "false"
                  access.token.claim: "true"
                  introspection.token.claim: "true"
                  userinfo.token.claim: "false"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-audience-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            apiVersion: client.keycloak.crossplane.io/v1alpha1
            kind: ProtocolMapper
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-sub-mapper
            spec:
              forProvider:
                name: Username as sub claim
                protocol: openid-connect
                protocolMapper: oidc-usermodel-property-mapper
                config:
                  user.attribute: username
                  id.token.claim: "true"
                  access.token.claim: "true"
                  claim.name: sub
                  userinfo.token.claim: "true"
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-usermodel-property-mapper
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ if ne $.observed.resources nil }}
            {{ if .observed.composite.resource.spec.defaultScopes }}
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientDefaultScopes
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ .observed.composite.resource.metadata.name }}-default-scopes
            spec:
              forProvider:
                {{ with .observed.composite.resource.spec.defaultScopes }}
                defaultScopes: {{ toYaml . | nindent 6 }}
                {{ end }}
                clientId: {{ ( index .observed.resources .observed.composite.resource.metadata.name ).resource.status.atProvider.id | default "null" }}
                realmIdRef:
                  name: {{ .observed.composite.resource.spec.realm }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-client-default-scopes
    - functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        inline:
          template: |
            {{ range .observed.composite.resource.spec.serviceAccountRoles }}
            ---
            apiVersion: meta.gotemplating.fn.crossplane.io/v1alpha1
            kind: ExtraResources
            requirements:
              client:
                apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
                kind: Client
                matchName: {{ .client }}
              realm:
                apiVersion: realm.keycloak.crossplane.io/v1alpha1
                kind: Realm
                matchName: {{ .realm }}
            {{ end }}
            {{ if and (ne .observed.resources nil) (ne .extraResources nil) }}
            {{ range $i, $serviceAccountRole := .observed.composite.resource.spec.serviceAccountRoles }}
            {{ $client := (index (index $.extraResources "client").items $i).resource }}
            {{ $realm := (index (index $.extraResources "realm").items $i).resource }}
            ---
            apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
            kind: ClientServiceAccountRole
            metadata:
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: {{ $.observed.composite.resource.metadata.name }}-{{ .role }}
            spec:
              forProvider:
                clientId: {{ $client.status.atProvider.id }}
                realmId: {{ $realm.status.atProvider.id }}
                role: {{ $serviceAccountRole.role }}
                serviceAccountUserId: {{ ( index $.observed.resources $.observed.composite.resource.metadata.name ).resource.status.atProvider.serviceAccountUserId | default "null" }}
            {{ end }}
            {{ end }}
        kind: GoTemplate
        source: Inline
      step: create-service-account-role
    - functionRef:
        name: function-auto-ready
      step: automatically-detect-ready-composed-resources
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/instance: proxmox-csi-plugin
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: proxmox-csi-plugin
    app.kubernetes.io/version: v0.9.0
    helm.sh/chart: proxmox-csi-plugin-0.3.1
  name: proxmox-csi-plugin-node
  namespace: csi-proxmox
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: node
      app.kubernetes.io/instance: proxmox-csi-plugin
      app.kubernetes.io/name: proxmox-csi-plugin
  template:
    metadata:
      labels:
        app.kubernetes.io/component: node
        app.kubernetes.io/instance: proxmox-csi-plugin
        app.kubernetes.io/name: proxmox-csi-plugin
    spec:
      containers:
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --node-id=$(NODE_NAME)
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          image: ghcr.io/sergelogvinov/proxmox-csi-node:v0.9.0
          imagePullPolicy: IfNotPresent
          name: proxmox-csi-plugin-node
          resources: {}
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - CHOWN
                - DAC_OVERRIDE
              drop:
                - ALL
            privileged: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
            - mountPath: /var/lib/kubelet
              mountPropagation: Bidirectional
              name: kubelet
            - mountPath: /dev
              name: dev
            - mountPath: /sys
              name: sys
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.proxmox.sinextra.dev/csi.sock
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.12.0
          imagePullPolicy: IfNotPresent
          name: csi-node-driver-registrar
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
            - mountPath: /registration
              name: registration
        - args:
            - -v=5
            - --csi-address=unix:///csi/csi.sock
          image: registry.k8s.io/sig-storage/livenessprobe:v2.14.0
          imagePullPolicy: IfNotPresent
          name: liveness-probe
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /csi
              name: socket
      enableServiceLinks: false
      priorityClassName: system-node-critical
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      serviceAccountName: proxmox-csi-plugin-node
      tolerations:
        - effect: NoSchedule
          key: node.kubernetes.io/unschedulable
          operator: Exists
        - effect: NoSchedule
          key: node.kubernetes.io/disk-pressure
          operator: Exists
      volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/csi.proxmox.sinextra.dev/
            type: DirectoryOrCreate
          name: socket
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: kubelet
        - hostPath:
            path: /dev
            type: Directory
          name: dev
        - hostPath:
            path: /sys
            type: Directory
          name: sys
  updateStrategy:
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-agent
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  template:
    metadata:
      annotations:
        cilium.io/cilium-configmap-checksum: 475e8a51a8f0969cca18506d3fb86aac63d46d473d7dba21136ee187bffd352d
        container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
        container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      labels:
        app.kubernetes.io/name: cilium-agent
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --config-dir=/tmp/cilium/config-map
          command:
            - cilium-agent
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: CILIUM_CLUSTERMESH_CONFIG
              value: /var/lib/cilium/clustermesh/
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
          image: quay.io/cilium/cilium:v1.16.5@sha256:758ca0793f5995bb938a2fa219dcce63dc0b3fa7fc4ce5cc851125281fb7361d
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                  - bash
                  - -c
                  - |
                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    # When running in AWS ENI mode, it's likely that 'aws-node' has
                    # had a chance to install SNAT iptables rules. These can result
                    # in dropped traffic, so we should attempt to remove them.
                    # We do it using a 'postStart' hook since this may need to run
                    # for nodes which might have already been init'ed but may still
                    # have dangling rules. This is safe because there are no
                    # dependencies on anything that is part of the startup script
                    # itself, and can be safely run multiple times per node (e.g. in
                    # case of a restart).
                    if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
                    then
                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
                    fi
                    echo 'Done!'
            preStop:
              exec:
                command:
                  - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - CHOWN
                - KILL
                - NET_ADMIN
                - NET_RAW
                - IPC_LOCK
                - SYS_ADMIN
                - SYS_RESOURCE
                - DAC_OVERRIDE
                - FOWNER
                - SETGID
                - SETUID
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              httpHeaders:
                - name: brief
                  value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /host/proc/sys/net
              name: host-proc-sys-net
            - mountPath: /host/proc/sys/kernel
              name: host-proc-sys-kernel
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
            - mountPath: /host/etc/cni/net.d
              name: etc-cni-netd
            - mountPath: /var/lib/cilium/clustermesh
              name: clustermesh-secrets
              readOnly: true
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /var/lib/cilium/tls/hubble
              name: hubble-tls
              readOnly: true
            - mountPath: /tmp
              name: tmp
      hostNetwork: true
      initContainers:
        - command:
            - cilium-dbg
            - build-config
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium:v1.16.5@sha256:758ca0793f5995bb938a2fa219dcce63dc0b3fa7fc4ce5cc851125281fb7361d
          imagePullPolicy: IfNotPresent
          name: config
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /tmp
              name: tmp
        - command:
            - sh
            - -ec
            - |
              cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
              nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
              rm /hostbin/cilium-sysctlfix
          env:
            - name: BIN_PATH
              value: /opt/cni/bin
          image: quay.io/cilium/cilium:v1.16.5@sha256:758ca0793f5995bb938a2fa219dcce63dc0b3fa7fc4ce5cc851125281fb7361d
          imagePullPolicy: IfNotPresent
          name: apply-sysctl-overwrites
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - SYS_CHROOT
                - SYS_PTRACE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /hostproc
              name: hostproc
            - mountPath: /hostbin
              name: cni-path
        - args:
            - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
          command:
            - /bin/bash
            - -c
            - --
          image: quay.io/cilium/cilium:v1.16.5@sha256:758ca0793f5995bb938a2fa219dcce63dc0b3fa7fc4ce5cc851125281fb7361d
          imagePullPolicy: IfNotPresent
          name: mount-bpf-fs
          securityContext:
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              mountPropagation: Bidirectional
              name: bpf-maps
        - command:
            - /init-container.sh
          env:
            - name: CILIUM_ALL_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-state
                  name: cilium-config
                  optional: true
            - name: CILIUM_BPF_STATE
              valueFrom:
                configMapKeyRef:
                  key: clean-cilium-bpf-state
                  name: cilium-config
                  optional: true
            - name: WRITE_CNI_CONF_WHEN_READY
              valueFrom:
                configMapKeyRef:
                  key: write-cni-conf-when-ready
                  name: cilium-config
                  optional: true
          image: quay.io/cilium/cilium:v1.16.5@sha256:758ca0793f5995bb938a2fa219dcce63dc0b3fa7fc4ce5cc851125281fb7361d
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_RESOURCE
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /sys/fs/bpf
              name: bpf-maps
            - mountPath: /sys/fs/cgroup
              mountPropagation: HostToContainer
              name: cilium-cgroup
            - mountPath: /var/run/cilium
              name: cilium-run
        - command:
            - /install-plugin.sh
          image: quay.io/cilium/cilium:v1.16.5@sha256:758ca0793f5995bb938a2fa219dcce63dc0b3fa7fc4ce5cc851125281fb7361d
          imagePullPolicy: IfNotPresent
          name: install-cni-binaries
          resources:
            requests:
              cpu: 100m
              memory: 10Mi
          securityContext:
            capabilities:
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-path
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccountName: cilium
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - emptyDir: {}
          name: tmp
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /sys/fs/cgroup
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - name: clustermesh-secrets
          projected:
            defaultMode: 256
            sources:
              - secret:
                  name: cilium-clustermesh
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: common-etcd-client.key
                    - key: tls.crt
                      path: common-etcd-client.crt
                    - key: ca.crt
                      path: common-etcd-client-ca.crt
                  name: clustermesh-apiserver-remote-cert
                  optional: true
              - secret:
                  items:
                    - key: tls.key
                      path: local-etcd-client.key
                    - key: tls.crt
                      path: local-etcd-client.crt
                    - key: ca.crt
                      path: local-etcd-client-ca.crt
                  name: clustermesh-apiserver-local-cert
                  optional: true
        - hostPath:
            path: /proc/sys/net
            type: Directory
          name: host-proc-sys-net
        - hostPath:
            path: /proc/sys/kernel
            type: Directory
          name: host-proc-sys-kernel
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
              - secret:
                  items:
                    - key: tls.crt
                      path: server.crt
                    - key: tls.key
                      path: server.key
                    - key: ca.crt
                      path: client-ca.crt
                  name: hubble-server-certs
                  optional: true
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/name: cilium-envoy
    app.kubernetes.io/part-of: cilium
    k8s-app: cilium-envoy
    name: cilium-envoy
  name: cilium-envoy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium-envoy
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/cilium-envoy: unconfined
      labels:
        app.kubernetes.io/name: cilium-envoy
        app.kubernetes.io/part-of: cilium
        k8s-app: cilium-envoy
        name: cilium-envoy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: cilium.io/no-schedule
                    operator: NotIn
                    values:
                      - "true"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium-envoy
              topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
        - args:
            - --
            - -c /var/run/cilium/envoy/bootstrap-config.json
            - --base-id 0
            - --log-level info
            - --log-format [%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v
          command:
            - /usr/bin/cilium-envoy-starter
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CILIUM_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: quay.io/cilium/cilium-envoy:v1.30.8-1733837904-eaae5aca0fb988583e5617170a65ac5aa51c0aa8@sha256:709c08ade3d17d52da4ca2af33f431360ec26268d288d9a6cd1d98acc9a1dced
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-envoy
          ports:
            - containerPort: 9964
              hostPort: 9964
              name: envoy-metrics
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
              drop:
                - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9878
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /var/run/cilium/envoy/sockets
              name: envoy-sockets
              readOnly: false
            - mountPath: /var/run/cilium/envoy/artifacts
              name: envoy-artifacts
              readOnly: true
            - mountPath: /var/run/cilium/envoy/
              name: envoy-config
              readOnly: true
            - mountPath: /sys/fs/bpf
              mountPropagation: HostToContainer
              name: bpf-maps
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      serviceAccountName: cilium-envoy
      terminationGracePeriodSeconds: 1
      tolerations:
        - operator: Exists
      volumes:
        - hostPath:
            path: /var/run/cilium/envoy/sockets
            type: DirectoryOrCreate
          name: envoy-sockets
        - hostPath:
            path: /var/run/cilium/envoy/artifacts
            type: DirectoryOrCreate
          name: envoy-artifacts
        - configMap:
            defaultMode: 256
            items:
              - key: bootstrap-config.json
                path: bootstrap-config.json
            name: cilium-envoy-config
          name: envoy-config
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "1"
  labels:
    app: startupapicheck
    app.kubernetes.io/component: startupapicheck
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: startupapicheck
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-startupapicheck
  namespace: cert-manager
spec:
  backoffLimit: 4
  template:
    metadata:
      labels:
        app: startupapicheck
        app.kubernetes.io/component: startupapicheck
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: startupapicheck
        app.kubernetes.io/version: v1.16.2
        helm.sh/chart: cert-manager-v1.16.2
    spec:
      containers:
        - args:
            - check
            - api
            - --wait=5m
            - -v
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-startupapicheck:v1.16.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-startupapicheck
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      enableServiceLinks: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cert-manager-startupapicheck
  ttlSecondsAfterFinished: 60
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: cert-manager
spec:
  encryptedData:
    cloudflare-dns-api-token: AgAJ9ChXMAezXNfPI8Qhwn0OzJcKtw95ReqgVgLGFV8x3T4BeZtUTgEJGPBlUliwFJNGMCsQcnasTXe5lyPOiCRxFZk1t1ZrUY++K6ErzT1F74wN3nqQY/Sa1slTFidyAIvbWagOMIKgjPpnvXfqo+nl5IffReTSrahI3oDC6Z3AAAVyIvXhUquYeAuGwXds+CJOUWhZCogPxYQnJf5BZCBMLzg6HGtvOgAaOpNVEP7SG0N8yhIJRGv6k1Dc2mGqGMmOQf3Ok201iPq6NkZE+uC3aO9iQGilK54WKkY/pSaUna7YPZ3FjqHKLx7BgC1w2aWFOTVXHfGRUGwnP7mTOzkSpSVSSJahqGc2O2/Bw0R6QdIZm6NSLqKRGZ6tt9rKc5zxjYSk4AMvDE1v2KZ+2VuhPk+2090sHjYGi2cq7H2u0C4OJkIZU2qhxp8L75mw/dUhX2ZEp8CtnN/PQZATY4EOJB3S9jk3NbOV7tN/uHjhYNpFVTQOx0ZLkPpYtLA8GZibC+K+wqpdiPrrUKGsARoX2t7yEHhBjz/fWkyiV206rkCz9m55eTBAtJH2p7YKt2pdVKWHedJ8Lc5yXIpTpKFn9fbgEbrvgOTzia9V7iidizD1QDcLzLYM9Jf9uwj9+vldkGB6hn5Nx8sgdtR1p8JmbR1A24yIfJmSfAeDSsLIcutjNRkDKvK8TwwxbgZnzSNLWtAioxnDO24rPP58qLgVz8lNz6EVXVd5i5dU5I7BGFuyJlB9/9xb
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: cert-manager
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: coturn
spec:
  encryptedData:
    password: AgBqb41JE43pliaqXSNHlr73oVa2YULlDYLFmhnHWsFdAMGCAOrX/3KY1PJK1nSKILblkVlo4y4zYXk1sS1J/2+4uTUStd9zeqaUbaN145joLDKqIJy9UxUMh2G19idlJmhgZ4IXWYpUQIDKgXExZUmbqOmVrecPbGmTnSAAsR2HwW4N4RUW9CzH2ecbTUXuJ5jVzv1p3HhDRyN+A0nkm9TKZOQVTJm6QWNcLF6jk0mecTZZ7m5nVrCWlN2fToBWIaFEnLwB8jZMsJO0SSbHP6CLLtl+tefHVYZC+NkOP0/98kYzE8T2UoxOPlkKZW4m9hAGII3XzzkP6cy/9QA48p7sQne44IZ2QYLMHN9vJou3cn+uDQMY4SA+nYUcBTRzbWX2j2Gd8fzDFYMY5d7j4/XIEmKFI3HKfaMfvyPxKi6zSdFwrk0zGNvU+Km1hwix6rEK4n3UqxwizzdOM2rA5QFjXnH/JLidLCcxwZKWUyh1haiXZTWxqCoKj7W9xxQohNWwG7bV2Szobw4GP3utkmet5wKGe5ZN2buasre1cyMTa/VHr4FtPT23KGbIl5iaVtXO7VxisRwpCWTBJNA1EdbTibPtYVCWqXhM5vj0XOcM6qU3IlBgSGFXWL9JCLRFRiqCvR4OPGxT9VXxOI2AN04JSoYNEGFpOuV8HglANFrBwb54N0FeHgpZ1PNIXlV3Wp6x8IEyl3U7SXnB96O2jXpe
    username: AgCNW2mqiJHi1sqEj60miMBjc6b8XhzN4CVguWfch3mQQzOAY+zd2Mh57dHqmql7bht+oEOtBenxxfxKQQllY6HkyKXK7SWPPPSL0X20oheQEslNpGEYpkpaFBjyolO1Prrx2NM94oZxcf6rPeccXo0vApZ1E3YV1iIIWsdBDpoOZEA2avLb5jc6yIt8S9sbTGlzCf0F5UeVTM2WifnbRpr7WDU1GRgDID1v2dS8jQOtdu82YYD4ux/+I9fL4It+nyBREvfyUeV9TZphxG26xtQgPcXsMs6R+EwjOPbSxivep3ClQ+QbeLvMDl6LUdOeoh9Bx+MzJG+GWKgsLcpOCfRVh+nS8od8XKQRhGrPwOdyVbzGbStVhoIK3OomFyQt7qokaRMXkkVbmsb4vOx25Y2GYcdThB6ERadpzsVA2dcyoa1yvrYDn9AREBzct6gT8sprT89JTUNC7YdbaS0FTedvzHEvF5INpdj7yVoMgp5Zz1Rh5PEYYelpHcLg6Cm49Og4XkjdDVtrxvXr73HKonftioktFi/uIRg168yo+QRedTnqNa0vlY27tnrwuDFp/nxfN5X4QBLBmwhEAEJG4DfpZUiYRrCd4sStOWP61MdoROE7TaFzs9TU70mLEP4fcJL2zXC5LWeU3C+alvsp6c55lov8NtaBFox2z76ug6zTipTutWuenqLZNvpaDtEq4B/Y8oV+Yjqc
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: coturn
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-admin-user
  namespace: gitea
spec:
  encryptedData:
    password: AgCGBjrd5MQeC0yx5epmouyLnJ6+5cZkaCiyXlyS4r+KSgvh1bSrtL7yyTI3LXh5ix7scx1j9cpzyRaGHVexM9LNM9XWqC7y7GRNJb3opCoDnSGcZ8PwBuLMTg4sxwM+fIQDq5oci4YyR+Jz8RUnslRCSBVr8qIE6xz1tMlpl16o0ni+vsb9q1AjuO9TTIhnctU/IJ54P4NQI7UYHBV29S7dLiY6R5JwYm992EIqFls6GPEiDGKKzHbphZnznTmlPENmM3fnC2UQzvoUlJ/sP1rbN9HM1gT08GCVnj/3M/W3fUb7dDAE7KAFnFAZSNhhQW2+8+5ipwndQy+CkrV1d3s+TE4rFvsuce87Dm9UTRXMB4brYQb480adZWIJJLxKR5fXy/cWeWOw+mO78FiY3szxgu8pSLUWndCBfRAFoDpuVSoQJJfNyL/9KTJI+IcubLHl+xboF4HpWnfKNRLix2JdYJCs4AmsJNAOcYBGa/fdnOdg6YQ7YhDRuQqlNflgA6UM4lta1mUMFBPdP55qTsdCnpYba0Z1uAWLS4EERVNTgPl0aGLlmmvYqsi8vWpu8JpoNiMFKtk8if0+ldSAQ0Z8ozgSGK7jwz0XWNmbhQLeyf1297qIaenWiNo8GACZbvPHvVOZ9At+xq4jLpMoQA4obMv0WBXpiohaHYq7lLTkFMyN1xFa8ABuguvDR8NjisuuB9tgxTY0zMDAdeQ1qkLKcioao5nxeA4Q
    username: AgA4Du7ge2bBzpz+hHKV3E74rVdFAVoGNs7Qts6Jmr0r3E4dNobiqyroocqsfHK65fOWCfnzBdYN1vKeVml66XG7GalcF21JvnYll9KC0qDRGO5qUW67rtZjG6F6BBV5CJTP4+1+VpNOcQxXdBNnj3WV6M78MhL1oYX1IeB/1tRJjDRgKcRJUc5vArSQAUHoykXq9zNwsib0G+J4EL7MaTfNEZgjpF5pgdb63f2TXiELEufCNzYisF5KyYsEa96NexnCKRowBWQ6TtiQunq4RoYZnxNBNvL03mYhEDpGH5DU/SNWdDRKQf9byaaKaGSgxKKks7a+vUBwYJDoWUd01tQOVN/uaEQwSJ/BazLoQjo244wxhu6yDWuZLfwc2VqlFsG53urT2uPwTv7sNhW5L2PUIWKgOaRhMAlaA9MWgwTfQYvvAOBmlV2GsgwY2IxRGiO0uSNeeAWS5cIhXuc+aZ8vNwj5CZR98WTtZ0E6mmyDmUpXZcakDE8W5enuYR6iejN555jlxK3U4/HvMKe9ywNXzG9LxqzLF40Tm55hH0nJHAFsin9aElfAmpeRY9UcmwIoNWfIHTM2RwFCm//S5GxYLt9uNwYPlfRg/DsREYy2qPNeXth05TbM1IND5ho0Qb6WLB3f0MM39N9YdPsv+nCDPr+Xs8uF/E0tHgowj0LdDgfaPlX+eLOQReiChorjhs3lybBN
  template:
    metadata:
      name: gitea-admin-user
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-oidc-credentials
  namespace: gitea
spec:
  encryptedData:
    key: AgA+nsX7ujf+uzZ2N653HXfGosxZT7zuvHTW/8aZAZ6Ll/wtswLCHwx6G3Sr2u6m8L6AaxhSoUE/RO9NEhQs+xCcTc9J5ODI1lsvd0Qiupw7+EZMn+4EBsT/LjypXfpDvhM9DTbyX7hbJxR04ncJW9q518CHqoT5j4pkm5PLglbmocb85j3QUpBNOaNFAe1SnhwrIoleX1q9BjdeOGP5cj8e4zbaMHTUft3NQyJyi0ghZ2CASEngTQWIVNHbElm04F6xOhnL69YY1Gh+2pO8BJLKRvCE2JA7FgBcRBwvTkGt6q11tbtEaKEoFiMqdEQKdb9AnZa8XErvBljIaZmtIRTsmoU6DZTgBXaFDaEcOKvM/NuSdLUz2j1gKmWZSJ6gMnI9blUxdaV8KHozygbBF0zj2d3eqXpmtI/atmXfbvRduuLLo1vhmaQ++P0vcHZyFlAIQTLJdCGwIt5hdK0SL6uChtYDqhYV+KebuFBmI1r1PKDaurqz+KViSIfK+kt152Mf/LVBq63foszybrDIpNyk2nqlYHJnCAeZ9HtvuZ0Zz5CtAl3KIeWFg1HlynTpNn1GJO/A8wdZRoAOr1JbX5REaoU18HS9n3X6Rrujm5SwydU26AbFT1rrJAVYAjUoufNV3e1HTulcwDjUdAAowvMwciKvpdhh48H2XlFOW3+MZOWBSo8ErVOa8/t9sSowzdqJd3Q+Mw==
    secret: AgB4vQvD4KBUOIkPCBL5+Fgb+jwbt2jvNcucCP0UFZrX3jN40bQxOqcElG1Z/6LZhoq2dh+amkpGIh3R/YKJLNeUsE1TIL9W3ThriO+Ng9AH/K6omywljIldSHsPRARjkAiuUKMlx1mEs7TRw1/YACmXKNw77KL5RO4NLtU7VgxyKcLe3JXIAexbNK6W3X8TkDqu9wx3eNduUet4Jt8jEbaMfINwexE0cH7NRVyQbLDT7i7MxJD1PisQWh6XH1ZGkLkxSxh+wYSHwIHSoMgQT3r4vxJ1rmvDCSEPMEBvB/3tLYYTGWAImffRIdnYHxWD/PDAaSqHqSMGeXtFnduv5SOYK4eSayz1p73IwcTvQc8ePUxgwS8uahN7UYRaoWem1Vs/uw63XBetm5PQX+wh/yvnK1NOdFAFIRn7Oh/iQhoy6kDIdzEndR9WZS0HW8IrMj8oFPTgJh1lRtWyXuhLiYiwsLcmMg6U2sPSJVRcnvxqdG+DqKjWL9WQjaP0EWZe3luole2CMfTkIBcPPJxMd9HJCgCcEdHl9Z5WZ3NPrOGb0DETOnTnn9hJPPlts3QJfqvyygNFqIUkSWE2zrDA4GWF2sMBE8bh8yQYGBD0COwxjvyWvfO0WlZ/63sjTwqeiV229MUGVNquD7BKvShGT71b8UFGfZG8rx5h1WiVnexCJ4Rth/u2eRaxptyD6NO0A43RAMoMJiDatL7ml/LMxfLyhDJvG368rYM=
  template:
    metadata:
      name: gitea-oidc-credentials
      namespace: gitea
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: gitea-postgresql
  namespace: gitea
spec:
  encryptedData:
    password: AgCG5I1alY74KUfK2r0gwZxaTrLD2py2Q7vBPPW+4LQg8KA+lTjYUEhN5BZLHzFRUtHW+KQVqqgxRlKOAsRZn55tgzzqxgmNxlipIuX1fGyb5Ds45azqT45nWfJlzf50r0WByMSjsdvVshkNjFjCXlPfwr2r73x+ZWqRv75Ra+6YsPHGWvkzglMQ8G04T+s2crb2PiqtOeSGsflJazLG5VAf8tddChGKsptPwate0UogTctFGHwhjMsTp/J2ivTqOh+VqPSknNuyJQfzYWHTrEHhNR4dNROy1lxFjA2/bbpUoezx4OHEmMB2iKmhVFycLPcxKG0IOykINLO11chia/sStL/3FQSJw0v99YqmgOl4g6HmkCiUSMQyULm2hdAsfbkgt6mGbSTqFkXS+uKClW5WKU0tzowXWQXZ4tCSC4ODZGDRuWbAsipClQLSknd9plh5FEzsrXu5Gujg+ALFDFef6lv6ukd6e9tOlAB68mMklXUVuas3a7JKSeZgYf4+JucmDyn1pDNpyT7KnOzP2GrQ77vTkfgay3QOZ4zh1CMw+5oxrdRbML0JcDvOTBraOKAu+nVwZCWK5nLmS81qj4fWRvqdiklw457cu9m9DZYJIgcp2OGKddyvyH/O8p5JqkmsRowd6LcmUt9F+/9ibnO3z/L9X1+pQkIfDnyBWdoGgIO1MSQzzsoqDdd+MDOxdZZTV9yYkQ==
    postgres-password: AgCfEmOIW7DTsRFmEUsqOAd1+ASfVPxQfZyIcrq1q9LVXX/FWY6dBTmLBpVRPLF1h3acLoUqqIPLGOK0wPgCbr9waEvM9HKsfhFqzTGvPIVid2r2tuga4u0pi0U6JqidcHROpyQCwE2NJrFiY2r3aGo5wstybYzHGDmTIAU8FfCTf38GzqgdneR3Gike9L9IoUmvEXzGu9edcQYinb5+yJ/wWp2+XA9936HmY8vTsjKlv2VWBiu5ZbWO3YRNYOvKesONDgvGh/3I9yagEqajjlpSEIAffDsXRnf16ihqGaxWk6KUD89yOtuiLo5BR9Z2KQmJ2jHB5T/666dZLMUGRgZBxf8G5wR64hSOaAoXI4HZ6xYYsmamGRrVExphzUNrkNu2wHmUwTW3LT8pXG1IyUAd0MZS8ZUlS3RdB6cyczYvhCuln2ER1iLXSPLjKy8HVhvDBwp8deGdvtgXK6USziDbYAaKvosYe5jZynLuEddCFJ55i+Kfk6hPotnnlltlIA5XA9ncK20L6r+tj6Y6CLYR+eShMAi4TS3d3vmmwcWdaqUevh6v+klpCCSJ8JyIP+YTBBl+0fWVNqQDqtAXHzFeU3IRl2MYCZ1pbaLQQgsORabT3tbYbZ1Ab0re56BnIDykw8zwylr2sOOq68laEYqmiXBKEc/9OZZNUFZx2BHE8cx4ewiTkXiCeH917XZRnC60Nlo2zn85v72N
  template:
    metadata:
      name: gitea-postgresql
      namespace: gitea
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-config
  namespace: immich
spec:
  encryptedData:
    immich-config.yaml: AgAA1uORMAGoH8uX44bzeylc/bxC39WkaKTzVBAsHzEem2iW+FzmGv5amYEBlgWnX8jjSGK4SS22g/n8DcL7GFsrgnrZNlJPUXzJQUD9w2DjjpVaIFrQHvjrnLgFLu9+od78RWXAWds85oiRX6LYCiqVc54Nyc5zbnQ8DHlvw+8VwfpitqWZA38cclJ+sDbB9hCLBPSEDsBlC37p8bQqjPoP2m68QhEpg4Kxrv40k7WxCvrNdNUCAiOh9hRdHvbCqEtleL0dk3aWJ3kGSf6p0rsVPl+cyxlfsFrFg74lN6gJKfgTV4j3mzv296wb1btDVN36JaOTpcdS4aVK10Kg53idBG4vX73eQZFRaPw7ZGi6u2A/c9I13etOMxrCX0y2Vxu2ZnAtnFdkAdlcsjhiX2LdjVYGzmT+ZqOlGp7lJd+gKX9CXhfrJlw3/2lQtRpqqWihIrrdkthm/4mAybayREV+Q/ojEXMmk20XkI6OLsP/ljZjMNA7vCB3tyLnbRfqHacpKbnVp/b1Jth6TOYZnvLhne5i0JmM6RCUiZXitJnxDzlQskFAUKoyBHorS8vYD8lYHqMv4iO2E4Qn38vHnSAaeQOH13Fe5tMJ+U/VPQ4ybawWCapUil9iuGTKwMQ4xbkpyN8z1KyP3y0RZf2w9NXoFeBZR/iSgGFUataEbpXWipGb66ieV0CAqqYGvLHFfk2kTbN9HMRKTcsClCaLOCFQKiMQLXNtxLsIsxPxrb1l1kI1xnW1H87upx1Y3SEzIHZdQgIeVmkVUFaVrFw5/WjqrdVZsxBxGkqvD6lKFY3X1srzcxh1A7LasSuoOJNVxVTFBr4LsQ7tNnT/rHaMZ4YSLwRJUQc6hnNK+4B4GaTX0190B3rion3vM3XsEqU8U3hpVNZupFnJe1Lwsp3Dom9nQga30WJjcD5SDGZQUtuPEjL5//A7uTMGrx4/B3ub2VAc+8LulG7dIAGSO7Ez0drVYdqcGoNQ+EPmY0WxO/CbYoD06QWbqQDgBD/f1ftERTwBGW0IQe6MEWqeTcgFdg==
  template:
    metadata:
      name: immich-config
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: immich-oidc-credentials
  namespace: immich
spec:
  encryptedData:
    clientId: AgAvvCAj4sOLQgLcYpkSUKYld6dWCwEsTxtJ0BRjSOdKU29lh7l7XMkLcqiQJKV36PL/he0YKD1MW/jWcD5oys7ZHtaoDKAfCmTMR8eWGU3UcY+0w0ELn+G6TL0EkgKY6Gmc0JOyguwJmsilD8iyW021NknF+hj/pIak5iLxHOidPbRmNCWqS0b+iM0WLfLs+jrstLOYfs/VA/dftqZMogN5JLQ59edb9DU7Z5DRpZbaqmE7kgLuRR3t8Iiz300S2Tvhx8E2/2gKLQbyVMmZ60KjdtNln6HDhIjwxHsas3iWUXlgh1V2sIh9hDOBS4yWPoGDRYEAUdBJfGLFUYi3K2cQOC2oAAYaU0WTTDRGzQMugPDbKONO+fDVjKT0UJpiBwJ5aoNW244f7qUNDbeGJBM4iyQYit24fsHyL+4jbXjaoN/mjfZuN57h+QV6K5sH+bJOSl0OyH8gT+UNYsIZ/Er1w+AsNMdxB6Laf7rPnxlVsyy6bDLp8mRE8lPjz8f9EPGZIHTkcLTgNdsbDowG2RfvEft+UkRVuk0g8++VxQs9T9jzUgU1CsIESW0mDwl/K64rJm4y6G6wAoqCB4RyS0XHQ0+e0dqpponRKpokqzxZPgfnlay1Hc+L2DnAlD3kYsvAnQdECQMaQ93xh2/18Ym5BSQCBDB5RnElbgJBj0uE3eOJ4CMMftts3b+rFtVi+SOvpkXciAg=
    clientSecret: AgBj7B514brIE0r4g9UmqIo8gnBAHA1G/e/reQH3ATOUz2gAMGmC6VVXhfLam4Nwq3U3s+SwcU+jvVYAu8mmf/2ltUIxvBIfFnk5olM8QlE7eTVQAOdKLbhBmIWN5WhTgP4kIXawny9i1h8ft7anYVpI+PJoAQ0X8OiRbybsb+O8wWDFaJQ/lEwFAww3yNtiltCf3b7wJHI8pjvjJrtHeJRb13+T/sisSSh85QdQdbeSXBFGAo3waPHcbtaTVAmMs74ZTpDw56o9rP8hPtybSN67IcBa4lt/p3Z5OsVQgHa8Q1mHdzb/8wLDY+39UbHIZtxhH46MYDs7BL7G6ddTZUViXYBZRFKBdcFSs5tT2jeWR021stpKosRSgrEv3P5d3QDMYIdsK6FZw+lMAzaT7glqxiYLa6Ot0lYLU/fF1iDJ+76J4u1Py+BqHHGQV4qE+q4WTCXnAHlKzLtoV2MiaZN7NaPSfVl751BJFl1lgXbAievsgGPU0rYDn2q+ZS89QYi3yPqhN/3ICTbOE/iMaHhrEKkBBRmz6rllLqrLcTwJo5AWJAUqSyPXY6Jbm3akT/MJJ06Rj3IulZPN2At2ps3tz7+DVSES/MgP6GmpY9ZpIbiqR28rnNjzqRK3gZ0QNvJrScoeAy+uQTFXjRyRuW/8yNPnkYsMpqDe9cmB88lbV1XJzjtdtKKX4e26nWlJZqaT/S7AeoFdDDwLuRSHs2oZYVvFnosLRG8=
  template:
    metadata:
      name: immich-oidc-credentials
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: immich-postgresql
  namespace: immich
spec:
  encryptedData:
    password: AgBg5CBTb0W5tXw8ibklTQP/1D/NEsliDeV/pHL1Pk39RFkUg3OB0pQbSe7ZVqcGHxyLaaTDr7o8NpDsAZnsJOH5vzpGBrddtkPJNwrSFxd2GAzGTI2SQH/hoQQfp27UpJzEqXiLIHw1rFTOTrJJayeWgwDHf26zc0oIBpbbQlu0o+26Vca7sArnGixX4iU5SS0HF4S2Hhk0CkL/dW18k/xrey+GlFdAtYcww2i5Cd6lxRTC5CIX3dQSqBWWcRy3Q4wOWH3zrQpkt96bSIlW9YrDD57dX7NAwCwrCx8pI6MdNAJEZpdjF7T6WAX+t5L1YzF7ecbtGvw4l3IavFKnvJZn1CCS95cI8xFrTw+igozYwMJB+oxre7E+q4vk20LTYuv+hSCeTIzX8t/5u0XqHW3/6XrqMNVOt8ZA0LIpABqqIgR/vEDsQPWwuehPwNvOhTJNKSBQRQi/SGtHUTkYCQwfLSPRpUV/MK++MaQin1nVy/Ch9dexVuWsR/cdffTwagzs4MPO1DDgfYNZe8sY9hPp3XUDjx03ayCT1rUA8Ymqajl2rRseCMuzdQnM33pizqOKXJys2DHWthr2xi0DCYd96dky/L92eHOC/tSQlyiYYjZGOUIMmHMdJpxkHUIXHJ/F1iTIyY4UMJMP3GkavggEk8WIImOebijYSuK1U1QJ/dlPuE4Q30YbhA7Ns5vj4Mu3IsAmAfg=
    postgres-password: AgAJAAmqRjNDaheSbzBtB39VjcDeSGEFyFnSmw7a3BlBBVqSuh2PXs01AVo483Qhy5VJQCHt/pkbnyw3bNn25OPmYZVdeoiwl6gZFwquY7fxpNANhvado6xeC0PFNhHAqX0M/U9UL5xPIfMRNc6JztvQaFosJD56EPiC3Mx76LrKc7hIDN8JxXDoba8NaUfQmvPeqX0bXucbDgDFv5yJmHWv4xtuTO6a/xwnt5uYn0nmC0ut07ntFeQSA4RqT2ptauwMy5SwM9JaR6fL8RUVQKX24Imk8s70XeVXE2mwdXu3w0fVx3PKlHcL2OwLumUsxhgW68ypeoHASR+B59mnJgMajYEQY+OwXwaaQNe1VAZyvfgwL4KUv7DvnwPgcUJNFzOowqmtfYzoydwKgtRboWlRvTR2sfWjtxgOXPxFLd/fKMtv1+CVbrRCivOGsBk7bG5k2QTraTuGCq3MlwMRm0vdhFVnOqZ79/Y9H6GZiFwxN+7W8w9wRKqRb7FmfLGgUbsf5UVqUpAqbHy82Uq3SvUwQ+f+rxBFN8RJVP2zDFiM8WBsywMU4Rvcl9SFcnsLTunaHalPMJw+AiflsT5X6hvrTo1S8xYujq2U2TltFTVK5SKRBrkmNVm+MWef1TmClN/2tFTqm0qy3qNrPZXdNzUVBpIyo7gB0EPuOJ6YXKKWI42LLavb12DdmDEsrK5VRTWcK0/RiMCH5e8G
  template:
    metadata:
      creationTimestamp: null
      name: immich-postgresql
      namespace: immich
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: crossplane-keycloak-credentials
  namespace: keycloak
spec:
  encryptedData:
    credentials: AgBTjhI8iY1NIToAtBAbRe9qVikwtCVKzH52DYazQICVjjHOBOmG+XHQ0XSnopit6le+ayC9ls2pp3zRMXacMyd6a9x5qmFcO9N0hTxPa9F9O2tuwAjUBaWWegM+WgMu2DXa/9nYdKi0Kz1HSDy0gnB9Wtlmd4Xjbr/tYpDRVTRKizCgs1LLNr7/o6G/ZV0zrJH0Hxt85cZNPjSIoODu4v69ea/Xq8ioqSC6GvfThFQdwx1CZi/TS1OAHarpSWl+i3RfPEls46iLNhHi3t8tD1YumSCENqpFtQ3B1jMc5kAE4kGtsvA9RdrGp8oDLIurmWOzL7HeYSJILrB6cVo7L+Y9wnI+ovCeyqbHeGIjBnYUUYTW7kvubbhBzzdhwDDVsUk5/3TYV8eQoccvYWPfmOhzIGEmBH7jOdD7TfCvbx1TMv2cuMjTGoE2A0WBE+hHY/OgtlWN+Mgvld83KvMLW8cNp2Q4ZhY+syo5okgkAUUqUbsw17bGpBnGObsipdB2j/PqqGTHlNcqskK46TVsu8MPV2MwWcZ/UfDbiEyA9GVYFB8aZXtoNwhXsRLNVS874K3HbtiMoNNWYaZ3ddairQl8uomhEm/H/33IiRJwElFgKfQvQOr5ak6yoBC75NoFetz4fZLr0XgWgB2RvIckb3+WxjrCmc5IdrlrL221otu70T1bZcvZN1vaDA9V2AseY7/WZJuGDqHpfeARkdXzDJd99LPW1Ww6eGBru1BQQWi0obilE+dVEueJ9neebkmZX07OTU1dOQRVvHFQb7k0U39s646mseV5g62d5J+2dnxE9bwoYdy9XY2Hu/oCqPW3ezgyENy0rjwhJo3ua36P2BSkDGI694N7EEb2GjuZSZKlDCWaAmfEGhj4Fw9z
  template:
    metadata:
      labels:
        type: provider-credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-admin-password
  namespace: keycloak
spec:
  encryptedData:
    password: AgBgtRvU9pbVKgL7wZQnogcTXA+I8XO3BqAzialWSeeX3YpYCBbhNELkTe4BcugoYfL+keC/0D2VqiECisYL4arvUkOmHXKBVUsl97rzSKcS8zSomy4OMx3lQWI6xBuB4vwLzDN6qcl+LfnoIJlzcycpgQhdWPPcEJ3utzp/dhO9LzocCW2xHREgaySYSkuaTBdq/Tj2HZKuaJ7hVNgbO00zjHZd7qAs0OVFoZpJlZyMYdZU0S1r0gL+XaP3v2ARZ3XQEDmmxufLOPWcxYOeJ712q8wFZn5sCJdns31sFCFYI5mN/d6iBvcEfuWLnCz1ET0DC6HgedLID1jB3sCjYnQdz92V2fkDyj6uzqb7X9CTKwn6JvY8Egd40KVbamsLuoiT2aNpFw23gkK3Ajus3H5S9aL3k8zCSgBrHBKkO2TZDER6Fzw39cLBSnRBWzPS7wy10JIfRNPKV9rmGlHWY6y+H4KWkPiIlUOqDGjlfaLPXyD9o0DcTs3rTPF/lScqPtqSlKJCEDwWqudC8UKLn5YSPfECdOnqNCDyj586h+FZk0IxU4MSG7HbLHi+B3/Aev3qibBeMIYSsUYGn+B61YY0INm+AEV+M2Co98WU46S7+zIHpxSK6LxV+ov6MIdyo1tGC4PnFd1bIZ2ZxjlUnwMfE8tFSyWO+963ECVcPx0rXbvEDnzKvPsW4f25ttk0zzjX2WLxOfs6fcOKnoeGnG+U
  template:
    metadata:
      name: keycloak-admin-password
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-db-credentials
  namespace: keycloak
spec:
  encryptedData:
    password: AgA+ujL86YIs5ecAexMZ/CgsEBN5VMSYoXV2C0+OKEbmlaNwqBDzlTbFiCMBLe+RIaLTbJ5Lj14HFrGWqx4SdKpIqwJwxtzYJqnUzsQmgHQrAg17OJHgZbMdtI01F13/LrPjaLi0dmygPzO6xw1/bw8RHd1GiAeVorUmQh0Z7zfw33x3YaSb/KC4lCyn+INYOrPG+Gl7WtwKe0g7lLQbkwIl89A37SGHABxErlmuecev6j8CDNQ/WGugHrz/WJK6Z4bNIItMSi0YUeSL60LniEF9Vdx8FrEZ/VMMvyzHz9Sy8X1ygcTKnQi2woKi8QIXi0t1yRAj9HNHmtojh6iDvFtQVldX+OGmCb52iwaS1gybmzPXEs+++vRG1btosXh2t2PGs0SeCvZul+ZVufOfB/i9pRuu6AvVOAqxGz2Z1bRinZfkbrYlweUv5GwZKFTb+xRBx5Umwv1jJ/zngCEEZ+xdAk5iC6IVhqfclW8qcESagEPgIK0ZPwD7KD2iLi9dy3Ob0nd8+jvlnIr1BmwoPc5pf+mbt50OpGAdMlBGfPaYRFFM95pFs3UEX/E0gM/RRWDzgz8c60fyvnV9ACeRLnDqQhMUp3G8z7P6ZndnvVRw9R1n0mXR7yUabUtju63/tHQpY9muJ0m7occD93W+18mUVDPoiBC/sQzppYtqBNo/CJ96L2D0wky/raUNKQQVaiYw/izlXIquW6kn
    postgres-password: AgChWk610AN5uWVgF6/DlIZZ/GJSFfUZi8s0VPluyo6/iT8yKahc1wWW2hnQ2CIfHAlziq/ksLWElPAgpFYSaQAcOLi/uOEi8K4tR994MAMYMX8zh46ZOm4e7da8E8kS7A96aez2cs5J19VPoGbEwteCqB+sZFby+4qLeRPTL6Fk0q8WppnhZDstYwvMYL2gpTeIDqQlwyg360QTHoFa7BAntdpJcO3eA11tTubhV16ImwV348LYo4x1o5fesgazepAope2KhujKuPjBjqEDfMObJUvTTQuXQswmVLG7N4eeBilo/CEAWt7JS4wDODQM1RxLjiySDqJJBjwKxISORX19Np1iPINmTuJddCiS+i4xo6qp25SbExx9AT94TW6bSZrAruJsPPrm30tc90/nGBRnM/kD0CtfOayudmrkli3NMm4fAJ+qWaNTaU4WAiy0jimO5BrPzK+OlCVR7VRh2r6nr83X4BHAYjY7cAiCNF9ptl4G3EKvYnMawM5I3pGCbyr+0XbihEXurphFso5ElvmhzDonv/rzLF+ta/frpYdN2o2I7UUamh+is6u3iYFl/Zh+k8eZuJxZkdY/h2pha5ZL9tjzniGFQxYbWqMZN5suZDBo/YtItHlhx8Jr1o3m7lwpW299P11DFoo8rPHPUfOxTet9iqCDHTTBGx8ymLqG1fnT91WbJP+JIFHY8O9+HnrY2+Wg5aGpDkg9
    username: AgAPDXqoidAyFnMjYy6AHu9w/IS/IyQF7EjGtb3kFaq86nTLvtdSPBgS2V1cEi/Ko8MI8e0B0TA6LfXfvFm85yTDzazGVCRWVDLWBdrt5FEKqg/gUbBZXvC/AcBCxMZxE+cXXLuICz5ogW0Z+nDhgo343IUTG9QgWKXVPVpp2IPqI7ka0IhwbBo9/Q7kd2UwkMDxzNIqgArr/l25fZ59YsjvASI9qFh6rsnSu4idsLGRFJwo5oKJMUVM1K4BaFz5fllBKijTvueLP1FyOMzYjA3DUdu4fr6dmqbZTZu8pzDr4AJdNsYTN54vaZj6+46DHPH29iSYHYHRMGvk4KcJsIOibTxlIxiXS+WzwuaVfPkdAYFwR0vcuBplsFG0yxhNA//bQCtMQIteF3ofGgLtSXdibaXstkuHCCvWR6Mg4/zN/xuDt5qrQW/cPepjKgxqWSObF95BKKp0Tv9fID1K4MlRLI4g+k6j/nMr8hli7ZMtoaQLUa26nnQU9V7dwK/r6GdgCPVtO1gHm5XPL8z4HB5pa7/pxQdieCHLBV9+TFSaEqxhDReje2ZyeLFehQEYdBqEarxqe8L/CNZIQPPzkDNn6X2qmpPiQFiraBOkYB1Q2yr0j5nj6Xuv1ObGlMCvo4fDVW4aWcOquu4IDL2J7nZkON6qbDnMHVoq7/cif8Pf448lA5iqzTRMIGeUlq31EigqJ2g7LNNGGHmUCQ==
  template:
    metadata:
      name: keycloak-db-credentials
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: keycloak-initial-password
  namespace: keycloak
spec:
  encryptedData:
    password: AgAh73gywqf2fP3OPwTeS3Dsw8mPr2cUc16Y9Lqrt6L6jbsL7Y8S3wAl4t3KhmtkEKxrRWwwK+D9TJPayAwmWYlqYwD5DhE1U9Yag8ZJkMlJOMjVsTGBWDScTSy/tJwHEtfD+rmo9HEjLY1sfdczmr94PygosHJvmrMFWO2YnZEvyKTXz0e+BPytcGOCh7/hd9t0c79VHDN0HZbd8hiHW3eVzQj2kv37X57H1iJx3XVI+53Vj1JJ/RZ9pM0CXanc1LyhGHhi0g5nokstwUXC/QgTEVqe76AC4EiyKVQ91dp7b5OsJvNRhsQy4u85C1GBANiDrxFBHmCbyn3JP2+yixjw5MvGCCGkD45dj2O5JrprGd7EmmBxfTMTYFc7j8BkTe0vcepfF80dDPr736dGHKc7VoB7mz9swFyhu+EIyS6ss0WwBRMNSwyUe+e/kbrp7SKYXyAKr+9mU/7DWnpcoek/JJsVfNvaF6QSVY7eqwsMmKXFkeqtGA0MdBiz9kKhg/NZmFGuerxAM8o1U7MpP7H+VKPW7zSQu9W+UTF4cJy3hP6PaCPduysvk5T7n+IvBbnhvlD8Db/KIF0gjpmp3E2mcPFVifEPIvLVtXbwjElRQVqLqS/69WK6JV8HriBfzIMPiiIklwCds6yUZQQKKCraHjTquj6KMib/wEsgFrbHkuKPAr8BIBkuuM/UCgf/loNFlB7vUrYtfhM=
  template:
    metadata:
      name: keycloak-initial-password
      namespace: keycloak
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: cloudflare-api-credentials
  namespace: netbird
spec:
  encryptedData:
    cloudflare-dns-api-token: AgBxmCyBQd1Ih5SaHD1CqdjuUOpVu9LgQ1wQbObXappqOJvdI09gKCmNQ+8ucNmoEbA88Of1ZNFIHM4UnvmdrXGRHIjJfR4E5GRQKzcmOb/FiIJ+97rYpm7fcp/S1Qcx2uA9THvhvO61O2UW4LdR/m2mFETs/tdgSdKkv5nRfxW+lWvPsMR/UFPiAJl0q77jUQksTT9QIrEmQ2L5miMtB2XtqtdBDx5m43xmwInl4XJeB5ZMqikonqb8D36bjgNYYgtmxIQ5arJc6W33u01n0rmkg7N6NQz/3lRXAzhluaitlUmYM1S+VbJUDuFUfmkTL0Wbz1Ab1eYoygshhWeAS/bK5ttkp4idfdHWd0zsRcM0ESs4T8ReQlelnii3ggh6SZXp8njIGucPduEeaBI81ODEsh6iS9t6OA0AIv4ffO0yRwA9LUsnDlmiPDf2NZe7ecPivM8uPZHFwhnci6fDp8kxdDGiEn3qzC6kFK3wOShTivtTu8LWieXNGb6PPf2vLU2L8+84MEwEHn5L2oQ/VNIUAGzrZMYHQTlTKajt6QNPxS6SwxSTWAh+nZzIwI6snSFKGZXFbcKisZl+V39JqOxIaPhZvwtOTBx6aDD5NGT6pVy4FAxfUwNTn9iElf4tBLdm/3oSrKaDyOKh6n2QO+uVK+H/jPt/HeRs3Y+MIDHyuEj+RzC/tOcNGZ3usX7yXO8B5W4rJut91dsQYMO+OXvGgEtinR4eVag9gIkKA+w8GPbXvvs4XF56
  template:
    metadata:
      name: cloudflare-api-credentials
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-agent-setup-key
  namespace: netbird
spec:
  encryptedData:
    setupKey: AgANdgUUaLKyvercS9O31avaxlyiD7zfL0ect3vj7hkE3hFbfmoarKkTPBx/C6Bdss9PmyBrkx1JsZbXynIev4XyJSqohYawCRMUUy0BLMxqbUwpYEFbQrkJ7m72NKtBJb3qClKW1+6MZCINBmJs0sziDv7Ev/hhWUYILKthjO/W+Z7qoAdbQ65bz89hVaV/Lv1P7Qjj0smYfVrBsYYM+ihhxeX0TDxFo6cyXorA35lRMEIAQJbULnB6UTlowgHkP05FSc5Ta5Ekxztxb5u4lcfa/YaI8ekjU8/DIf9WPjjNNW8tNeFWD95lkSGy2pXUD2ZgjUbhBS+guEhzzaxgayMZhl7xigJQt2uXZilKKfqBJ8c+y84uYPoYuVhiJoKlWhMoAT6vNC9En4oH8lIn4Hvuf840WDt1XFbx8F8jfqlfUnjgJfZDK3ksoiH64kb8S3AIZrgURLeMplIngC+QybVWUwr3mPbtAFTJQsV2whkSd1uelD1A3xsEDHGnBPiwhSFP2thn53/4khRzNJ2atDb6GLOW3BR/OL4HPgJL9HGAYiQqZgEjKZaxpBuOHm2S++S4z6kQmq+0ziIOwfq6XbOJwl2TFdJ/HVclTiHUQyfNFWKP4DBDmZKGyDMxOXJdyAqkMPx4YnPPbuZBdy7fViRuH7bCljluni/aDi/XfUNEAac/SxQIBgcJKTyMsRteiE9Mt92iHQfQhjBjbvNZ64G6S7k6mo/dUWtAAsM240ySkjrLKlo=
  template:
    metadata:
      name: netbird-agent-setup-key
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-backend-oidc-credentials
  namespace: netbird
spec:
  encryptedData:
    clientId: AgCEhhixK/003E5HfsqYAQbGo87FBib3OgmtyIgHbCb8EcscFh03JJMWhrWCbRN7k3kC3q7CY3KAreG0/jMP4jJi9Xb1xRJaX9W0/bxSmFPh392XQfK9H5X32bsI6KyQ0Sm1E6FgN4d1rKdLL2qt2WG/Pvl37bKpQ/n+AEyBpXUcqCxbaSsIqDGiBam5/VVBXNEh94aE3J5hYARNPUgUOOwzxjZLxcPAw6jFKy5yJcXeahZ+1gaJpjv585ENeTlElZqJAc8p+lqw7s9Mf7+EW7FrhiJIv0ZUumGwdpBaNOk+D/64G0CStvdzQ+wXR4XC+SCqDsHZNuAy03nu6EqQUhRmOJI+SP5SUJaMABc9TjxeM9DwCHuaPElURGgiHXdOU8CwWi1kNkgzlojO2CuFgeMqHGrM8Gr87hQI0iSe59NnJVjUMZp3N89UeC5nP9A5fhYLP32rTJMRhIgVJqis9yS9XiVfh/9OXhLOv2klFwnHlIR6cH7YGHOcNna43aAsiuM2YkszGz5JKs9/Q0OJc9wTu3MxKrE7pkMe4U1svNqJra+HiE0ivbq7+R/TTUNYG1E3kk2yx/uF1+YRcMX/hlSLpzJuIeZ+Z2ogx+IkHSbBEFkd8Olh5DsBnUpxsRZPa3sDXNjY49dHFTLPeLY+tsrqgWPyNs6lcOOYN6FhTapaYE/Pg0AB8g9JBa147fLJ04uZ/6bLyZgmjU2+vgYjV+A=
    clientSecret: AgBMGLAe/rx3YoeqFv81ioQC8GI/GcIg2IXiNYOb61l3bVi5VmAT2uV7CmNuU+iIL/FIslgZeLYK1WSWJ7JM9gMzrZuAYFxi23SsYjafWRTWZfTYrQVK0dlp9TCBY3s8C5gujiE08htCmQRGyRnsaFmuxJfwGCzFnPYHKPKJWdgzvrIcY8Jy/bQgvBt3YWZ1264N8rmEb8zzFmn3dPZ2YD4ryXr6lY450VmdxYg/wCx9/8UuHcRqy3FH6UA8pC1Y6w6PsAN68ENobkmAT2aPRLntqMvqnrMaNkJWDx1VUuz8O4F9KfpHdvPko+r6xaeI5cuLaMR9VzGS/7mtADe3zPniPzcjj7sFmX0Wt/QaFGx5JsaetPs4ngw3IQwttPkvRibI31ZGEioJDSwBT3JfSisz/oS7fG1IaRh60zswNBx0CaA0vHJht3bbb/XPdHdoEgoPK6qRXokW7lHIx3OxP3FmSqUKV/51y2LqEIJY+T0DWCI3xXiOAxUKWw0ThThUmtIuEsNFJ/riCgLo0levCrUtTzV/+VxOmo8SDwZlhoIpbh0OOk2YyoPT+bCBDZUDWbxnEkVxL/z9ansvJ7db9j2qzv4b9qknPVDC8eB+BjMHZE+IK43BX5tBAobAck0bGRTN3R5XrDv2oo/bG1weDjj/j0aRFVmp5i3NyIFqSYY7p+d+82p5N2XbWVnynEUogx4RIKnHd/hCV0MLeeNWiyuSXcgTaeb35VMpvIExb22kQQ==
  template:
    metadata:
      name: netbird-backend-oidc-credentials
      namespace: netbird
    type: Opaque
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-relay-credentials
  namespace: netbird
spec:
  encryptedData:
    authSecret: AgB9VjwTn3KK5wsczoJf0WoHRX7BdJ7ENaxbuZtmbzAI/NFQrQIPprRvI9tvVDDSlL8i0vlzN6a2bxFF6jApj+lth1N8JHePWs3g+6lqyA8ihYXLoPGb7bQCpMlccPj3tYKSkWhfJEW2lJMMsN+toZxnoNM+UG1oLO1524ejNAz+NqoADtHv0gVgwFQfoBWEOom26ppqzV8/PZhBoL5HJefw/2UC1qQxQ06uDrvRatnC85XZ9w+5lnjGOsvEAq86Ia1YtERxeXRUcMx7DMARCISQ6e3uzGw4uzj4ypgpq67i6wqppPi5b8H4I+cacxrKp5oZsH4la2ZvnRMOzqD4rGG1TF69fzES/o6J9uzORcA5wugi7X7RXGEQQXEIxqR6cUsbxwjy5r03h54TN5988swLfjo17UG4bP5i96R3c3X4a2IvGnVC5dg7CKS5S6xALUnl84Z9V3v8oltxEVa9z7FZ32Z3piHr+vc0Ub9uVmylBnjEqZa6u4BqIUlG4NN87sNuBRhSpZnjY82j0RqeIU3nj0ej+8KpoMFYOSW2VGDOya5lNoRk8tck1UXcx6Kg47UpkFkR4Kbtw4RJoJPyVb8ujyy/Bpt7u36mu9Y4mAVyHoJgAXXYZVUIV2smNFB0GtZ/sPmHhe4s1SyfRDtYXz6g5dGxyO7mQQTpi/SokBpQMm7U1PxNZtv3N6zdxCjBOFlmdIuLBGoZV8emCe/HhWw7RD1rBw==
  template:
    metadata:
      name: netbird-relay-credentials
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: netbird-turn-credentials
  namespace: netbird
spec:
  encryptedData:
    password: AgCG/s3MyJZ4M4qNyjVMonU+TDY+ocaxyye0tMF5kh7uVSJjHoASnEfKjnPpOC/quWksWploqGTgyHsL0RCY9BOPMpAuE4Pp+53VY6jAvH/NrJZOsqQEdgtdmjchE+3d3vZOCM5tHkjZSClvTGgTRFkXdl5pIWE4+o4WuLGtcPFrB4uLYCwq6UgazoOarKTAaj3oPuqssh4HFGyds8gKNxBHaq6cO3RvcH20P7dvnw005UCDymyVoTjxiuueT5SM48aH7O8aXhRYPbFbLwHsD70Q0UaYRT+UUPWnvpHWpDheHr3dVvWM3nkXxkOwre5Ld75BjfDfIM0/CihULYIOxUi/A5e2HTdauGQCGUSJvfh0z+VwKeVsLe6mBEJ6EKCWpWPDwVtnfEYk2Yohj/Z4O5F0ie2eRhXN+QsdCu+RQAxf47gFiKe0GueN1tVn+/QxGrrv4Lp2Jb1FiMwPSmSd4kJW44cKjd36j5ESrXyifOOwvcC+7eCDFOPfTMqYmloRp4RyXTec/BFzxZZ1ONWRYq0Tni4tlw04CCNsEVy+OYDArWu48v6cfPTAiHjgZBZHyIrRpG6cPOGzcWzJLdkQXkK6kxbUOqLVjPNEW2x71kRqiyAquszYV92FBoE3n8T1Au8GsADuurnN/t1f1B8SFi0BX+dILcMkxmQgVTBk4+946/NT2jRILE4ADCWYCI2CFzK5esvn44rlCzB5TeAPZA/l
    username: AgAr8X32/nMm8HRFPn5EK8rta+TqJtiBsmNKJqRe8EANSWCj/UlVfLSv2g1NSsOYZgorVKgHsS8ROj6K0xBbN13hGIwJ0kFICi9REHTdl5j34ElrCPsbYMwr3ZHjTHqTn1h/k1LS6COpp1lzAtiE9iUHKRu2EPLDJO1E5teRFLIBXMXanWWiLzjzgbqHImmW0p8/75yqs0DK1xYNiAAge7F7KJM9wq3lpRYbJ9oh9fv/hOEAL637NXr/fKtxMYj+hYzXzKSolbVep9iBmltCI5BJJMbsGLFaSa5oSjG+pS+2UB/k/osUiO2TfRpwn04yIiiMh5hmXqsAIN5T8t41l/46hqOvMMzrWfxHHNPGHBdlMuypXMVNrjloBLxbjApD8fNYR+TlkC6QqPEdv2Thd6Fta68JUWhLPyzbVhhGD9roMvY5r1diN4QQmMEfhl1JEumNWYDA7rSi1wrMjLr1XUJe+8WfxfW3f0RzNwXJvFtMYsqnjUJJkjmuswCwpfW5jBkAE2Q5wTdaFdoGCrZZsHUbiT3Vittw2/QUWvVLNvx3DZT3T9D6T7nVuq+3I7ORNFcXYhO8fdwevMhv/1L47n/EkVo/7ZfSj2xIvaDZ4/GTQvshttkOQkGFchFmkbmH/F/NP3/Hha0hk40s8n+1GR/nOsUEnEVBlSDbL8Aolt9ZC5DSJY8ISkPwWcLv8DqdY95MpmC+qgrT
  template:
    metadata:
      name: netbird-turn-credentials
      namespace: netbird
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-admin-user
  namespace: nextcloud
spec:
  encryptedData:
    password: AgA133ceVXtfCLHa+fpkKReprcm1tpKZkbpTwYf4tFydhIIO2QLXRD2CioaSDYsk0aNlQUzXZyQJyMfplLs6G7Q0t/2d8cYVgVmEBcdiXgB3CF80gY4/2newgaCQp1MbCYoc28q4scE1vZvqgRfSGWiO12sxPklMWtrAxgIoNz8myEhKEuy42GfneXDUjfN+LvHFSJugDCVBLE4rc5MkNdB40UO6R54zxL40FQf7azZEMcLDIaUfM+ki4445V00rEMA8J9k9DEdPIXwRKva+osIcbKDuWv3L43fEyP0cfj1g5lgNcm+oX/Kmo+Lp48+jvdYOMQCnWW2A19pBIHlym33bPR/ViMkl/o+ztKiH6UxxNy2FyUZkIxKjL+1h/XctiqiNjJg7zCWEn2Mtue0zN0CMSe37ApzE5DzngsHpLAkvJ0DxHq3PPd3l3TevgeVj0hc2gR0sT08TuT9TRKK/EqCcPHpmeKtnzzruV7X+Vl0nreAXS1T/x2COW4FP8RE4RkXSDxf8OPhDy+MSFX5ortwbGM29yOt/9YU1Sq9uDiCj3OeXkFq0wEHCMsaSVJB255cMTR/a8FNrQFIeHT/UkRVQuSCVKAejpeN3EQnMr6ni9BQWu0BVLTwGa3XXQ94ucnmQtye3TwhIz30Hb/4mLloU5jMYUap3NwBp9cMwOxrKyH2HU0cDARAwgsgJhRA6a+OAGPz4eBwxZpuRza5CPRXNefF3cg==
    username: AgApHO0tUVeG++vsB3O4rLN+RrgTFG4uYNvk3lI8+KMV+n3faNak6j+RwhNtwqwcDO+loVQCfvi+mpW9txEqCfTRQ/xbI7oAHGP2fg5YJw4fHFDdKyH5tAeYE27nF21cAjABFymTqkqyD00A7qlorjljc6ld+KUwS67XL0LFsssuNfZQ9bxDjl/Yk2a+4ykNXNYQnMO7hqvxcrIhq9Dbbr8rOFNYTM6ByhBwaU7R5/QgUJ/52Eyb3Og0C97SSesNQUH2UsjHFVEqdMD7dGmdiAdvlA/5wd1V8yFlaUfPkOkWnCpnkpz4frSIMKJwQcIcDF5/YIfMtb/i4AOcUAv9qW4brlS4iCB37juPfItZkUtXm/iHItEAE3E2lkl9f+t1St1f9KuLh3aNOSgfiwTSU26Iw/850ZUjQKhkSXDw4uqiZ7XJC89dT1aavQfPLugj/5U7vuGRkkC+O04x3N6lLbDOqNC7O8H+sIwqd2AnOvVFvGf9ucnyY5ziNuDgNUL8bJQPd06zGh4u15tbOaat4LJ0DbNQACp4Fln9ivipgLvDb/8+WYSmkX2gL2TPMjzoq/FLtB3jgbhGcCDb0gqDkE/UT19JVHjCoBZAHi4Zds4cSknKiqbO8X5mcK4j4dQNutsA5cAbWasjtC18tCySevkmu/6k3oZutg4OYaoiWA0lHwu8AtFlQJGVMg2m4hEp2vaZ0SRsEQ==
  template:
    metadata:
      name: nextcloud-admin-user
      namespace: nextcloud
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: nextcloud-database-auth
  namespace: nextcloud
spec:
  encryptedData:
    mariadb-database: AgAJjONCV3vYfnRn9F/Swg7ZPECa3rom6sS/9LVkyJJhhDxuAekF37urjn10HhsKSWHcb3+iqWd3IGXfVnBdBa2eEDdg8SQ/ljyvLqw8NjcaXIRdokUgRQ7NZlmTK6/ANFY8JUV960jseSKsCrxaA/WGBQnGTXf6MI3DrJlfdy3TxNXOYgXVACGh5sHOW/h38lbzLLshYbU9NwJz9KvDHdlDzjV2cF7FsXW91vclK6eNpMoABgkd2daOpwy83FT6mCAxkK9cCHlrsAEVXGMoilqPMVkvcM7VxXyixGeOXN6Ll8X79lkj5ja5uzTADEjdXhQiqyRrFzB/EAJF2LHy1sxgpxbpXCi2Ru/znJj7scomE6TF51VlY81sHxLFUxAgwMgQcnLQh3i1vYru/1NCrdeC9IAWdYCe6wuiOsf9FcHq4rx6N8zlkUFgewXVAWadUEpGad9QUQm//aVjq2kFM1kxRfiL25hoB6ec4WXhDgcy4f7CKtKIgS5IC3hxm3b4XAuuekr0s4G2+XGf1endy/Fwu4gVyFzRfQQx8hU75s2jaktmamaIfYlxuHfEGJnnxSbIFOUGM3sQd88sT/7gEXuNQCDXcsNylSaQNwYjPbu/iTlscMlowlTb32W1iM3IwSvx2xeo0DOMVzFUBqE5fxpG9zyA8U7gte7QQ1RWvRXE7Ll4dsOuHziPANZdMyRml8WCqgmcvtzc2s8=
    mariadb-host: AgBXn4r+i1apjXlWD08Qqx5Jh25Un1b85dgd2gLVrZmZo+PYXyAjQA5DLn0rSTfQJtcwp+8/huL94jxm3q1wXQkh4N5TsB7OItt7LModq6ZntgsDqvN+wCK9mvubBee4SSCKvQolgoI92YkJBBM3kdqN9Qvi/AY7X3zTUk2KjwlCeURVWY+L6kNEtPetNz5zZbMz/LAo6wPBX14ZhZL/48WWTURCyJqj9OHYpoH25yQaSEIaemAHRFh+5KLe7+J2nUmuiViXu2OuxmhwKKZYANAiKfur62Phmbcp0BTJnXjXyVAX/1kqmpkspuBOd1ITYg5HpD28BMHjeVWmoWVZf35eQ3IivD9O/Eg7l0SzBoGIBla4dRXbuKRb0hZON6CVqMzv86vo5zkLtd0n6s9vo1p8qQJ0eUCxHfXzwepRo96trvq+FUwFH9/dGhGan6VZP2RUwcvdY7Tk62fzEA6sqPuR/XFjfn4BrzMjaH+bB1DmtHjz/oeOX97wlCGboj1gJfvTIBF7HjzuDLJ1rcI7UTMTA88RscYMqlXhfG1Ao2ZD32d64q8VibWDgVoJMhskUcePivURV/o1xm74rYp1TxOdNn5TqDDzhYFSjgWcjmpbeqC3A3cxlf3DnjwFnm/daDRBXW/W4261GI0Zks8CTgWs9xaV/sNX0/Nei2o5Mtr1q1k6vm4n57Xg17th9OMS2JML5JshKodn9AWqKOqgvzA2Hw==
    mariadb-password: AgA8r1aajxOhMcQbDLegI4FOUu7lU0hlnWpniUCKz/oAVHf58bxsZBXsXwnF8InhUrrtUk7tEZeWK/YjbbI19IlQRuaJ0CXfbycSOnma/VkVTx2N+U8ODpqcT6btzdQXq9qi4NQkv3c/9JLxqiSCJi7cm/T+x6taSEzoL4Aa/zzJ+yMXZU8sB+mvkIxQMwriHQeQKoxK8AHYTG2WQM9a0tIjLsmXU7GhpekryMRKCaJDdjlSpFZCKO+ym/eCGeqP3oMXV7MRpB0l9qLIzs+GwKigup/x9lqLmHCuxaEeFZlkThLDTiuVXGoIZ+fwEOct2TH1aVKZFQRfRG/EPC5+AJApub2iiFofOJb8SxLHv9DFmfbPz3108vFUN0P3u8x2nZLwCWF9BjvpFrhYxWgr4M9PVtLu8caOBP3Ja9W6jYqawh+7ARXx7wW9GOPA1P9JUbt/7mCuueiuWw1xgchfceU9aZeFkebrhcH4/sK+Favt618Lv7XB8WIT8LK4IjEFs4SNW2rG6p7rYnCvtaL+Fc4YR8J+kkXxlo0LHLN9dM6TSiueorQdkpudxG7IJc/0xIs/h4OTpdPPhkEK76GQHen9X718nFI8oXPraGmyygcdIIBgV964PEMH8EwPZIHgHfddVosO2mupMwgccl8GDNUDTf4MO6hKPUWRb4P+QlUaFqwAw7jvu1psLA65NgWHxUn6FGruI39iPcUB9P0uQd2sSKNabw==
    mariadb-replication-password: AgAu3zmyxd/YLtPdmamJh8Re6VoBPXuwp9zeQTIK4Kd36Un7Ut/nTqxxbcdOESmEg3dVy1fnxIJp4NiLTY/xmkFGK3XpAgu5UOYSQolxzChB6sRC0nVNeeGixt4/tpv2PQocbFNo4XKKWiNZ9fLsEbLDAM/tZPhVamlbqCclQelZ9+PBcHcc0c2NkHqORHF5tW5QSCvOgt7b9zG//X/JGon2fa1ittDrqOr3s6Y7KJwT9YLQXCo0tlLjQd6yPQ7ew+KIwd+ZOlq7istE39o5s4qIRDuTHR+soGaTBcAHjBdVOBaM1naGtSWMuw6pUrRP+KdGOp3Uh1YuwjelmATAIR9+4/Gvvo7hSPlI8JaAV/RwJPlMJTe9MBWDV/fnLYnCWea/vNIj3n4eRkeXHEHW3mqu4K7PXN59Uw5lU8WzHI3Zm+vU6f20tYYA6E4wfD5FP2EwHgCKOEloYm9tji4OhTkERSQGnH3IKNk+Sh+UhDMGOd8V2VZXHoPEsLKKGny6D3FqRKUFzL4+ysjN16p1J6K9wJGwMHhBOabp5f1JhKoKOgLsbXrAEV5gdIzdlo5EoipA5jI1l7k3gcFUIptuxsBTAZwrBoNx29cebj9C/FRuz2kEC7J4LKDHlBHjhlJ3S0yZ9LwQzQk5LKKnwcSCKARAImrih7lvZsAzDAtWfDCstg3lBT6slx/mpvSBzAYhow6vXAI9XA6wxOy+iQUl8++oaM+oEg==
    mariadb-root-password: AgBjgQYehZaqqgIT1yQuE+EWb7o6m1ad8PTkkfSReV5QMDF6p/1/8bTkYxiXDR6VBRNJBKeZAhHHZMw54KRdDlj2JMPPr2VwKV1AJ0Kb1zXr2I+3cMydk93K9UHWx23QzececXBIXsXnry+ah8rEfJqENsrtqJSe6dv+T7cLJX9Sso1zvRqYq5uYvjEx/HmfX0r9q5R2RXhhBnJmBAIOVoh5khSCVsqO4Ah+Xt1VVhmWQWEdJmj/c4EXjqnJq753HlDmkI21N+A3VPbGIQLswkIDzIZeShjYAcZCXkLq3DDwJuN81fPSQZN/2Sp7nUm+HTdyOjTxXhEMbszWKwCW87C2uhEh0QgJb0iyc700/KHX9jV4Y1hwyC7thmSRf4p+O3iqSBgaImADh1BVBcpZM4D1CSjz/dpfcdDIxEAGHsiQDZexMJK9sC5Nx2kXP4pWXLxLc6XidsX7BWzIl7lzcY+rneZUWVtiRUARtSqYmO7l6KLmxUz4FuwPraD5B7S+1iy9kZ26JJ4AMJ+CklHMbR2rxUjcOt8KXkMMcUDaPoHs+gMddof8AKfBZGQkBc9oPnmM4T5yWhyklX9Q6zbk11bAplgQyjEroCWUb4q0ozb1EVI7FdtROGfvYU5R616//zo/AZVbAMncnRVdSkSq6Nmu0eaDs0mUttTf69o9D9yM3oQg4RO3yOTVELUi4n1mohxr9T7heip2aDp3S5DYvGsMWL63BA==
    mariadb-username: AgCVfKxxY8KGPODGmC6S8JHqVOVIvrz+t3mW+wAROGJa2mHeRur+lpt29tQ2a2Cc9dsGDg3s4cQ8xqVcFASkpD2gw/MeMK1dt/teNjYFLuWY/50S0a24nXHMzKwrZPTRu+a+7UrhMysitU9AdJ7uFqvLEftKj6y+zVXnB1OtTDfp5kV7XUq2aF9SVelETxoqUqPBWFUbWvAF0jyVMZ2EZbYOv9UmqD60C8VTA8gd+IDUcycnPfY/5dvz0YzufAxzyAQK0fGuX6WHEsdUqNX1YKdI7GQx1L3qMPgPSw6wlkYR22whwdgenyt63DffRR7mHhUmQeL9I75UUWuozFu/V/20/48lsskmSUCdJHxoiFvGdu0v6dPpfr0XMMQR1O5Uao0XLbjU6Asp9QTMAUxYIr7mdFU2Ae7jliRSviWD/2UOhtT5885Ua3dWOBvbAdsSwi6UEZFvH7GbkJRLDKmeoigCxKA9GmWOACrdCg7vvKVvYLFrct2NBCxpbKwTUw/qkT5KRlxx684lgv60ICYTPLLfxH9WI9MtxTEX94nUh9X8bKi2YkUi1KA0j9xOWXlKN+un1k+fUy8+jszNv+SMU7elCkoS6zFBHmg4uqIe2ItjBx62PR1rQ6SC7cl0NgXYu25ANW/Js9Ougeq7awOEhjj5ndB/OHdtEU5vFVeGc3XZa2TYRBuvkvPD75fTHT7SnxATRra6daehsv4=
  template:
    metadata:
      name: nextcloud-database-auth
      namespace: nextcloud
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  labels:
    app.kubernetes.io/instance: coturn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: coturn
    app.kubernetes.io/version: 4.6.2-r8
    helm.sh/chart: coturn-1.0.0
  name: coturn
  namespace: coturn
spec:
  dnsNames:
    - coturn.homelab.olav.ninja
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt
  secretName: coturn
  usages:
    - digital signature
    - key encipherment
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: keycloak-jwt-signing-certificate
  namespace: keycloak
spec:
  commonName: homelab
  duration: 8760h
  isCA: false
  issuerRef:
    group: cert-manager.io
    kind: Issuer
    name: self-signed-issuer
  privateKey:
    algorithm: RSA
    size: 2048
  renewBefore: 720h
  secretName: keycloak-jwt-signing-certificate
  subject:
    organizations:
      - homelab
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
  namespace: cert-manager
spec:
  acme:
    email: olav.s.th@gmail.com
    privateKeySecretRef:
      name: letsencrypt-issuer-account-key
    server: https://acme-v02.api.letsencrypt.org/directory
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              key: cloudflare-dns-api-token
              name: cloudflare-api-credentials
            email: olav.s.th@gmail.com
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: self-signed-issuer
  namespace: keycloak
spec:
  selfSigned: {}
---
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: default-l2-announcement-policy
  namespace: kube-system
spec:
  externalIPs: true
  loadBalancerIPs: true
---
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: default-ip-pool
  namespace: kube-system
spec:
  blocks:
    - start: 192.168.0.90
      stop: 192.168.0.99
---
apiVersion: keycloak.crossplane.io/v1beta1
kind: ProviderConfig
metadata:
  name: default
  namespace: crossplane
spec:
  credentials:
    secretRef:
      key: credentials
      name: crossplane-keycloak-credentials
      namespace: keycloak
    source: Secret
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://upload.wikimedia.org/wikipedia/commons/b/bb/Gitea_Logo.svg
    ingress.kubernetes.io/proxy-body-size: 10000m
    traefik.ingress.kubernetes.io/router.middlewares: gitea-login-redirect-keycloak@kubernetescrd
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea
  namespace: gitea
spec:
  rules:
    - host: gitea.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: gitea-http
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - gitea.homelab.olav.ninja
      secretName: gitea-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "false"
  labels:
    app.kubernetes.io/instance: hajimari
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: hajimari
    app.kubernetes.io/version: v0.3.1
    helm.sh/chart: hajimari-2.0.2
  name: hajimari
  namespace: hajimari
spec:
  rules:
    - host: homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: hajimari
                port:
                  number: 3000
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - homelab.olav.ninja
      secretName: hajimari-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/appName: Immich
    hajimari.io/enable: "true"
    hajimari.io/icon: https://user-images.githubusercontent.com/27055614/182044984-2ee6d1ed-c4a7-4331-8a4b-64fcde77fe1f.png
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: server
    app.kubernetes.io/version: v1.119.0
    helm.sh/chart: immich-0.8.5
  name: immich-server
  namespace: immich
spec:
  rules:
    - host: immich.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: immich-server
                port:
                  number: 2283
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - immich.homelab.olav.ninja
      secretName: immich-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak
  namespace: keycloak
spec:
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  name: http
            path: /
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak.homelab.olav.ninja-tls
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.middlewares: keycloak-ipallowlist@kubernetescrd
  name: keycloak-admin
  namespace: keycloak
spec:
  ingressClassName: traefik
  rules:
    - host: keycloak.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  name: http
            path: /admin
            pathType: Prefix
  tls:
    - hosts:
        - keycloak.homelab.olav.ninja
      secretName: keycloak.homelab.olav.ninja-tls
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-management
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-management
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /api
            pathType: Prefix
          - backend:
              service:
                name: netbird-backend-management
                port:
                  number: 80
            path: /management.ManagementService/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-backend
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-signal
    app.kubernetes.io/version: 0.26.3
    helm.sh/chart: netbird-0.14.2
  name: netbird-backend-signal
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-backend-signal
                port:
                  number: 80
            path: /signalexchange.SignalExchange/
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  labels:
    app.kubernetes.io/instance: netbird-dashboard
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: netbird-dashboard
    app.kubernetes.io/version: v2.1.3
    helm.sh/chart: netbird-dashboard-1.0.0
  name: netbird-dashboard
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-dashboard
                port:
                  number: 80
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    traefik.ingress.kubernetes.io/router.entrypoints: websecure, webpublic
  name: netbird-relay
  namespace: netbird
spec:
  rules:
    - host: netbird.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: netbird-relay
                port:
                  number: 80
            path: /relay
            pathType: Prefix
  tls:
    - hosts:
        - netbird.homelab.olav.ninja
      secretName: netbird-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    hajimari.io/enable: "true"
    hajimari.io/icon: https://nextcloud.com/wp-content/uploads/2022/08/nextcloud-logo-icon.svg
  labels:
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nextcloud
    helm.sh/chart: nextcloud-6.6.2
  name: nextcloud
  namespace: nextcloud
spec:
  rules:
    - host: nextcloud.homelab.olav.ninja
      http:
        paths:
          - backend:
              service:
                name: nextcloud
                port:
                  number: 8080
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - nextcloud.homelab.olav.ninja
      secretName: nextcloud-tls-certificate
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
  labels:
    app.kubernetes.io/instance: traefik-traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-33.2.1
  name: traefik
spec:
  controller: traefik.io/ingress-controller
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.5.20
  name: gitea-postgresql
  namespace: gitea
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: gitea
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.0.0
    helm.sh/chart: postgresql-16.0.0
  name: immich-postgresql
  namespace: immich
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: immich
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.5
    helm.sh/chart: redis-19.5.3
  name: immich-redis
  namespace: immich
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 6379
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: immich
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 26.0.7
    helm.sh/chart: keycloak-24.3.2
  name: keycloak
  namespace: keycloak
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 7800
        - port: 8080
  podSelector:
    matchLabels:
      app.kubernetes.io/component: keycloak
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/component: primary
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 17.2.0
    helm.sh/chart: postgresql-16.2.5
  name: keycloak-postgresql
  namespace: keycloak
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
  podSelector:
    matchLabels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: nextcloud
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/version: 11.3.2
    helm.sh/chart: mariadb-18.2.0
  name: nextcloud-mariadb
  namespace: nextcloud
spec:
  egress:
    - {}
  ingress:
    - ports:
        - port: 3306
        - port: 3306
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: nextcloud
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mariadb
      app.kubernetes.io/version: 11.3.2
      helm.sh/chart: mariadb-18.2.0
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: openidclient.keycloak.crossplane.io/v1alpha1
kind: ClientScope
metadata:
  name: netbird-api
  namespace: netbird
spec:
  forProvider:
    consentScreenText: Netbird Management API
    includeInTokenScope: true
    name: netbird-api
    realmIdRef:
      name: homelab
---
apiVersion: pkg.crossplane.io/v1
kind: Provider
metadata:
  name: provider-keycloak
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/provider-keycloak:v1.9.2
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-auto-ready
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-auto-ready:v0.4.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-extra-resources
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-extra-resources:v0.0.3
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-go-templating
  namespace: crossplane
spec:
  package: xpkg.upbound.io/crossplane-contrib/function-go-templating:v0.9.0
---
apiVersion: pkg.crossplane.io/v1beta1
kind: Function
metadata:
  name: function-keycloak-builtin-objects
  namespace: crossplane
spec:
  package: registry.gitlab.com/corewire/images/crossplane/function-keycloak-builtin-objects:v3.0.0
  packagePullPolicy: Always
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: KeystoreRsa
metadata:
  name: jwt-signing-certificate
  namespace: keycloak
spec:
  forProvider:
    active: true
    algorithm: RS256
    certificateSecretRef:
      key: tls.crt
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    enabled: true
    name: jwt-signing-certificate
    priority: 110
    privateKeySecretRef:
      key: tls.key
      name: keycloak-jwt-signing-certificate
      namespace: keycloak
    providerId: rsa
    realmIdRef:
      name: homelab
---
apiVersion: realm.keycloak.crossplane.io/v1alpha1
kind: Realm
metadata:
  name: homelab
  namespace: keycloak
spec:
  forProvider:
    realm: homelab
---
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: csi.proxmox.sinextra.dev
spec:
  attachRequired: true
  podInfoOnMount: true
  storageCapacity: true
  volumeLifecycleModes:
    - Persistent
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: gitea-ssh
  namespace: gitea
spec:
  entryPoints:
    - ssh
  routes:
    - match: HostSNI(`*`)
      services:
        - name: gitea-ssh
          port: 22
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: external-cluster-ingressroute
  namespace: traefik
spec:
  entryPoints:
    - webpublic
  routes:
    - match: HostSNIRegexp(`jiyoung.cloud|{subdomain:[a-z]+}.jiyoung.cloud`)
      services:
        - name: external-cluster
          port: ingress-port
  tls:
    passthrough: true
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: login-redirect-keycloak
  namespace: gitea
spec:
  redirectRegex:
    permanent: false
    regex: ^(https?://[^/]+)/user/login(\?.*)?$
    replacement: ${1}/user/oauth2/keycloak${2}
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: ipallowlist
  namespace: keycloak
spec:
  ipAllowList:
    sourceRange:
      - 192.168.0.1/24
      - 10.0.0.0/8
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: securityheaders
  namespace: traefik
spec:
  headers:
    customFrameOptionsValue: SAMEORIGIN
    forceSTSHeader: false
    referrerPolicy: same-origin
    sslRedirect: true
    stsPreload: false
    stsSeconds: 15552000
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: jiyoung
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - temporary: true
        valueSecretRef:
          key: password
          name: keycloak-initial-password
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: jiyoung
---
apiVersion: user.keycloak.crossplane.io/v1alpha1
kind: User
metadata:
  name: olav
  namespace: keycloak
spec:
  forProvider:
    enabled: true
    initialPassword:
      - temporary: true
        valueSecretRef:
          key: password
          name: keycloak-initial-password
          namespace: keycloak
    realmIdRef:
      name: homelab
    username: olav
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
  labels:
    app: gitea
    app.kubernetes.io/instance: gitea
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gitea
    app.kubernetes.io/version: 1.22.3
    helm.sh/chart: gitea-10.6.0
    version: 1.22.3
  name: gitea-test-connection
  namespace: gitea
spec:
  containers:
    - args:
        - gitea-http:3000
      command:
        - wget
      image: busybox:latest
      name: wget
  restartPolicy: Never
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /mutate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    rules:
      - apiGroups:
          - cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
        resources:
          - certificaterequests
    sideEffects: None
    timeoutSeconds: 30
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from-secret: cert-manager/cert-manager-webhook-ca
  labels:
    app: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: cert-manager
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: webhook
    app.kubernetes.io/version: v1.16.2
    helm.sh/chart: cert-manager-v1.16.2
  name: cert-manager-webhook
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: cert-manager-webhook
        namespace: cert-manager
        path: /validate
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: webhook.cert-manager.io
    namespaceSelector:
      matchExpressions:
        - key: cert-manager.io/disable-validation
          operator: NotIn
          values:
            - "true"
    rules:
      - apiGroups:
          - cert-manager.io
          - acme.cert-manager.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - '*/*'
    sideEffects: None
    timeoutSeconds: 30
